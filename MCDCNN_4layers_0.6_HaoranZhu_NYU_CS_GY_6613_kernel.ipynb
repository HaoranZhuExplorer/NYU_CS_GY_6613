{"cells":[{"metadata":{},"cell_type":"markdown","source":"<font size=\"5\">**Project Summary**</font><br>\nIn this competition, the main task is to do surface time series classification. 1d convolution is a good technique when doing the time series classification job. So I apply 1d convolutional neural network to solve this problem. The whole code is written in Pytorch."},{"metadata":{},"cell_type":"markdown","source":"<font size=\"5\">**External libs we are using**</font><br>"},{"metadata":{"_uuid":"a6ed998e-ee9a-4333-8332-8fa753b7f0d4","_cell_guid":"e6e0bc2b-9cba-4688-9e0b-15f11d219c2d","trusted":true},"cell_type":"code","source":"# libs we are using\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport torch\nfrom torch import nn\nfrom torch import optim\nfrom torch.nn import functional as F\nfrom torch.utils.data import TensorDataset, DataLoader","execution_count":72,"outputs":[]},{"metadata":{"_uuid":"b8906e48-2384-4bb3-bba7-722160428e39","_cell_guid":"295d38d1-6663-4575-93ba-c6cfac9f6129","trusted":true},"cell_type":"markdown","source":"<font size=\"5\">**Data preparations**</font><br>\n\nIn this project, I use the raw data as input of the network. I concatenated all datasets into one single numpy array. The first 3810 rows are training data with given labels, the rest 3816 rows are testing data where labels are not given. In total raw data has 7816 rows. In addition to that, we split the raw training data into training data and validation data by the ratio of 80/20.\n\n\nI create three datasets and data loaders for them to make the data ready for model's training. The process is straightforward. I split the labelled data into two subsets, and keep testing data as is. Also, I convert Numpy arrays into `torch.tensor` objects of proper type (float for samples, and long - for targets).\n"},{"metadata":{"_uuid":"94c41c8d-0c91-4aa1-83a4-ff3f518f35f4","_cell_guid":"88b9ec00-fc64-4040-896b-895d06cd58f2","trusted":true},"cell_type":"code","source":"def create_datasets(data, target, train_size, valid_pct=0.2, seed=None):\n    \"\"\"Converts NumPy arrays into PyTorch datsets.\n    \n    Three datasets are created in total:\n        * training dataset\n        * validation dataset\n        * testing (un-labelled) dataset\n\n    \"\"\"\n    raw =  data\n    sz = train_size\n    idx = np.arange(sz)\n    trn_idx, val_idx = train_test_split(\n        idx, test_size=valid_pct, random_state=seed)\n     \n    trn_ds = TensorDataset(\n        torch.tensor(raw[:sz][trn_idx]).float(), \n        torch.tensor(target[:sz][trn_idx]).long())\n    val_ds = TensorDataset(\n        torch.tensor(raw[:sz][val_idx]).float(), \n        torch.tensor(target[:sz][val_idx]).long())\n    tst_ds = TensorDataset(\n        torch.tensor(raw[sz:]).float(), \n        torch.tensor(target[sz:]).long())\n    return trn_ds, val_ds, tst_ds\n\n# We use this function to divide the data\ndef create_datasets2(data_arr, train_size, valid_pct=0.2, seed=None):\n    \"\"\"Converts NumPy arrays into PyTorch datsets.\n    \n    Three datasets are created in total:\n        * training dataset\n        * validation dataset\n        * testing (un-labelled) dataset\n\n    \"\"\"\n    raw =  data_arr\n    sz = train_size\n\n    idx = np.arange(sz)\n    print(\"total data length\")\n    print(len(idx))\n    trn_idx, tst_idx = train_test_split(idx, test_size=0.2, random_state=1)\n    trn_idx, val_idx = train_test_split(trn_idx, test_size=0.25, random_state=1)\n    \n    print(\"input one dimension shape\")\n    print(raw[0].shape)\n    print(\"training data length\")\n    print(len(trn_idx))\n    print(\"validation data le\")\n    print(len(val_idx))\n    print(\"testing d\")\n    print(len(tst_idx))\n    \n    trn_ds = TensorDataset(\n        torch.tensor(raw[0][:sz][trn_idx]).float(),\n        torch.tensor(raw[1][:sz][trn_idx]).float(),\n        torch.tensor(raw[2][:sz][trn_idx]).float(),\n        torch.tensor(raw[3][:sz][trn_idx]).float(),\n        torch.tensor(raw[4][:sz][trn_idx]).float(),\n        torch.tensor(raw[5][:sz][trn_idx]).float(),\n        torch.tensor(raw[6][:sz][trn_idx]).float(),\n        torch.tensor(raw[7][:sz][trn_idx]).float(),\n        torch.tensor(raw[8][:sz][trn_idx]).float(),\n        torch.tensor(target[:sz][trn_idx]).long())\n    \n    val_ds = TensorDataset(\n        torch.tensor(raw[0][:sz][val_idx]).float(),\n        torch.tensor(raw[1][:sz][val_idx]).float(), \n        torch.tensor(raw[2][:sz][val_idx]).float(), \n        torch.tensor(raw[3][:sz][val_idx]).float(), \n        torch.tensor(raw[4][:sz][val_idx]).float(), \n        torch.tensor(raw[5][:sz][val_idx]).float(), \n        torch.tensor(raw[6][:sz][val_idx]).float(), \n        torch.tensor(raw[7][:sz][val_idx]).float(), \n        torch.tensor(raw[8][:sz][val_idx]).float(), \n        torch.tensor(target[:sz][val_idx]).long())\n    \n    tst_ds = TensorDataset(\n        torch.tensor(raw[0][:sz][tst_idx]).float(),\n        torch.tensor(raw[1][:sz][tst_idx]).float(),\n        torch.tensor(raw[2][:sz][tst_idx]).float(),\n        torch.tensor(raw[3][:sz][tst_idx]).float(),\n        torch.tensor(raw[4][:sz][tst_idx]).float(),\n        torch.tensor(raw[5][:sz][tst_idx]).float(),\n        torch.tensor(raw[6][:sz][tst_idx]).float(),\n        torch.tensor(raw[7][:sz][tst_idx]).float(),\n        torch.tensor(raw[8][:sz][tst_idx]).float(),\n        torch.tensor(target[:sz][tst_idx]).long())\n    \n    return trn_ds, val_ds, tst_ds\n\ndef create_loaders(data, bs=128, jobs=0):\n    \"\"\"Wraps the datasets returned by create_datasets function with data loaders.\"\"\"\n    \n    trn_ds, val_ds, tst_ds = data\n    trn_dl = DataLoader(trn_ds, batch_size=bs, shuffle=True, num_workers=jobs)\n    val_dl = DataLoader(val_ds, batch_size=bs, shuffle=False, num_workers=jobs)\n    tst_dl = DataLoader(tst_ds, batch_size=bs, shuffle=False, num_workers=jobs)\n    return trn_dl, val_dl, tst_dl","execution_count":73,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"5\">**Neural Network Architecture**</font><br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Flatten(nn.Module):\n    \"\"\"Converts N-dimensional tensor into 'flat' one.\"\"\"\n\n    def __init__(self, keep_batch_dim=True):\n        super().__init__()\n        self.keep_batch_dim = keep_batch_dim\n\n    def forward(self, x):\n        if self.keep_batch_dim:\n            return x.view(x.size(0), -1)\n        return x.view(-1)","execution_count":74,"outputs":[]},{"metadata":{"_uuid":"f144e459-53bb-4b5a-860d-abec117fc2cc","_cell_guid":"07870588-c88f-42ff-b782-5ae3c5e6e568","trusted":true},"cell_type":"code","source":"class Surface_Classifier(nn.Module):\n    def __init__(self, raw_ni, no, drop=.5):\n        super().__init__()\n        \n        self.conv1d_channel_0 = nn.Sequential(\n            nn.Conv1d(1, 8, 16, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n\n            nn.Conv1d(8, 16, 11, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n            \n            nn.Conv1d(16, 24, 8, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.Conv1d(24, 32, 5, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n        )\n            \n            \n        \n        self.conv1d_channel_1 = nn.Sequential(\n            nn.Conv1d(1, 8, 16, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n\n            nn.Conv1d(8, 16, 11, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n            \n            nn.Conv1d(16, 24, 8, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.Conv1d(24, 32, 5, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n        )\n        \n        self.conv1d_channel_2 = nn.Sequential(\n            nn.Conv1d(1, 8, 16, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n\n            nn.Conv1d(8, 16, 11, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n            \n            nn.Conv1d(16, 24, 8, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.Conv1d(24, 32, 5, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n        )\n        \n        self.conv1d_channel_3 = nn.Sequential(\n             nn.Conv1d(1, 8, 16, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n\n            nn.Conv1d(8, 16, 11, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n            \n            nn.Conv1d(16, 24, 8, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.Conv1d(24, 32, 5, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n        )\n        \n        self.conv1d_channel_4 = nn.Sequential(\n            nn.Conv1d(1, 8, 16, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n\n            nn.Conv1d(8, 16, 11, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n            \n            nn.Conv1d(16, 24, 8, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.Conv1d(24, 32, 5, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n        )\n        \n        self.conv1d_channel_5 = nn.Sequential(\n            nn.Conv1d(1, 8, 16, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n\n            nn.Conv1d(8, 16, 11, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n            \n            nn.Conv1d(16, 24, 8, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.Conv1d(24, 32, 5, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n        )\n        \n        self.conv1d_channel_6 = nn.Sequential(\n            nn.Conv1d(1, 8, 16, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n\n            nn.Conv1d(8, 16, 11, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n            \n            nn.Conv1d(16, 24, 8, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.Conv1d(24, 32, 5, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n           \n        )\n        \n        self.conv1d_channel_7 = nn.Sequential(\n            nn.Conv1d(1, 8, 16, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n\n            nn.Conv1d(8, 16, 11, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n            \n            nn.Conv1d(16, 24, 8, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.Conv1d(24, 32, 5, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n        )\n            \n\n       \n        \n        self.conv1d_channel_8 = nn.Sequential(\n            nn.Conv1d(1, 8, 16, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n\n            nn.Conv1d(8, 16, 11, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n            \n            nn.Conv1d(16, 24, 8, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.Conv1d(24, 32, 5, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n        )\n\n            \n\n        self.dense = nn.Sequential(\n            nn.Linear(288, 84),  nn.ReLU(),nn.Dropout(),\n            nn.Linear(84, 36),  nn.ReLU(),nn.Dropout(),\n            nn.Linear(36, no),  nn.ReLU()\n        )\n        \n            \n\n    def forward(self, t_channel_0, t_channel_1, t_channel_2, t_channel_3, t_channel_4, t_channel_5, t_channel_6, t_channel_7, t_channel_8):\n        conv1d_out_channel_0 = self.conv1d_channel_0(t_channel_0)\n        conv1d_out_channel_1 = self.conv1d_channel_1(t_channel_1)\n        conv1d_out_channel_2 = self.conv1d_channel_2(t_channel_2)\n        conv1d_out_channel_3 = self.conv1d_channel_3(t_channel_3)\n        conv1d_out_channel_4 = self.conv1d_channel_4(t_channel_4)\n        conv1d_out_channel_5 = self.conv1d_channel_5(t_channel_5)\n        conv1d_out_channel_6 = self.conv1d_channel_6(t_channel_6)\n        conv1d_out_channel_7 = self.conv1d_channel_7(t_channel_7)\n        conv1d_out_channel_8 = self.conv1d_channel_8(t_channel_8)\n        \n        t_in = torch.cat([conv1d_out_channel_0,conv1d_out_channel_1, conv1d_out_channel_2, conv1d_out_channel_3, conv1d_out_channel_4, conv1d_out_channel_5, conv1d_out_channel_6, conv1d_out_channel_7, conv1d_out_channel_8], dim=1)\n        res = t_in.view(t_in.size(0), -1)\n        out = self.dense(res)\n        return out\n        ","execution_count":75,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"5\">**Initial setup for random seed and project path**</font><br>"},{"metadata":{"_uuid":"b4b74c47-df3d-4201-93a7-f8cb79d6a652","_cell_guid":"aa2ea0fc-dfc2-436e-9a73-aab6bfb63d20","trusted":true},"cell_type":"code","source":"\n# set up the seed\nseed = 1\nnp.random.seed(seed)\n\n# path to sample submission\nsample = Path.cwd().parent/'input'/'career-con-2019'/'sample_submission.csv'\n\nROOT = Path.cwd().parent/'input'/'career-con-2019-preprocessed-data'\nraw_arr = np.load(ROOT/'feat.npy').transpose(0, 2, 1)\ntarget = np.load(ROOT/'target.npy')\n\nraw_dim_data = [None]*9\n#print(raw_dim_data)\nfor i in range(0, 9):\n    raw_dim_data[i] = raw_arr[:,i,:]\n#    print(\"raw data shape\")\n    \n    raw_dim_data[i] = raw_dim_data[i].reshape([7626,1,128])\n#    print(raw_dim_data[i].shape)\n    \n# print(\"raw array shape\")\n# print(raw_arr.shape)\n# print(\"label array shape\")\n# print(target.shape)\n\ntrn_sz = 3810  # only the first `trn_sz` rows in each array include labelled data\n#datasets = create_datasets((raw_arr), target, trn_sz, seed=seed)\ndatasets = create_datasets2((raw_dim_data), trn_sz, seed=seed)\n\n\n# make sure that we run on a proper device (not relevant for Kaggle kernels but helpful in Jupyter sessions)\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\n\ntrn_dl, val_dl, tst_dl = create_loaders(datasets, bs=256)","execution_count":76,"outputs":[{"output_type":"stream","text":"total data length\n3810\ninput one dimension shape\n(7626, 1, 128)\ntraining data length\n2286\nvalidation data le\n762\ntesting d\n762\n","name":"stdout"}]},{"metadata":{"_uuid":"b9026a32-1355-4689-8bf9-581a5228af69","_cell_guid":"5db66d39-80aa-48f4-9aab-4f44eace0192","trusted":true},"cell_type":"markdown","source":"<font size=\"5\">**Training model**</font><br>\nNow everything is ready to create a training loop and see if our model works. For each training epoch, the loop performs the following actions:\n1. train model on the `trn_ds` dataset;\n2. verify quality on the `val_ds` dataset;\n3. check if the quality improved since previous epoch, and if so, save the model's weights onto disk;\n4. in case if the model's quality isn't impoving for `patience` epochs, the training is stopped.\nAlso, the code tracks loss and accuracy history, and prints current scores with exponentially increasing logging frequency, i.e., only at 1, 2, 4, 8... epochs."},{"metadata":{"_uuid":"dee51d07-1cff-4db3-a87d-bebc4bf5bcc0","_cell_guid":"72095ada-fe94-4757-9a8a-918481b04336","trusted":true},"cell_type":"code","source":"raw_feat = raw_arr.shape[1]\n\n\nlr = 0.002\nn_epochs = 10000\niterations_per_epoch = len(trn_dl)\nnum_classes = 9\nbest_acc = 0\npatience, trials = 500, 0\nbase = 1\nstep = 2\nloss_history = []\nacc_history = []\n\nmodel = Surface_Classifier(raw_feat, num_classes).to(device)\nmodel.cuda()\ncriterion = nn.CrossEntropyLoss(reduction='sum')\nopt = optim.Adam(model.parameters(), lr=lr)\n\nprint('Start model training')\n\nfor epoch in range(1, n_epochs + 1):\n    \n    model.train()\n    epoch_loss = 0\n    for i, batch in enumerate(trn_dl):\n        #x_raw, y_batch = [t.to(device) for t in batch]\n        x_channel_0, x_channel_1,  x_channel_2,  x_channel_3,  x_channel_4,  x_channel_5,  x_channel_6,  x_channel_7,  x_channel_8, y_batch = [t.to(device) for t in batch]\n        opt.zero_grad()\n        \n#         print(\"channel 0 type\")\n#         print(type(x_channel_0))\n#         print(\"channel 0 shape\")\n#         print(x_channel_0.shape)\n#         print(\"batch type\")\n#         print(type(batch))\n#         print(len(batch))\n#         print(batch[0].shape)\n#         print(batch[9].shape)\n\n        out = model(x_channel_0, x_channel_1, x_channel_2, x_channel_3, x_channel_4, x_channel_5, x_channel_6, x_channel_7, x_channel_8)\n        \n        \n#         out = []\n#         with torch.no_grad():\n#             for x_channel_0, x_channel_1,  x_channel_2,  x_channel_3,  x_channel_4,  x_channel_5,  x_channel_6,  x_channel_7,  x_channel_8, y_batch in batch:\n#                 output = model(x_channel_0, x_channel_1, x_channel_2, x_channel_3, x_channel_4, x_channel_5, x_channel_6, x_channel_7, x_channel_8)\n#                 out.append(output.detach())\n#         out = torch.cat(out)\n        \n\n    \n        loss = criterion(out, y_batch)\n        epoch_loss += loss.item()\n        loss.backward()\n        opt.step()\n        \n    epoch_loss /= trn_sz\n    loss_history.append(epoch_loss)\n    \n    model.eval()\n    correct, total = 0, 0\n    \n    for batch in val_dl:\n        x_channel_0, x_channel_1,  x_channel_2,  x_channel_3,  x_channel_4,  x_channel_5,  x_channel_6,  x_channel_7,  x_channel_8, y_batch = [t.to(device) for t in batch]\n        \n        out = model(x_channel_0, x_channel_1, x_channel_2, x_channel_3, x_channel_4, x_channel_5, x_channel_6, x_channel_7, x_channel_8)\n        preds = F.log_softmax(out, dim=1).argmax(dim=1)\n        total += y_batch.size(0)\n        correct += (preds == y_batch).sum().item()\n    \n    acc = correct / total\n    acc_history.append(acc)\n\n    if epoch % base == 0:\n        print(f'Epoch: {epoch:3d}. Loss: {epoch_loss:.4f}. Acc.: {acc:2.2%}')\n        base *= step\n    print(f'Epoch: {epoch:3d}. Loss: {epoch_loss:.4f}. Acc.: {acc:2.2%}')\n\n    \n    if acc > best_acc:\n        trials = 0\n        best_acc = acc\n        torch.save(model.state_dict(), 'best.pth')\n        print(f'Epoch {epoch} best model saved with accuracy: {best_acc:2.2%}')\n    else:\n        trials += 1\n        if trials >= patience:\n            print(f'Early stopping on epoch {epoch}')\n            break\n            \nprint('Done!')","execution_count":null,"outputs":[{"output_type":"stream","text":"Start model training\nEpoch:   1. Loss: 1.3139. Acc.: 21.78%\nEpoch:   1. Loss: 1.3139. Acc.: 21.78%\nEpoch 1 best model saved with accuracy: 21.78%\nEpoch:   2. Loss: 1.2944. Acc.: 21.78%\nEpoch:   2. Loss: 1.2944. Acc.: 21.78%\nEpoch:   3. Loss: 1.2618. Acc.: 21.78%\nEpoch:   4. Loss: 1.2461. Acc.: 28.48%\nEpoch:   4. Loss: 1.2461. Acc.: 28.48%\nEpoch 4 best model saved with accuracy: 28.48%\nEpoch:   5. Loss: 1.2251. Acc.: 38.71%\nEpoch 5 best model saved with accuracy: 38.71%\nEpoch:   6. Loss: 1.2085. Acc.: 37.40%\nEpoch:   7. Loss: 1.1964. Acc.: 36.09%\nEpoch:   8. Loss: 1.1653. Acc.: 33.86%\nEpoch:   8. Loss: 1.1653. Acc.: 33.86%\nEpoch:   9. Loss: 1.1543. Acc.: 35.70%\nEpoch:  10. Loss: 1.1343. Acc.: 36.09%\nEpoch:  11. Loss: 1.1247. Acc.: 33.86%\nEpoch:  12. Loss: 1.1141. Acc.: 33.99%\nEpoch:  13. Loss: 1.1176. Acc.: 37.80%\nEpoch:  14. Loss: 1.1067. Acc.: 32.55%\nEpoch:  15. Loss: 1.1036. Acc.: 37.27%\nEpoch:  16. Loss: 1.0960. Acc.: 36.75%\nEpoch:  16. Loss: 1.0960. Acc.: 36.75%\nEpoch:  17. Loss: 1.0887. Acc.: 35.17%\nEpoch:  18. Loss: 1.0904. Acc.: 35.83%\nEpoch:  19. Loss: 1.0876. Acc.: 36.48%\nEpoch:  20. Loss: 1.0688. Acc.: 40.55%\nEpoch 20 best model saved with accuracy: 40.55%\nEpoch:  21. Loss: 1.0745. Acc.: 40.29%\nEpoch:  22. Loss: 1.0619. Acc.: 36.61%\nEpoch:  23. Loss: 1.0537. Acc.: 42.13%\nEpoch 23 best model saved with accuracy: 42.13%\nEpoch:  24. Loss: 1.0535. Acc.: 37.40%\nEpoch:  25. Loss: 1.0472. Acc.: 40.42%\nEpoch:  26. Loss: 1.0332. Acc.: 39.24%\nEpoch:  27. Loss: 1.0230. Acc.: 41.73%\nEpoch:  28. Loss: 1.0164. Acc.: 39.11%\nEpoch:  29. Loss: 1.0176. Acc.: 40.81%\nEpoch:  30. Loss: 1.0195. Acc.: 41.34%\nEpoch:  31. Loss: 1.0076. Acc.: 42.52%\nEpoch 31 best model saved with accuracy: 42.52%\nEpoch:  32. Loss: 1.0012. Acc.: 41.60%\nEpoch:  32. Loss: 1.0012. Acc.: 41.60%\nEpoch:  33. Loss: 1.0091. Acc.: 40.55%\nEpoch:  34. Loss: 0.9892. Acc.: 40.81%\nEpoch:  35. Loss: 0.9987. Acc.: 40.94%\nEpoch:  36. Loss: 0.9852. Acc.: 41.34%\nEpoch:  37. Loss: 0.9837. Acc.: 42.91%\nEpoch 37 best model saved with accuracy: 42.91%\nEpoch:  38. Loss: 0.9852. Acc.: 41.73%\nEpoch:  39. Loss: 0.9735. Acc.: 42.65%\nEpoch:  40. Loss: 0.9731. Acc.: 41.99%\nEpoch:  41. Loss: 0.9775. Acc.: 43.83%\nEpoch 41 best model saved with accuracy: 43.83%\nEpoch:  42. Loss: 0.9699. Acc.: 41.73%\nEpoch:  43. Loss: 0.9735. Acc.: 43.04%\nEpoch:  44. Loss: 0.9583. Acc.: 42.78%\nEpoch:  45. Loss: 0.9625. Acc.: 42.78%\nEpoch:  46. Loss: 0.9568. Acc.: 43.83%\nEpoch:  47. Loss: 0.9655. Acc.: 42.52%\nEpoch:  48. Loss: 0.9570. Acc.: 44.88%\nEpoch 48 best model saved with accuracy: 44.88%\nEpoch:  49. Loss: 0.9432. Acc.: 42.65%\nEpoch:  50. Loss: 0.9402. Acc.: 42.91%\nEpoch:  51. Loss: 0.9443. Acc.: 44.36%\nEpoch:  52. Loss: 0.9461. Acc.: 42.39%\nEpoch:  53. Loss: 0.9372. Acc.: 41.86%\nEpoch:  54. Loss: 0.9364. Acc.: 43.44%\nEpoch:  55. Loss: 0.9292. Acc.: 43.18%\nEpoch:  56. Loss: 0.9301. Acc.: 42.52%\nEpoch:  57. Loss: 0.9227. Acc.: 44.36%\nEpoch:  58. Loss: 0.9208. Acc.: 46.46%\nEpoch 58 best model saved with accuracy: 46.46%\nEpoch:  59. Loss: 0.9201. Acc.: 43.70%\nEpoch:  60. Loss: 0.9073. Acc.: 45.67%\nEpoch:  61. Loss: 0.9172. Acc.: 47.11%\nEpoch 61 best model saved with accuracy: 47.11%\nEpoch:  62. Loss: 0.9140. Acc.: 45.14%\nEpoch:  63. Loss: 0.9198. Acc.: 46.98%\nEpoch:  64. Loss: 0.9001. Acc.: 45.67%\nEpoch:  64. Loss: 0.9001. Acc.: 45.67%\nEpoch:  65. Loss: 0.9063. Acc.: 44.88%\nEpoch:  66. Loss: 0.9079. Acc.: 45.28%\nEpoch:  67. Loss: 0.8877. Acc.: 48.95%\nEpoch 67 best model saved with accuracy: 48.95%\nEpoch:  68. Loss: 0.9036. Acc.: 45.41%\nEpoch:  69. Loss: 0.8983. Acc.: 48.43%\nEpoch:  70. Loss: 0.8846. Acc.: 49.61%\nEpoch 70 best model saved with accuracy: 49.61%\nEpoch:  71. Loss: 0.8881. Acc.: 49.21%\nEpoch:  72. Loss: 0.8670. Acc.: 50.26%\nEpoch 72 best model saved with accuracy: 50.26%\nEpoch:  73. Loss: 0.8668. Acc.: 47.64%\nEpoch:  74. Loss: 0.8695. Acc.: 44.88%\nEpoch:  75. Loss: 0.8694. Acc.: 49.87%\nEpoch:  76. Loss: 0.8585. Acc.: 51.05%\nEpoch 76 best model saved with accuracy: 51.05%\nEpoch:  77. Loss: 0.8683. Acc.: 48.29%\nEpoch:  78. Loss: 0.8608. Acc.: 50.00%\nEpoch:  79. Loss: 0.8605. Acc.: 47.77%\nEpoch:  80. Loss: 0.8528. Acc.: 48.82%\nEpoch:  81. Loss: 0.8536. Acc.: 49.21%\nEpoch:  82. Loss: 0.8556. Acc.: 47.38%\nEpoch:  83. Loss: 0.8519. Acc.: 49.87%\nEpoch:  84. Loss: 0.8397. Acc.: 50.79%\nEpoch:  85. Loss: 0.8347. Acc.: 53.28%\nEpoch 85 best model saved with accuracy: 53.28%\nEpoch:  86. Loss: 0.8175. Acc.: 53.28%\nEpoch:  87. Loss: 0.8227. Acc.: 51.84%\nEpoch:  88. Loss: 0.8192. Acc.: 51.05%\nEpoch:  89. Loss: 0.8299. Acc.: 52.10%\nEpoch:  90. Loss: 0.8314. Acc.: 51.18%\nEpoch:  91. Loss: 0.8152. Acc.: 52.36%\nEpoch:  92. Loss: 0.8146. Acc.: 55.91%\nEpoch 92 best model saved with accuracy: 55.91%\nEpoch:  93. Loss: 0.8280. Acc.: 49.87%\nEpoch:  94. Loss: 0.8061. Acc.: 53.15%\nEpoch:  95. Loss: 0.8078. Acc.: 51.44%\nEpoch:  96. Loss: 0.8011. Acc.: 54.07%\nEpoch:  97. Loss: 0.8020. Acc.: 48.82%\nEpoch:  98. Loss: 0.8056. Acc.: 56.17%\nEpoch 98 best model saved with accuracy: 56.17%\nEpoch:  99. Loss: 0.7896. Acc.: 51.05%\nEpoch: 100. Loss: 0.8036. Acc.: 52.23%\nEpoch: 101. Loss: 0.8094. Acc.: 54.33%\nEpoch: 102. Loss: 0.7962. Acc.: 55.51%\nEpoch: 103. Loss: 0.7993. Acc.: 51.05%\nEpoch: 104. Loss: 0.7975. Acc.: 54.86%\nEpoch: 105. Loss: 0.7939. Acc.: 54.46%\nEpoch: 106. Loss: 0.7955. Acc.: 54.86%\nEpoch: 107. Loss: 0.7905. Acc.: 53.28%\nEpoch: 108. Loss: 0.7835. Acc.: 57.35%\nEpoch 108 best model saved with accuracy: 57.35%\nEpoch: 109. Loss: 0.7858. Acc.: 53.94%\nEpoch: 110. Loss: 0.7760. Acc.: 57.74%\nEpoch 110 best model saved with accuracy: 57.74%\nEpoch: 111. Loss: 0.7758. Acc.: 54.33%\nEpoch: 112. Loss: 0.7903. Acc.: 53.02%\nEpoch: 113. Loss: 0.7940. Acc.: 54.33%\nEpoch: 114. Loss: 0.7739. Acc.: 55.38%\nEpoch: 115. Loss: 0.7729. Acc.: 53.15%\nEpoch: 116. Loss: 0.7903. Acc.: 57.48%\nEpoch: 117. Loss: 0.7804. Acc.: 53.54%\nEpoch: 118. Loss: 0.7651. Acc.: 54.99%\nEpoch: 119. Loss: 0.7761. Acc.: 55.64%\nEpoch: 120. Loss: 0.7547. Acc.: 54.59%\nEpoch: 121. Loss: 0.7609. Acc.: 55.51%\nEpoch: 122. Loss: 0.7666. Acc.: 53.02%\nEpoch: 123. Loss: 0.7573. Acc.: 55.12%\nEpoch: 124. Loss: 0.7721. Acc.: 56.69%\nEpoch: 125. Loss: 0.7581. Acc.: 54.59%\nEpoch: 126. Loss: 0.7662. Acc.: 56.04%\nEpoch: 127. Loss: 0.7595. Acc.: 54.33%\nEpoch: 128. Loss: 0.7532. Acc.: 57.22%\nEpoch: 128. Loss: 0.7532. Acc.: 57.22%\nEpoch: 129. Loss: 0.7738. Acc.: 53.41%\nEpoch: 130. Loss: 0.7749. Acc.: 55.91%\nEpoch: 131. Loss: 0.7701. Acc.: 55.12%\nEpoch: 132. Loss: 0.7430. Acc.: 57.35%\nEpoch: 133. Loss: 0.7511. Acc.: 56.17%\nEpoch: 134. Loss: 0.7487. Acc.: 56.82%\nEpoch: 135. Loss: 0.7628. Acc.: 57.48%\nEpoch: 136. Loss: 0.7597. Acc.: 55.51%\nEpoch: 137. Loss: 0.7489. Acc.: 54.99%\nEpoch: 138. Loss: 0.7607. Acc.: 58.66%\nEpoch 138 best model saved with accuracy: 58.66%\nEpoch: 139. Loss: 0.7618. Acc.: 53.67%\nEpoch: 140. Loss: 0.7428. Acc.: 58.27%\nEpoch: 141. Loss: 0.7475. Acc.: 56.43%\nEpoch: 142. Loss: 0.7382. Acc.: 56.96%\nEpoch: 143. Loss: 0.7409. Acc.: 56.82%\nEpoch: 144. Loss: 0.7307. Acc.: 56.56%\nEpoch: 145. Loss: 0.7453. Acc.: 54.33%\nEpoch: 146. Loss: 0.7435. Acc.: 58.53%\nEpoch: 147. Loss: 0.7416. Acc.: 57.22%\nEpoch: 148. Loss: 0.7477. Acc.: 56.30%\nEpoch: 149. Loss: 0.7520. Acc.: 57.22%\nEpoch: 150. Loss: 0.7484. Acc.: 56.82%\nEpoch: 151. Loss: 0.7520. Acc.: 57.09%\nEpoch: 152. Loss: 0.7263. Acc.: 57.22%\nEpoch: 153. Loss: 0.7488. Acc.: 53.94%\nEpoch: 154. Loss: 0.7413. Acc.: 56.69%\nEpoch: 155. Loss: 0.7414. Acc.: 52.62%\nEpoch: 156. Loss: 0.7362. Acc.: 56.56%\nEpoch: 157. Loss: 0.7348. Acc.: 53.67%\nEpoch: 158. Loss: 0.7454. Acc.: 56.17%\nEpoch: 159. Loss: 0.7484. Acc.: 59.06%\nEpoch 159 best model saved with accuracy: 59.06%\nEpoch: 160. Loss: 0.7385. Acc.: 56.04%\nEpoch: 161. Loss: 0.7497. Acc.: 52.76%\nEpoch: 162. Loss: 0.7361. Acc.: 56.96%\nEpoch: 163. Loss: 0.7472. Acc.: 55.51%\nEpoch: 164. Loss: 0.7299. Acc.: 56.17%\nEpoch: 165. Loss: 0.7441. Acc.: 55.25%\nEpoch: 166. Loss: 0.7404. Acc.: 55.64%\nEpoch: 167. Loss: 0.7294. Acc.: 54.86%\nEpoch: 168. Loss: 0.7150. Acc.: 58.14%\nEpoch: 169. Loss: 0.7268. Acc.: 54.20%\nEpoch: 170. Loss: 0.7369. Acc.: 56.96%\nEpoch: 171. Loss: 0.7527. Acc.: 55.51%\nEpoch: 172. Loss: 0.7298. Acc.: 57.09%\nEpoch: 173. Loss: 0.7368. Acc.: 57.35%\nEpoch: 174. Loss: 0.7273. Acc.: 55.91%\nEpoch: 175. Loss: 0.7135. Acc.: 59.32%\nEpoch 175 best model saved with accuracy: 59.32%\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 176. Loss: 0.7232. Acc.: 58.40%\nEpoch: 177. Loss: 0.7333. Acc.: 53.81%\nEpoch: 178. Loss: 0.7272. Acc.: 58.66%\nEpoch: 179. Loss: 0.7180. Acc.: 52.23%\nEpoch: 180. Loss: 0.7341. Acc.: 59.45%\nEpoch 180 best model saved with accuracy: 59.45%\nEpoch: 181. Loss: 0.7243. Acc.: 55.25%\nEpoch: 182. Loss: 0.7431. Acc.: 58.53%\nEpoch: 183. Loss: 0.7247. Acc.: 53.28%\nEpoch: 184. Loss: 0.7121. Acc.: 55.25%\nEpoch: 185. Loss: 0.7129. Acc.: 54.33%\nEpoch: 186. Loss: 0.7118. Acc.: 58.01%\nEpoch: 187. Loss: 0.7187. Acc.: 54.46%\nEpoch: 188. Loss: 0.7178. Acc.: 58.14%\nEpoch: 189. Loss: 0.7108. Acc.: 57.35%\nEpoch: 190. Loss: 0.7087. Acc.: 55.12%\nEpoch: 191. Loss: 0.7031. Acc.: 58.01%\nEpoch: 192. Loss: 0.7411. Acc.: 57.48%\nEpoch: 193. Loss: 0.7244. Acc.: 54.72%\nEpoch: 194. Loss: 0.7147. Acc.: 55.77%\nEpoch: 195. Loss: 0.7139. Acc.: 55.51%\nEpoch: 196. Loss: 0.7038. Acc.: 56.17%\nEpoch: 197. Loss: 0.7274. Acc.: 57.61%\nEpoch: 198. Loss: 0.7211. Acc.: 57.09%\nEpoch: 199. Loss: 0.7219. Acc.: 57.09%\nEpoch: 200. Loss: 0.7156. Acc.: 57.22%\nEpoch: 201. Loss: 0.7183. Acc.: 55.91%\nEpoch: 202. Loss: 0.7175. Acc.: 56.82%\nEpoch: 203. Loss: 0.7062. Acc.: 57.61%\nEpoch: 204. Loss: 0.6947. Acc.: 54.72%\nEpoch: 205. Loss: 0.7117. Acc.: 58.40%\nEpoch: 206. Loss: 0.7058. Acc.: 57.74%\nEpoch: 207. Loss: 0.7121. Acc.: 55.38%\nEpoch: 208. Loss: 0.7234. Acc.: 58.79%\nEpoch: 209. Loss: 0.6990. Acc.: 50.13%\nEpoch: 210. Loss: 0.7093. Acc.: 59.19%\nEpoch: 211. Loss: 0.7069. Acc.: 52.36%\nEpoch: 212. Loss: 0.7166. Acc.: 58.92%\nEpoch: 213. Loss: 0.7236. Acc.: 52.76%\nEpoch: 214. Loss: 0.7011. Acc.: 60.37%\nEpoch 214 best model saved with accuracy: 60.37%\nEpoch: 215. Loss: 0.7158. Acc.: 56.82%\nEpoch: 216. Loss: 0.7087. Acc.: 57.48%\nEpoch: 217. Loss: 0.7091. Acc.: 57.22%\nEpoch: 218. Loss: 0.7075. Acc.: 56.30%\nEpoch: 219. Loss: 0.6970. Acc.: 55.77%\nEpoch: 220. Loss: 0.6955. Acc.: 56.69%\nEpoch: 221. Loss: 0.7016. Acc.: 57.35%\nEpoch: 222. Loss: 0.6946. Acc.: 55.38%\nEpoch: 223. Loss: 0.7046. Acc.: 58.79%\nEpoch: 224. Loss: 0.7013. Acc.: 58.14%\nEpoch: 225. Loss: 0.6979. Acc.: 57.48%\nEpoch: 226. Loss: 0.6895. Acc.: 55.91%\nEpoch: 227. Loss: 0.7183. Acc.: 55.25%\nEpoch: 228. Loss: 0.7016. Acc.: 56.69%\nEpoch: 229. Loss: 0.6970. Acc.: 56.04%\nEpoch: 230. Loss: 0.7015. Acc.: 58.27%\nEpoch: 231. Loss: 0.6954. Acc.: 54.07%\nEpoch: 232. Loss: 0.6882. Acc.: 58.14%\nEpoch: 233. Loss: 0.6955. Acc.: 52.89%\nEpoch: 234. Loss: 0.6986. Acc.: 56.43%\nEpoch: 235. Loss: 0.6903. Acc.: 56.17%\nEpoch: 236. Loss: 0.6876. Acc.: 53.67%\nEpoch: 237. Loss: 0.6972. Acc.: 54.46%\nEpoch: 238. Loss: 0.6788. Acc.: 57.87%\nEpoch: 239. Loss: 0.6910. Acc.: 54.59%\nEpoch: 240. Loss: 0.6921. Acc.: 56.04%\nEpoch: 241. Loss: 0.6838. Acc.: 58.40%\nEpoch: 242. Loss: 0.6853. Acc.: 54.99%\nEpoch: 243. Loss: 0.6882. Acc.: 57.35%\nEpoch: 244. Loss: 0.6796. Acc.: 56.69%\nEpoch: 245. Loss: 0.6616. Acc.: 55.91%\nEpoch: 246. Loss: 0.6878. Acc.: 53.67%\nEpoch: 247. Loss: 0.6793. Acc.: 56.17%\nEpoch: 248. Loss: 0.6886. Acc.: 55.25%\nEpoch: 249. Loss: 0.6923. Acc.: 53.81%\nEpoch: 250. Loss: 0.6752. Acc.: 55.25%\nEpoch: 251. Loss: 0.6910. Acc.: 54.33%\nEpoch: 252. Loss: 0.6762. Acc.: 56.82%\nEpoch: 253. Loss: 0.6816. Acc.: 58.27%\nEpoch: 254. Loss: 0.6817. Acc.: 55.64%\nEpoch: 255. Loss: 0.6947. Acc.: 59.06%\nEpoch: 256. Loss: 0.6737. Acc.: 56.04%\nEpoch: 256. Loss: 0.6737. Acc.: 56.04%\nEpoch: 257. Loss: 0.6743. Acc.: 58.14%\nEpoch: 258. Loss: 0.6761. Acc.: 55.51%\nEpoch: 259. Loss: 0.6883. Acc.: 60.10%\nEpoch: 260. Loss: 0.6897. Acc.: 53.94%\nEpoch: 261. Loss: 0.6879. Acc.: 56.69%\nEpoch: 262. Loss: 0.6711. Acc.: 53.94%\nEpoch: 263. Loss: 0.6670. Acc.: 56.96%\nEpoch: 264. Loss: 0.6796. Acc.: 54.86%\nEpoch: 265. Loss: 0.6815. Acc.: 59.19%\nEpoch: 266. Loss: 0.6720. Acc.: 55.64%\nEpoch: 267. Loss: 0.6877. Acc.: 54.59%\nEpoch: 268. Loss: 0.6816. Acc.: 56.30%\nEpoch: 269. Loss: 0.6746. Acc.: 56.69%\nEpoch: 270. Loss: 0.6680. Acc.: 57.61%\nEpoch: 271. Loss: 0.6839. Acc.: 59.97%\nEpoch: 272. Loss: 0.6690. Acc.: 54.59%\nEpoch: 273. Loss: 0.6837. Acc.: 57.61%\nEpoch: 274. Loss: 0.6550. Acc.: 55.25%\nEpoch: 275. Loss: 0.6694. Acc.: 57.22%\nEpoch: 276. Loss: 0.6904. Acc.: 56.30%\nEpoch: 277. Loss: 0.6705. Acc.: 55.12%\nEpoch: 278. Loss: 0.6552. Acc.: 57.22%\nEpoch: 279. Loss: 0.6739. Acc.: 57.09%\nEpoch: 280. Loss: 0.6521. Acc.: 56.04%\nEpoch: 281. Loss: 0.6608. Acc.: 59.84%\nEpoch: 282. Loss: 0.6718. Acc.: 53.15%\nEpoch: 283. Loss: 0.6839. Acc.: 56.43%\nEpoch: 284. Loss: 0.6893. Acc.: 56.69%\nEpoch: 285. Loss: 0.6778. Acc.: 55.64%\nEpoch: 286. Loss: 0.6722. Acc.: 55.12%\nEpoch: 287. Loss: 0.6716. Acc.: 54.99%\nEpoch: 288. Loss: 0.6765. Acc.: 53.81%\nEpoch: 289. Loss: 0.6783. Acc.: 57.74%\nEpoch: 290. Loss: 0.6668. Acc.: 54.72%\nEpoch: 291. Loss: 0.6526. Acc.: 58.27%\nEpoch: 292. Loss: 0.6581. Acc.: 53.54%\nEpoch: 293. Loss: 0.6604. Acc.: 58.66%\nEpoch: 294. Loss: 0.6478. Acc.: 55.64%\nEpoch: 295. Loss: 0.6562. Acc.: 55.12%\nEpoch: 296. Loss: 0.6615. Acc.: 56.04%\nEpoch: 297. Loss: 0.6520. Acc.: 55.91%\nEpoch: 298. Loss: 0.6623. Acc.: 54.20%\nEpoch: 299. Loss: 0.6436. Acc.: 58.53%\nEpoch: 300. Loss: 0.6496. Acc.: 54.46%\nEpoch: 301. Loss: 0.6548. Acc.: 55.77%\nEpoch: 302. Loss: 0.6471. Acc.: 54.07%\nEpoch: 303. Loss: 0.6545. Acc.: 55.91%\nEpoch: 304. Loss: 0.6644. Acc.: 55.38%\nEpoch: 305. Loss: 0.6614. Acc.: 57.61%\nEpoch: 306. Loss: 0.6507. Acc.: 59.32%\nEpoch: 307. Loss: 0.6539. Acc.: 55.64%\nEpoch: 308. Loss: 0.6480. Acc.: 58.79%\nEpoch: 309. Loss: 0.6384. Acc.: 54.99%\nEpoch: 310. Loss: 0.6548. Acc.: 58.53%\nEpoch: 311. Loss: 0.6588. Acc.: 55.64%\nEpoch: 312. Loss: 0.6604. Acc.: 57.35%\nEpoch: 313. Loss: 0.6427. Acc.: 56.96%\nEpoch: 314. Loss: 0.6407. Acc.: 57.09%\nEpoch: 315. Loss: 0.6434. Acc.: 57.09%\nEpoch: 316. Loss: 0.6556. Acc.: 58.53%\nEpoch: 317. Loss: 0.6624. Acc.: 54.33%\nEpoch: 318. Loss: 0.6407. Acc.: 56.04%\nEpoch: 319. Loss: 0.6436. Acc.: 55.12%\nEpoch: 320. Loss: 0.6509. Acc.: 59.32%\nEpoch: 321. Loss: 0.6580. Acc.: 55.25%\nEpoch: 322. Loss: 0.6606. Acc.: 56.82%\nEpoch: 323. Loss: 0.6536. Acc.: 54.72%\nEpoch: 324. Loss: 0.6269. Acc.: 57.87%\nEpoch: 325. Loss: 0.6313. Acc.: 56.69%\nEpoch: 326. Loss: 0.6508. Acc.: 56.56%\nEpoch: 327. Loss: 0.6544. Acc.: 56.17%\nEpoch: 328. Loss: 0.6648. Acc.: 53.54%\nEpoch: 329. Loss: 0.6664. Acc.: 60.50%\nEpoch 329 best model saved with accuracy: 60.50%\nEpoch: 330. Loss: 0.6336. Acc.: 57.48%\nEpoch: 331. Loss: 0.6557. Acc.: 55.77%\nEpoch: 332. Loss: 0.6571. Acc.: 58.53%\nEpoch: 333. Loss: 0.6542. Acc.: 58.40%\nEpoch: 334. Loss: 0.6496. Acc.: 57.87%\nEpoch: 335. Loss: 0.6437. Acc.: 56.04%\nEpoch: 336. Loss: 0.6373. Acc.: 59.32%\nEpoch: 337. Loss: 0.6317. Acc.: 57.35%\nEpoch: 338. Loss: 0.6396. Acc.: 56.30%\nEpoch: 339. Loss: 0.6386. Acc.: 58.01%\nEpoch: 340. Loss: 0.6524. Acc.: 54.33%\nEpoch: 341. Loss: 0.6495. Acc.: 56.30%\nEpoch: 342. Loss: 0.6489. Acc.: 54.99%\nEpoch: 343. Loss: 0.6332. Acc.: 57.87%\nEpoch: 344. Loss: 0.6358. Acc.: 58.53%\nEpoch: 345. Loss: 0.6423. Acc.: 60.76%\nEpoch 345 best model saved with accuracy: 60.76%\nEpoch: 346. Loss: 0.6496. Acc.: 55.77%\nEpoch: 347. Loss: 0.6338. Acc.: 60.10%\nEpoch: 348. Loss: 0.6547. Acc.: 57.22%\nEpoch: 349. Loss: 0.6380. Acc.: 58.27%\nEpoch: 350. Loss: 0.6473. Acc.: 60.89%\nEpoch 350 best model saved with accuracy: 60.89%\nEpoch: 351. Loss: 0.6501. Acc.: 56.43%\nEpoch: 352. Loss: 0.6642. Acc.: 57.48%\nEpoch: 353. Loss: 0.6543. Acc.: 56.43%\nEpoch: 354. Loss: 0.6384. Acc.: 59.19%\nEpoch: 355. Loss: 0.6287. Acc.: 56.04%\nEpoch: 356. Loss: 0.6439. Acc.: 56.82%\nEpoch: 357. Loss: 0.6370. Acc.: 58.01%\nEpoch: 358. Loss: 0.6355. Acc.: 55.77%\nEpoch: 359. Loss: 0.6302. Acc.: 56.17%\nEpoch: 360. Loss: 0.6374. Acc.: 54.86%\nEpoch: 361. Loss: 0.6655. Acc.: 57.61%\nEpoch: 362. Loss: 0.6484. Acc.: 59.32%\nEpoch: 363. Loss: 0.6412. Acc.: 60.50%\nEpoch: 364. Loss: 0.6418. Acc.: 58.40%\nEpoch: 365. Loss: 0.6465. Acc.: 61.15%\nEpoch 365 best model saved with accuracy: 61.15%\nEpoch: 366. Loss: 0.6257. Acc.: 58.01%\nEpoch: 367. Loss: 0.6401. Acc.: 59.06%\nEpoch: 368. Loss: 0.6434. Acc.: 58.14%\nEpoch: 369. Loss: 0.6205. Acc.: 59.32%\nEpoch: 370. Loss: 0.6338. Acc.: 54.59%\nEpoch: 371. Loss: 0.6407. Acc.: 59.84%\nEpoch: 372. Loss: 0.6225. Acc.: 57.48%\nEpoch: 373. Loss: 0.6345. Acc.: 60.37%\nEpoch: 374. Loss: 0.6259. Acc.: 58.53%\nEpoch: 375. Loss: 0.6345. Acc.: 56.30%\nEpoch: 376. Loss: 0.6398. Acc.: 57.61%\nEpoch: 377. Loss: 0.6340. Acc.: 55.25%\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 378. Loss: 0.6231. Acc.: 59.06%\nEpoch: 379. Loss: 0.6305. Acc.: 57.22%\nEpoch: 380. Loss: 0.6333. Acc.: 59.58%\nEpoch: 381. Loss: 0.6406. Acc.: 57.35%\nEpoch: 382. Loss: 0.6432. Acc.: 57.09%\nEpoch: 383. Loss: 0.6246. Acc.: 58.14%\nEpoch: 384. Loss: 0.6311. Acc.: 58.40%\nEpoch: 385. Loss: 0.6389. Acc.: 59.45%\nEpoch: 386. Loss: 0.6440. Acc.: 55.91%\nEpoch: 387. Loss: 0.6495. Acc.: 59.97%\nEpoch: 388. Loss: 0.6378. Acc.: 56.04%\nEpoch: 389. Loss: 0.6361. Acc.: 57.61%\nEpoch: 390. Loss: 0.6286. Acc.: 54.20%\nEpoch: 391. Loss: 0.6245. Acc.: 58.79%\nEpoch: 392. Loss: 0.6213. Acc.: 56.56%\nEpoch: 393. Loss: 0.6390. Acc.: 59.58%\nEpoch: 394. Loss: 0.6327. Acc.: 56.43%\nEpoch: 395. Loss: 0.6267. Acc.: 58.66%\nEpoch: 396. Loss: 0.6370. Acc.: 52.49%\nEpoch: 397. Loss: 0.6296. Acc.: 58.01%\nEpoch: 398. Loss: 0.6286. Acc.: 55.64%\nEpoch: 399. Loss: 0.6332. Acc.: 59.06%\nEpoch: 400. Loss: 0.6281. Acc.: 59.45%\nEpoch: 401. Loss: 0.6279. Acc.: 58.79%\nEpoch: 402. Loss: 0.6285. Acc.: 58.27%\nEpoch: 403. Loss: 0.6190. Acc.: 60.63%\nEpoch: 404. Loss: 0.6361. Acc.: 58.79%\nEpoch: 405. Loss: 0.6355. Acc.: 58.27%\nEpoch: 406. Loss: 0.6255. Acc.: 58.92%\nEpoch: 407. Loss: 0.6227. Acc.: 61.68%\nEpoch 407 best model saved with accuracy: 61.68%\nEpoch: 408. Loss: 0.6258. Acc.: 59.58%\nEpoch: 409. Loss: 0.6283. Acc.: 58.92%\nEpoch: 410. Loss: 0.6380. Acc.: 56.96%\nEpoch: 411. Loss: 0.6284. Acc.: 59.19%\nEpoch: 412. Loss: 0.6150. Acc.: 56.04%\nEpoch: 413. Loss: 0.6269. Acc.: 60.37%\nEpoch: 414. Loss: 0.6262. Acc.: 58.92%\nEpoch: 415. Loss: 0.6319. Acc.: 60.89%\nEpoch: 416. Loss: 0.6289. Acc.: 58.14%\nEpoch: 417. Loss: 0.6244. Acc.: 58.92%\nEpoch: 418. Loss: 0.6210. Acc.: 59.71%\nEpoch: 419. Loss: 0.6214. Acc.: 58.92%\nEpoch: 420. Loss: 0.6252. Acc.: 59.19%\nEpoch: 421. Loss: 0.6314. Acc.: 55.25%\nEpoch: 422. Loss: 0.6210. Acc.: 59.32%\nEpoch: 423. Loss: 0.6174. Acc.: 56.43%\nEpoch: 424. Loss: 0.6273. Acc.: 58.01%\nEpoch: 425. Loss: 0.6046. Acc.: 58.92%\nEpoch: 426. Loss: 0.6270. Acc.: 53.94%\nEpoch: 427. Loss: 0.6402. Acc.: 58.40%\nEpoch: 428. Loss: 0.6202. Acc.: 59.19%\nEpoch: 429. Loss: 0.6209. Acc.: 56.43%\nEpoch: 430. Loss: 0.6474. Acc.: 58.01%\nEpoch: 431. Loss: 0.6231. Acc.: 58.79%\nEpoch: 432. Loss: 0.6252. Acc.: 57.61%\nEpoch: 433. Loss: 0.6328. Acc.: 55.91%\nEpoch: 434. Loss: 0.6428. Acc.: 59.58%\nEpoch: 435. Loss: 0.6252. Acc.: 57.61%\nEpoch: 436. Loss: 0.6495. Acc.: 57.87%\nEpoch: 437. Loss: 0.6370. Acc.: 56.04%\nEpoch: 438. Loss: 0.6183. Acc.: 56.17%\nEpoch: 439. Loss: 0.6288. Acc.: 54.72%\nEpoch: 440. Loss: 0.6191. Acc.: 59.71%\nEpoch: 441. Loss: 0.6279. Acc.: 55.64%\nEpoch: 442. Loss: 0.6115. Acc.: 57.61%\nEpoch: 443. Loss: 0.6126. Acc.: 59.97%\nEpoch: 444. Loss: 0.6229. Acc.: 54.33%\nEpoch: 445. Loss: 0.6148. Acc.: 57.48%\nEpoch: 446. Loss: 0.6320. Acc.: 56.30%\nEpoch: 447. Loss: 0.6448. Acc.: 58.79%\nEpoch: 448. Loss: 0.6319. Acc.: 56.69%\nEpoch: 449. Loss: 0.6262. Acc.: 56.82%\nEpoch: 450. Loss: 0.6316. Acc.: 59.45%\nEpoch: 451. Loss: 0.6247. Acc.: 55.91%\nEpoch: 452. Loss: 0.6191. Acc.: 59.45%\nEpoch: 453. Loss: 0.6261. Acc.: 56.17%\nEpoch: 454. Loss: 0.6280. Acc.: 57.48%\nEpoch: 455. Loss: 0.6390. Acc.: 58.14%\nEpoch: 456. Loss: 0.6223. Acc.: 55.91%\nEpoch: 457. Loss: 0.6370. Acc.: 57.22%\nEpoch: 458. Loss: 0.6183. Acc.: 54.46%\nEpoch: 459. Loss: 0.6156. Acc.: 56.82%\nEpoch: 460. Loss: 0.6244. Acc.: 55.51%\nEpoch: 461. Loss: 0.6126. Acc.: 59.84%\nEpoch: 462. Loss: 0.6297. Acc.: 55.38%\nEpoch: 463. Loss: 0.6176. Acc.: 58.53%\nEpoch: 464. Loss: 0.6196. Acc.: 58.53%\nEpoch: 465. Loss: 0.6157. Acc.: 56.96%\nEpoch: 466. Loss: 0.6154. Acc.: 56.96%\nEpoch: 467. Loss: 0.6338. Acc.: 55.38%\nEpoch: 468. Loss: 0.6334. Acc.: 58.92%\nEpoch: 469. Loss: 0.6057. Acc.: 56.30%\nEpoch: 470. Loss: 0.6011. Acc.: 60.10%\nEpoch: 471. Loss: 0.6261. Acc.: 55.91%\nEpoch: 472. Loss: 0.6309. Acc.: 58.66%\nEpoch: 473. Loss: 0.6244. Acc.: 56.56%\nEpoch: 474. Loss: 0.6011. Acc.: 58.66%\nEpoch: 475. Loss: 0.6033. Acc.: 54.59%\nEpoch: 476. Loss: 0.6088. Acc.: 58.01%\nEpoch: 477. Loss: 0.6104. Acc.: 57.09%\nEpoch: 478. Loss: 0.6399. Acc.: 57.48%\nEpoch: 479. Loss: 0.5986. Acc.: 58.27%\nEpoch: 480. Loss: 0.6269. Acc.: 57.48%\nEpoch: 481. Loss: 0.6127. Acc.: 57.09%\nEpoch: 482. Loss: 0.6034. Acc.: 58.66%\nEpoch: 483. Loss: 0.6126. Acc.: 57.87%\nEpoch: 484. Loss: 0.6065. Acc.: 59.45%\nEpoch: 485. Loss: 0.6155. Acc.: 59.32%\nEpoch: 486. Loss: 0.6106. Acc.: 59.71%\nEpoch: 487. Loss: 0.5965. Acc.: 60.24%\nEpoch: 488. Loss: 0.6270. Acc.: 56.96%\nEpoch: 489. Loss: 0.6203. Acc.: 60.24%\nEpoch: 490. Loss: 0.6210. Acc.: 56.43%\nEpoch: 491. Loss: 0.6253. Acc.: 56.30%\nEpoch: 492. Loss: 0.6364. Acc.: 56.69%\nEpoch: 493. Loss: 0.6269. Acc.: 56.56%\nEpoch: 494. Loss: 0.6311. Acc.: 55.77%\nEpoch: 495. Loss: 0.6057. Acc.: 59.06%\nEpoch: 496. Loss: 0.5992. Acc.: 58.92%\nEpoch: 497. Loss: 0.6225. Acc.: 55.25%\nEpoch: 498. Loss: 0.6102. Acc.: 58.14%\nEpoch: 499. Loss: 0.6243. Acc.: 58.01%\nEpoch: 500. Loss: 0.6139. Acc.: 56.82%\nEpoch: 501. Loss: 0.6114. Acc.: 59.45%\nEpoch: 502. Loss: 0.6115. Acc.: 59.45%\nEpoch: 503. Loss: 0.6066. Acc.: 55.64%\nEpoch: 504. Loss: 0.6207. Acc.: 60.63%\nEpoch: 505. Loss: 0.5938. Acc.: 60.10%\nEpoch: 506. Loss: 0.5909. Acc.: 55.91%\nEpoch: 507. Loss: 0.6157. Acc.: 59.32%\nEpoch: 508. Loss: 0.6605. Acc.: 58.53%\nEpoch: 509. Loss: 0.6314. Acc.: 57.09%\nEpoch: 510. Loss: 0.6183. Acc.: 59.58%\nEpoch: 511. Loss: 0.6080. Acc.: 58.92%\nEpoch: 512. Loss: 0.6408. Acc.: 59.97%\nEpoch: 512. Loss: 0.6408. Acc.: 59.97%\nEpoch: 513. Loss: 0.6106. Acc.: 57.09%\nEpoch: 514. Loss: 0.6144. Acc.: 58.66%\nEpoch: 515. Loss: 0.6168. Acc.: 59.45%\nEpoch: 516. Loss: 0.6364. Acc.: 56.56%\nEpoch: 517. Loss: 0.5993. Acc.: 60.76%\nEpoch: 518. Loss: 0.6126. Acc.: 56.82%\nEpoch: 519. Loss: 0.6032. Acc.: 59.32%\nEpoch: 520. Loss: 0.6097. Acc.: 58.01%\nEpoch: 521. Loss: 0.6121. Acc.: 54.86%\nEpoch: 522. Loss: 0.6067. Acc.: 58.53%\nEpoch: 523. Loss: 0.6054. Acc.: 61.81%\nEpoch 523 best model saved with accuracy: 61.81%\nEpoch: 524. Loss: 0.6212. Acc.: 54.99%\nEpoch: 525. Loss: 0.6146. Acc.: 60.63%\nEpoch: 526. Loss: 0.6104. Acc.: 60.63%\nEpoch: 527. Loss: 0.6052. Acc.: 58.79%\nEpoch: 528. Loss: 0.6030. Acc.: 58.01%\nEpoch: 529. Loss: 0.5994. Acc.: 56.56%\nEpoch: 530. Loss: 0.6009. Acc.: 58.66%\nEpoch: 531. Loss: 0.6022. Acc.: 58.53%\nEpoch: 532. Loss: 0.6176. Acc.: 56.82%\nEpoch: 533. Loss: 0.6190. Acc.: 59.58%\nEpoch: 534. Loss: 0.6146. Acc.: 59.32%\nEpoch: 535. Loss: 0.6228. Acc.: 56.30%\nEpoch: 536. Loss: 0.6154. Acc.: 57.09%\nEpoch: 537. Loss: 0.6123. Acc.: 58.92%\nEpoch: 538. Loss: 0.6066. Acc.: 57.35%\nEpoch: 539. Loss: 0.5979. Acc.: 57.09%\nEpoch: 540. Loss: 0.5912. Acc.: 55.64%\nEpoch: 541. Loss: 0.6039. Acc.: 59.45%\nEpoch: 542. Loss: 0.6017. Acc.: 58.14%\nEpoch: 543. Loss: 0.6001. Acc.: 57.22%\nEpoch: 544. Loss: 0.6132. Acc.: 57.22%\nEpoch: 545. Loss: 0.6125. Acc.: 57.22%\nEpoch: 546. Loss: 0.5947. Acc.: 58.40%\nEpoch: 547. Loss: 0.6162. Acc.: 57.61%\nEpoch: 548. Loss: 0.6010. Acc.: 57.48%\nEpoch: 549. Loss: 0.6164. Acc.: 57.48%\nEpoch: 550. Loss: 0.6097. Acc.: 59.84%\nEpoch: 551. Loss: 0.6014. Acc.: 58.92%\nEpoch: 552. Loss: 0.5975. Acc.: 58.40%\nEpoch: 553. Loss: 0.5986. Acc.: 58.79%\nEpoch: 554. Loss: 0.6020. Acc.: 58.92%\nEpoch: 555. Loss: 0.6100. Acc.: 56.96%\nEpoch: 556. Loss: 0.6044. Acc.: 58.79%\nEpoch: 557. Loss: 0.6114. Acc.: 61.29%\nEpoch: 558. Loss: 0.6023. Acc.: 58.40%\nEpoch: 559. Loss: 0.5914. Acc.: 61.68%\nEpoch: 560. Loss: 0.5804. Acc.: 60.37%\nEpoch: 561. Loss: 0.6051. Acc.: 58.27%\nEpoch: 562. Loss: 0.5975. Acc.: 59.58%\nEpoch: 563. Loss: 0.6147. Acc.: 59.58%\nEpoch: 564. Loss: 0.6019. Acc.: 60.10%\nEpoch: 565. Loss: 0.5910. Acc.: 61.81%\nEpoch: 566. Loss: 0.5905. Acc.: 59.58%\nEpoch: 567. Loss: 0.5867. Acc.: 59.32%\nEpoch: 568. Loss: 0.6040. Acc.: 58.92%\nEpoch: 569. Loss: 0.6126. Acc.: 59.58%\nEpoch: 570. Loss: 0.5948. Acc.: 59.19%\nEpoch: 571. Loss: 0.5919. Acc.: 58.27%\nEpoch: 572. Loss: 0.5708. Acc.: 61.02%\nEpoch: 573. Loss: 0.5933. Acc.: 57.61%\nEpoch: 574. Loss: 0.5848. Acc.: 57.87%\nEpoch: 575. Loss: 0.5887. Acc.: 57.48%\nEpoch: 576. Loss: 0.6072. Acc.: 58.14%\nEpoch: 577. Loss: 0.6125. Acc.: 58.40%\nEpoch: 578. Loss: 0.6223. Acc.: 60.63%\nEpoch: 579. Loss: 0.5971. Acc.: 61.29%\nEpoch: 580. Loss: 0.5918. Acc.: 58.79%\nEpoch: 581. Loss: 0.5960. Acc.: 58.40%\nEpoch: 582. Loss: 0.6061. Acc.: 60.37%\nEpoch: 583. Loss: 0.6107. Acc.: 60.24%\nEpoch: 584. Loss: 0.5991. Acc.: 59.32%\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 585. Loss: 0.6008. Acc.: 59.58%\nEpoch: 586. Loss: 0.6014. Acc.: 58.27%\nEpoch: 587. Loss: 0.6005. Acc.: 59.45%\nEpoch: 588. Loss: 0.5999. Acc.: 61.02%\nEpoch: 589. Loss: 0.6076. Acc.: 58.40%\nEpoch: 590. Loss: 0.5857. Acc.: 61.55%\nEpoch: 591. Loss: 0.5896. Acc.: 56.17%\nEpoch: 592. Loss: 0.6127. Acc.: 60.63%\nEpoch: 593. Loss: 0.6141. Acc.: 57.74%\nEpoch: 594. Loss: 0.5879. Acc.: 59.71%\nEpoch: 595. Loss: 0.6089. Acc.: 59.97%\nEpoch: 596. Loss: 0.5938. Acc.: 59.58%\nEpoch: 597. Loss: 0.5984. Acc.: 58.14%\nEpoch: 598. Loss: 0.5878. Acc.: 57.74%\nEpoch: 599. Loss: 0.5952. Acc.: 58.40%\nEpoch: 600. Loss: 0.5975. Acc.: 56.56%\nEpoch: 601. Loss: 0.5961. Acc.: 59.32%\nEpoch: 602. Loss: 0.5882. Acc.: 56.43%\nEpoch: 603. Loss: 0.5880. Acc.: 58.27%\nEpoch: 604. Loss: 0.5992. Acc.: 60.63%\nEpoch: 605. Loss: 0.6035. Acc.: 62.34%\nEpoch 605 best model saved with accuracy: 62.34%\nEpoch: 606. Loss: 0.5770. Acc.: 56.96%\nEpoch: 607. Loss: 0.5795. Acc.: 61.15%\nEpoch: 608. Loss: 0.5847. Acc.: 57.74%\nEpoch: 609. Loss: 0.6087. Acc.: 59.84%\nEpoch: 610. Loss: 0.5865. Acc.: 59.19%\nEpoch: 611. Loss: 0.6083. Acc.: 58.27%\nEpoch: 612. Loss: 0.5928. Acc.: 59.84%\nEpoch: 613. Loss: 0.5981. Acc.: 57.74%\nEpoch: 614. Loss: 0.5900. Acc.: 58.92%\nEpoch: 615. Loss: 0.5729. Acc.: 58.01%\nEpoch: 616. Loss: 0.6188. Acc.: 58.27%\nEpoch: 617. Loss: 0.5876. Acc.: 61.02%\nEpoch: 618. Loss: 0.5812. Acc.: 57.61%\nEpoch: 619. Loss: 0.5926. Acc.: 59.19%\nEpoch: 620. Loss: 0.5905. Acc.: 58.14%\nEpoch: 621. Loss: 0.6052. Acc.: 57.48%\nEpoch: 622. Loss: 0.5914. Acc.: 60.10%\nEpoch: 623. Loss: 0.6024. Acc.: 56.30%\nEpoch: 624. Loss: 0.5873. Acc.: 57.22%\nEpoch: 625. Loss: 0.5925. Acc.: 59.84%\nEpoch: 626. Loss: 0.5929. Acc.: 57.22%\nEpoch: 627. Loss: 0.6048. Acc.: 60.37%\nEpoch: 628. Loss: 0.5985. Acc.: 60.10%\nEpoch: 629. Loss: 0.6407. Acc.: 59.45%\nEpoch: 630. Loss: 0.5811. Acc.: 62.07%\nEpoch: 631. Loss: 0.6074. Acc.: 58.92%\nEpoch: 632. Loss: 0.6014. Acc.: 60.50%\nEpoch: 633. Loss: 0.6121. Acc.: 59.71%\nEpoch: 634. Loss: 0.5904. Acc.: 61.15%\nEpoch: 635. Loss: 0.5898. Acc.: 61.15%\nEpoch: 636. Loss: 0.5969. Acc.: 59.32%\nEpoch: 637. Loss: 0.5897. Acc.: 58.53%\nEpoch: 638. Loss: 0.5947. Acc.: 59.84%\nEpoch: 639. Loss: 0.5924. Acc.: 56.96%\nEpoch: 640. Loss: 0.6040. Acc.: 58.27%\nEpoch: 641. Loss: 0.6046. Acc.: 60.63%\nEpoch: 642. Loss: 0.6002. Acc.: 60.37%\nEpoch: 643. Loss: 0.6101. Acc.: 56.82%\nEpoch: 644. Loss: 0.6000. Acc.: 59.71%\nEpoch: 645. Loss: 0.5928. Acc.: 58.14%\nEpoch: 646. Loss: 0.6119. Acc.: 59.58%\nEpoch: 647. Loss: 0.5897. Acc.: 58.40%\nEpoch: 648. Loss: 0.6104. Acc.: 60.76%\nEpoch: 649. Loss: 0.6155. Acc.: 58.14%\nEpoch: 650. Loss: 0.6061. Acc.: 59.97%\nEpoch: 651. Loss: 0.5838. Acc.: 60.24%\nEpoch: 652. Loss: 0.5934. Acc.: 59.32%\nEpoch: 653. Loss: 0.5911. Acc.: 58.40%\nEpoch: 654. Loss: 0.5851. Acc.: 58.79%\nEpoch: 655. Loss: 0.5815. Acc.: 59.32%\nEpoch: 656. Loss: 0.5861. Acc.: 59.45%\nEpoch: 657. Loss: 0.5974. Acc.: 60.37%\nEpoch: 658. Loss: 0.5992. Acc.: 58.40%\nEpoch: 659. Loss: 0.5968. Acc.: 60.89%\nEpoch: 660. Loss: 0.5884. Acc.: 60.63%\nEpoch: 661. Loss: 0.6008. Acc.: 61.68%\nEpoch: 662. Loss: 0.6138. Acc.: 59.97%\nEpoch: 663. Loss: 0.5855. Acc.: 61.94%\nEpoch: 664. Loss: 0.5886. Acc.: 60.89%\nEpoch: 665. Loss: 0.5850. Acc.: 59.45%\nEpoch: 666. Loss: 0.5872. Acc.: 60.50%\nEpoch: 667. Loss: 0.5835. Acc.: 59.97%\nEpoch: 668. Loss: 0.5906. Acc.: 58.27%\nEpoch: 669. Loss: 0.5635. Acc.: 60.24%\nEpoch: 670. Loss: 0.6043. Acc.: 60.50%\nEpoch: 671. Loss: 0.5989. Acc.: 57.74%\nEpoch: 672. Loss: 0.5804. Acc.: 60.50%\nEpoch: 673. Loss: 0.5700. Acc.: 61.02%\nEpoch: 674. Loss: 0.5981. Acc.: 58.27%\nEpoch: 675. Loss: 0.5797. Acc.: 61.68%\nEpoch: 676. Loss: 0.5890. Acc.: 61.81%\nEpoch: 677. Loss: 0.5893. Acc.: 57.35%\nEpoch: 678. Loss: 0.5805. Acc.: 60.63%\nEpoch: 679. Loss: 0.5920. Acc.: 60.76%\nEpoch: 680. Loss: 0.5949. Acc.: 62.20%\nEpoch: 681. Loss: 0.6003. Acc.: 60.50%\nEpoch: 682. Loss: 0.5797. Acc.: 58.66%\nEpoch: 683. Loss: 0.5841. Acc.: 61.55%\nEpoch: 684. Loss: 0.5982. Acc.: 58.66%\nEpoch: 685. Loss: 0.5777. Acc.: 60.10%\nEpoch: 686. Loss: 0.5797. Acc.: 59.45%\nEpoch: 687. Loss: 0.5809. Acc.: 56.82%\nEpoch: 688. Loss: 0.5943. Acc.: 60.10%\nEpoch: 689. Loss: 0.5813. Acc.: 61.15%\nEpoch: 690. Loss: 0.5887. Acc.: 57.09%\nEpoch: 691. Loss: 0.5798. Acc.: 58.40%\nEpoch: 692. Loss: 0.5928. Acc.: 59.58%\nEpoch: 693. Loss: 0.5656. Acc.: 59.06%\nEpoch: 694. Loss: 0.5835. Acc.: 58.01%\nEpoch: 695. Loss: 0.5748. Acc.: 59.32%\nEpoch: 696. Loss: 0.5623. Acc.: 59.19%\nEpoch: 697. Loss: 0.5807. Acc.: 58.01%\nEpoch: 698. Loss: 0.5862. Acc.: 58.53%\nEpoch: 699. Loss: 0.5897. Acc.: 59.06%\nEpoch: 700. Loss: 0.5798. Acc.: 59.58%\nEpoch: 701. Loss: 0.5892. Acc.: 61.29%\nEpoch: 702. Loss: 0.5831. Acc.: 57.35%\nEpoch: 703. Loss: 0.5942. Acc.: 61.02%\nEpoch: 704. Loss: 0.5828. Acc.: 61.15%\nEpoch: 705. Loss: 0.5641. Acc.: 60.50%\nEpoch: 706. Loss: 0.5927. Acc.: 61.02%\nEpoch: 707. Loss: 0.5879. Acc.: 60.37%\nEpoch: 708. Loss: 0.5848. Acc.: 62.47%\nEpoch 708 best model saved with accuracy: 62.47%\nEpoch: 709. Loss: 0.5724. Acc.: 59.97%\nEpoch: 710. Loss: 0.5787. Acc.: 61.15%\nEpoch: 711. Loss: 0.5646. Acc.: 61.68%\nEpoch: 712. Loss: 0.5869. Acc.: 61.29%\nEpoch: 713. Loss: 0.5709. Acc.: 60.24%\nEpoch: 714. Loss: 0.5799. Acc.: 60.50%\nEpoch: 715. Loss: 0.5859. Acc.: 61.55%\nEpoch: 716. Loss: 0.5782. Acc.: 60.37%\nEpoch: 717. Loss: 0.5792. Acc.: 60.37%\nEpoch: 718. Loss: 0.5993. Acc.: 60.89%\nEpoch: 719. Loss: 0.5813. Acc.: 58.79%\nEpoch: 720. Loss: 0.5793. Acc.: 58.40%\nEpoch: 721. Loss: 0.5869. Acc.: 61.29%\nEpoch: 722. Loss: 0.5789. Acc.: 59.58%\nEpoch: 723. Loss: 0.5844. Acc.: 59.06%\nEpoch: 724. Loss: 0.5847. Acc.: 61.29%\nEpoch: 725. Loss: 0.5845. Acc.: 59.58%\nEpoch: 726. Loss: 0.5872. Acc.: 59.06%\nEpoch: 727. Loss: 0.5742. Acc.: 59.58%\nEpoch: 728. Loss: 0.5906. Acc.: 59.19%\nEpoch: 729. Loss: 0.5729. Acc.: 61.02%\nEpoch: 730. Loss: 0.5833. Acc.: 63.39%\nEpoch 730 best model saved with accuracy: 63.39%\nEpoch: 731. Loss: 0.6029. Acc.: 60.10%\nEpoch: 732. Loss: 0.5733. Acc.: 60.89%\nEpoch: 733. Loss: 0.5795. Acc.: 58.92%\nEpoch: 734. Loss: 0.5968. Acc.: 58.66%\nEpoch: 735. Loss: 0.5700. Acc.: 60.24%\nEpoch: 736. Loss: 0.5851. Acc.: 57.87%\nEpoch: 737. Loss: 0.5782. Acc.: 59.71%\nEpoch: 738. Loss: 0.5840. Acc.: 59.84%\nEpoch: 739. Loss: 0.5786. Acc.: 59.97%\nEpoch: 740. Loss: 0.5942. Acc.: 59.06%\nEpoch: 741. Loss: 0.5891. Acc.: 60.89%\nEpoch: 742. Loss: 0.5763. Acc.: 57.74%\nEpoch: 743. Loss: 0.5983. Acc.: 57.74%\nEpoch: 744. Loss: 0.6050. Acc.: 62.86%\nEpoch: 745. Loss: 0.6046. Acc.: 57.09%\nEpoch: 746. Loss: 0.5853. Acc.: 59.06%\nEpoch: 747. Loss: 0.5733. Acc.: 60.63%\nEpoch: 748. Loss: 0.5717. Acc.: 60.63%\nEpoch: 749. Loss: 0.5918. Acc.: 57.61%\nEpoch: 750. Loss: 0.5748. Acc.: 60.37%\nEpoch: 751. Loss: 0.5705. Acc.: 59.58%\nEpoch: 752. Loss: 0.5868. Acc.: 61.68%\nEpoch: 753. Loss: 0.5715. Acc.: 58.66%\nEpoch: 754. Loss: 0.5749. Acc.: 61.29%\nEpoch: 755. Loss: 0.5918. Acc.: 59.06%\nEpoch: 756. Loss: 0.5800. Acc.: 60.89%\nEpoch: 757. Loss: 0.5762. Acc.: 58.01%\nEpoch: 758. Loss: 0.5803. Acc.: 60.24%\nEpoch: 759. Loss: 0.5591. Acc.: 61.94%\nEpoch: 760. Loss: 0.5705. Acc.: 61.29%\nEpoch: 761. Loss: 0.5703. Acc.: 59.32%\nEpoch: 762. Loss: 0.5786. Acc.: 59.19%\nEpoch: 763. Loss: 0.5728. Acc.: 58.27%\nEpoch: 764. Loss: 0.5898. Acc.: 61.68%\nEpoch: 765. Loss: 0.5993. Acc.: 59.32%\nEpoch: 766. Loss: 0.5673. Acc.: 62.73%\nEpoch: 767. Loss: 0.6066. Acc.: 59.45%\nEpoch: 768. Loss: 0.5910. Acc.: 61.29%\nEpoch: 769. Loss: 0.5741. Acc.: 60.63%\nEpoch: 770. Loss: 0.5924. Acc.: 62.07%\nEpoch: 771. Loss: 0.5826. Acc.: 59.19%\nEpoch: 772. Loss: 0.5858. Acc.: 61.55%\nEpoch: 773. Loss: 0.5799. Acc.: 58.66%\nEpoch: 774. Loss: 0.5762. Acc.: 61.94%\nEpoch: 775. Loss: 0.5729. Acc.: 58.01%\nEpoch: 776. Loss: 0.5866. Acc.: 62.99%\nEpoch: 777. Loss: 0.5725. Acc.: 57.87%\nEpoch: 778. Loss: 0.5891. Acc.: 60.50%\nEpoch: 779. Loss: 0.5812. Acc.: 60.10%\nEpoch: 780. Loss: 0.5857. Acc.: 61.15%\nEpoch: 781. Loss: 0.5899. Acc.: 60.76%\nEpoch: 782. Loss: 0.5758. Acc.: 61.68%\nEpoch: 783. Loss: 0.5810. Acc.: 61.15%\nEpoch: 784. Loss: 0.5756. Acc.: 62.34%\nEpoch: 785. Loss: 0.5743. Acc.: 62.86%\nEpoch: 786. Loss: 0.5813. Acc.: 59.06%\nEpoch: 787. Loss: 0.5630. Acc.: 59.84%\nEpoch: 788. Loss: 0.5725. Acc.: 60.24%\nEpoch: 789. Loss: 0.5848. Acc.: 62.07%\nEpoch: 790. Loss: 0.5807. Acc.: 60.37%\nEpoch: 791. Loss: 0.5898. Acc.: 62.07%\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 792. Loss: 0.5671. Acc.: 62.99%\nEpoch: 793. Loss: 0.5800. Acc.: 62.20%\nEpoch: 794. Loss: 0.5975. Acc.: 61.29%\nEpoch: 795. Loss: 0.5584. Acc.: 62.34%\nEpoch: 796. Loss: 0.5861. Acc.: 61.15%\nEpoch: 797. Loss: 0.5969. Acc.: 60.37%\nEpoch: 798. Loss: 0.5808. Acc.: 59.71%\nEpoch: 799. Loss: 0.5745. Acc.: 62.20%\nEpoch: 800. Loss: 0.5832. Acc.: 59.32%\nEpoch: 801. Loss: 0.5941. Acc.: 60.10%\nEpoch: 802. Loss: 0.5776. Acc.: 60.24%\nEpoch: 803. Loss: 0.5789. Acc.: 60.63%\nEpoch: 804. Loss: 0.5830. Acc.: 61.81%\nEpoch: 805. Loss: 0.5594. Acc.: 61.68%\nEpoch: 806. Loss: 0.5712. Acc.: 60.50%\nEpoch: 807. Loss: 0.5638. Acc.: 57.87%\nEpoch: 808. Loss: 0.5803. Acc.: 61.42%\nEpoch: 809. Loss: 0.5701. Acc.: 61.02%\nEpoch: 810. Loss: 0.5815. Acc.: 60.24%\nEpoch: 811. Loss: 0.5749. Acc.: 61.55%\nEpoch: 812. Loss: 0.5763. Acc.: 61.02%\nEpoch: 813. Loss: 0.5943. Acc.: 58.40%\nEpoch: 814. Loss: 0.5812. Acc.: 61.29%\nEpoch: 815. Loss: 0.5771. Acc.: 62.07%\nEpoch: 816. Loss: 0.5839. Acc.: 61.42%\nEpoch: 817. Loss: 0.5593. Acc.: 61.29%\nEpoch: 818. Loss: 0.5767. Acc.: 60.89%\nEpoch: 819. Loss: 0.5652. Acc.: 60.10%\nEpoch: 820. Loss: 0.5837. Acc.: 64.70%\nEpoch 820 best model saved with accuracy: 64.70%\nEpoch: 821. Loss: 0.5629. Acc.: 62.86%\nEpoch: 822. Loss: 0.5647. Acc.: 59.45%\nEpoch: 823. Loss: 0.5887. Acc.: 62.99%\nEpoch: 824. Loss: 0.5624. Acc.: 59.84%\nEpoch: 825. Loss: 0.5570. Acc.: 59.06%\nEpoch: 826. Loss: 0.5578. Acc.: 63.78%\nEpoch: 827. Loss: 0.5837. Acc.: 57.09%\nEpoch: 828. Loss: 0.5862. Acc.: 61.15%\nEpoch: 829. Loss: 0.5836. Acc.: 59.19%\nEpoch: 830. Loss: 0.5929. Acc.: 58.79%\nEpoch: 831. Loss: 0.5733. Acc.: 58.53%\nEpoch: 832. Loss: 0.5839. Acc.: 60.37%\nEpoch: 833. Loss: 0.5874. Acc.: 58.14%\nEpoch: 834. Loss: 0.5722. Acc.: 60.50%\nEpoch: 835. Loss: 0.5616. Acc.: 58.53%\nEpoch: 836. Loss: 0.5927. Acc.: 57.74%\nEpoch: 837. Loss: 0.5701. Acc.: 61.29%\nEpoch: 838. Loss: 0.5647. Acc.: 58.53%\nEpoch: 839. Loss: 0.5680. Acc.: 59.32%\nEpoch: 840. Loss: 0.5882. Acc.: 60.50%\nEpoch: 841. Loss: 0.5835. Acc.: 59.58%\nEpoch: 842. Loss: 0.5659. Acc.: 61.42%\nEpoch: 843. Loss: 0.5721. Acc.: 60.37%\nEpoch: 844. Loss: 0.5545. Acc.: 60.89%\nEpoch: 845. Loss: 0.5680. Acc.: 61.68%\nEpoch: 846. Loss: 0.5829. Acc.: 61.02%\nEpoch: 847. Loss: 0.5840. Acc.: 60.89%\nEpoch: 848. Loss: 0.5743. Acc.: 61.94%\nEpoch: 849. Loss: 0.5649. Acc.: 63.25%\nEpoch: 850. Loss: 0.5924. Acc.: 62.99%\nEpoch: 851. Loss: 0.5842. Acc.: 63.78%\nEpoch: 852. Loss: 0.5800. Acc.: 58.79%\nEpoch: 853. Loss: 0.5901. Acc.: 61.94%\nEpoch: 854. Loss: 0.5750. Acc.: 62.07%\nEpoch: 855. Loss: 0.5864. Acc.: 59.58%\nEpoch: 856. Loss: 0.5693. Acc.: 58.92%\nEpoch: 857. Loss: 0.6011. Acc.: 62.73%\nEpoch: 858. Loss: 0.5565. Acc.: 59.58%\nEpoch: 859. Loss: 0.5746. Acc.: 57.35%\nEpoch: 860. Loss: 0.5829. Acc.: 61.94%\nEpoch: 861. Loss: 0.5810. Acc.: 59.71%\nEpoch: 862. Loss: 0.5927. Acc.: 58.53%\nEpoch: 863. Loss: 0.5661. Acc.: 61.81%\nEpoch: 864. Loss: 0.5698. Acc.: 61.02%\nEpoch: 865. Loss: 0.5768. Acc.: 59.19%\nEpoch: 866. Loss: 0.5654. Acc.: 61.42%\nEpoch: 867. Loss: 0.5722. Acc.: 61.02%\nEpoch: 868. Loss: 0.5579. Acc.: 62.20%\nEpoch: 869. Loss: 0.5659. Acc.: 62.73%\nEpoch: 870. Loss: 0.5821. Acc.: 60.37%\nEpoch: 871. Loss: 0.5495. Acc.: 62.34%\nEpoch: 872. Loss: 0.5806. Acc.: 63.12%\nEpoch: 873. Loss: 0.5544. Acc.: 60.89%\nEpoch: 874. Loss: 0.5714. Acc.: 60.63%\nEpoch: 875. Loss: 0.5501. Acc.: 61.55%\nEpoch: 876. Loss: 0.5568. Acc.: 61.15%\nEpoch: 877. Loss: 0.5656. Acc.: 59.84%\nEpoch: 878. Loss: 0.5653. Acc.: 62.99%\nEpoch: 879. Loss: 0.5596. Acc.: 61.29%\nEpoch: 880. Loss: 0.5727. Acc.: 61.42%\nEpoch: 881. Loss: 0.5789. Acc.: 61.94%\nEpoch: 882. Loss: 0.5709. Acc.: 61.29%\nEpoch: 883. Loss: 0.5808. Acc.: 62.99%\nEpoch: 884. Loss: 0.5655. Acc.: 61.94%\nEpoch: 885. Loss: 0.5783. Acc.: 61.29%\nEpoch: 886. Loss: 0.5550. Acc.: 61.68%\nEpoch: 887. Loss: 0.5732. Acc.: 60.63%\nEpoch: 888. Loss: 0.5736. Acc.: 62.07%\nEpoch: 889. Loss: 0.5809. Acc.: 58.79%\nEpoch: 890. Loss: 0.5757. Acc.: 59.71%\nEpoch: 891. Loss: 0.5645. Acc.: 59.84%\nEpoch: 892. Loss: 0.5609. Acc.: 62.73%\nEpoch: 893. Loss: 0.5704. Acc.: 61.15%\nEpoch: 894. Loss: 0.5602. Acc.: 62.07%\nEpoch: 895. Loss: 0.5609. Acc.: 59.84%\nEpoch: 896. Loss: 0.5564. Acc.: 62.07%\nEpoch: 897. Loss: 0.5658. Acc.: 61.42%\nEpoch: 898. Loss: 0.5674. Acc.: 58.66%\nEpoch: 899. Loss: 0.5732. Acc.: 61.94%\nEpoch: 900. Loss: 0.5616. Acc.: 60.63%\nEpoch: 901. Loss: 0.5873. Acc.: 58.27%\nEpoch: 902. Loss: 0.5650. Acc.: 61.02%\nEpoch: 903. Loss: 0.5811. Acc.: 60.76%\nEpoch: 904. Loss: 0.5801. Acc.: 61.55%\nEpoch: 905. Loss: 0.5696. Acc.: 61.68%\nEpoch: 906. Loss: 0.5737. Acc.: 62.34%\nEpoch: 907. Loss: 0.5540. Acc.: 60.50%\nEpoch: 908. Loss: 0.5531. Acc.: 61.94%\nEpoch: 909. Loss: 0.5702. Acc.: 60.76%\nEpoch: 910. Loss: 0.5612. Acc.: 62.73%\nEpoch: 911. Loss: 0.5890. Acc.: 59.45%\nEpoch: 912. Loss: 0.5775. Acc.: 61.55%\nEpoch: 913. Loss: 0.5660. Acc.: 63.39%\nEpoch: 914. Loss: 0.5725. Acc.: 61.02%\nEpoch: 915. Loss: 0.5856. Acc.: 60.63%\nEpoch: 916. Loss: 0.5622. Acc.: 60.50%\nEpoch: 917. Loss: 0.5679. Acc.: 59.06%\nEpoch: 918. Loss: 0.6287. Acc.: 64.83%\nEpoch 918 best model saved with accuracy: 64.83%\nEpoch: 919. Loss: 0.5626. Acc.: 61.55%\nEpoch: 920. Loss: 0.5669. Acc.: 63.39%\nEpoch: 921. Loss: 0.5557. Acc.: 61.02%\nEpoch: 922. Loss: 0.5630. Acc.: 61.29%\nEpoch: 923. Loss: 0.5526. Acc.: 61.02%\nEpoch: 924. Loss: 0.5787. Acc.: 60.63%\nEpoch: 925. Loss: 0.5735. Acc.: 62.99%\nEpoch: 926. Loss: 0.5710. Acc.: 62.73%\nEpoch: 927. Loss: 0.5560. Acc.: 60.76%\nEpoch: 928. Loss: 0.5500. Acc.: 61.55%\nEpoch: 929. Loss: 0.5445. Acc.: 59.32%\nEpoch: 930. Loss: 0.5503. Acc.: 60.24%\nEpoch: 931. Loss: 0.5498. Acc.: 62.73%\nEpoch: 932. Loss: 0.5708. Acc.: 59.97%\nEpoch: 933. Loss: 0.5610. Acc.: 59.32%\nEpoch: 934. Loss: 0.5428. Acc.: 63.25%\nEpoch: 935. Loss: 0.5722. Acc.: 61.29%\nEpoch: 936. Loss: 0.5490. Acc.: 62.73%\nEpoch: 937. Loss: 0.5624. Acc.: 61.94%\nEpoch: 938. Loss: 0.5521. Acc.: 63.12%\nEpoch: 939. Loss: 0.5692. Acc.: 63.12%\nEpoch: 940. Loss: 0.5511. Acc.: 62.99%\nEpoch: 941. Loss: 0.5663. Acc.: 63.12%\nEpoch: 942. Loss: 0.5678. Acc.: 61.81%\nEpoch: 943. Loss: 0.5648. Acc.: 62.47%\nEpoch: 944. Loss: 0.5634. Acc.: 62.07%\nEpoch: 945. Loss: 0.5471. Acc.: 61.94%\nEpoch: 946. Loss: 0.5549. Acc.: 61.02%\nEpoch: 947. Loss: 0.5564. Acc.: 61.94%\nEpoch: 948. Loss: 0.5464. Acc.: 59.84%\nEpoch: 949. Loss: 0.5539. Acc.: 61.81%\nEpoch: 950. Loss: 0.5670. Acc.: 61.42%\nEpoch: 951. Loss: 0.5536. Acc.: 61.02%\nEpoch: 952. Loss: 0.5466. Acc.: 62.60%\nEpoch: 953. Loss: 0.5590. Acc.: 61.94%\nEpoch: 954. Loss: 0.5696. Acc.: 60.37%\nEpoch: 955. Loss: 0.5694. Acc.: 62.73%\nEpoch: 956. Loss: 0.5586. Acc.: 62.99%\nEpoch: 957. Loss: 0.5697. Acc.: 58.79%\nEpoch: 958. Loss: 0.5719. Acc.: 61.55%\nEpoch: 959. Loss: 0.5570. Acc.: 61.29%\nEpoch: 960. Loss: 0.5667. Acc.: 61.94%\nEpoch: 961. Loss: 0.5449. Acc.: 60.89%\nEpoch: 962. Loss: 0.5633. Acc.: 65.35%\nEpoch 962 best model saved with accuracy: 65.35%\nEpoch: 963. Loss: 0.5606. Acc.: 62.73%\nEpoch: 964. Loss: 0.5468. Acc.: 62.60%\nEpoch: 965. Loss: 0.5665. Acc.: 61.15%\nEpoch: 966. Loss: 0.5486. Acc.: 64.44%\nEpoch: 967. Loss: 0.5697. Acc.: 61.42%\nEpoch: 968. Loss: 0.5942. Acc.: 62.73%\nEpoch: 969. Loss: 0.5768. Acc.: 62.34%\nEpoch: 970. Loss: 0.5510. Acc.: 63.52%\nEpoch: 971. Loss: 0.5458. Acc.: 63.39%\nEpoch: 972. Loss: 0.5738. Acc.: 62.34%\nEpoch: 973. Loss: 0.5517. Acc.: 63.39%\nEpoch: 974. Loss: 0.5576. Acc.: 62.60%\nEpoch: 975. Loss: 0.5819. Acc.: 61.29%\nEpoch: 976. Loss: 0.5771. Acc.: 64.57%\nEpoch: 977. Loss: 0.5635. Acc.: 62.20%\nEpoch: 978. Loss: 0.5735. Acc.: 59.84%\nEpoch: 979. Loss: 0.5457. Acc.: 63.91%\nEpoch: 980. Loss: 0.5785. Acc.: 61.29%\nEpoch: 981. Loss: 0.5623. Acc.: 63.25%\nEpoch: 982. Loss: 0.5577. Acc.: 61.42%\nEpoch: 983. Loss: 0.5584. Acc.: 61.81%\nEpoch: 984. Loss: 0.5526. Acc.: 61.68%\nEpoch: 985. Loss: 0.5712. Acc.: 63.12%\nEpoch: 986. Loss: 0.5520. Acc.: 62.20%\nEpoch: 987. Loss: 0.5577. Acc.: 64.04%\nEpoch: 988. Loss: 0.5402. Acc.: 60.76%\nEpoch: 989. Loss: 0.5636. Acc.: 61.42%\nEpoch: 990. Loss: 0.5609. Acc.: 64.17%\nEpoch: 991. Loss: 0.5638. Acc.: 64.70%\nEpoch: 992. Loss: 0.5583. Acc.: 64.17%\nEpoch: 993. Loss: 0.5258. Acc.: 61.68%\nEpoch: 994. Loss: 0.5516. Acc.: 63.12%\nEpoch: 995. Loss: 0.5572. Acc.: 62.07%\nEpoch: 996. Loss: 0.5510. Acc.: 63.39%\nEpoch: 997. Loss: 0.5543. Acc.: 62.99%\nEpoch: 998. Loss: 0.5497. Acc.: 62.86%\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 999. Loss: 0.5604. Acc.: 63.52%\nEpoch: 1000. Loss: 0.5520. Acc.: 64.70%\nEpoch: 1001. Loss: 0.5498. Acc.: 61.81%\nEpoch: 1002. Loss: 0.5563. Acc.: 62.34%\nEpoch: 1003. Loss: 0.5500. Acc.: 61.94%\nEpoch: 1004. Loss: 0.5515. Acc.: 62.47%\nEpoch: 1005. Loss: 0.5409. Acc.: 63.52%\nEpoch: 1006. Loss: 0.5565. Acc.: 61.94%\nEpoch: 1007. Loss: 0.5361. Acc.: 62.34%\nEpoch: 1008. Loss: 0.5718. Acc.: 62.07%\nEpoch: 1009. Loss: 0.5461. Acc.: 62.99%\nEpoch: 1010. Loss: 0.5547. Acc.: 62.60%\nEpoch: 1011. Loss: 0.5623. Acc.: 62.86%\nEpoch: 1012. Loss: 0.5652. Acc.: 61.15%\nEpoch: 1013. Loss: 0.5550. Acc.: 63.25%\nEpoch: 1014. Loss: 0.5708. Acc.: 61.02%\nEpoch: 1015. Loss: 0.5568. Acc.: 61.68%\nEpoch: 1016. Loss: 0.5657. Acc.: 62.07%\nEpoch: 1017. Loss: 0.5549. Acc.: 62.20%\nEpoch: 1018. Loss: 0.5539. Acc.: 61.42%\nEpoch: 1019. Loss: 0.5241. Acc.: 63.78%\nEpoch: 1020. Loss: 0.5692. Acc.: 61.29%\nEpoch: 1021. Loss: 0.5543. Acc.: 61.55%\nEpoch: 1022. Loss: 0.5696. Acc.: 62.60%\nEpoch: 1023. Loss: 0.5460. Acc.: 62.60%\nEpoch: 1024. Loss: 0.5447. Acc.: 63.25%\nEpoch: 1024. Loss: 0.5447. Acc.: 63.25%\nEpoch: 1025. Loss: 0.5466. Acc.: 63.12%\nEpoch: 1026. Loss: 0.5724. Acc.: 64.04%\nEpoch: 1027. Loss: 0.5529. Acc.: 62.99%\nEpoch: 1028. Loss: 0.5460. Acc.: 63.25%\nEpoch: 1029. Loss: 0.5596. Acc.: 62.60%\nEpoch: 1030. Loss: 0.5537. Acc.: 62.34%\nEpoch: 1031. Loss: 0.5634. Acc.: 60.10%\nEpoch: 1032. Loss: 0.5634. Acc.: 62.07%\nEpoch: 1033. Loss: 0.5635. Acc.: 62.34%\nEpoch: 1034. Loss: 0.5509. Acc.: 62.07%\nEpoch: 1035. Loss: 0.5490. Acc.: 62.60%\nEpoch: 1036. Loss: 0.5495. Acc.: 63.91%\nEpoch: 1037. Loss: 0.5624. Acc.: 58.79%\nEpoch: 1038. Loss: 0.5715. Acc.: 61.15%\nEpoch: 1039. Loss: 0.5608. Acc.: 61.68%\nEpoch: 1040. Loss: 0.5636. Acc.: 59.32%\nEpoch: 1041. Loss: 0.5541. Acc.: 62.99%\nEpoch: 1042. Loss: 0.5518. Acc.: 62.20%\nEpoch: 1043. Loss: 0.5905. Acc.: 61.68%\nEpoch: 1044. Loss: 0.5699. Acc.: 60.63%\nEpoch: 1045. Loss: 0.5792. Acc.: 62.34%\nEpoch: 1046. Loss: 0.5597. Acc.: 62.34%\nEpoch: 1047. Loss: 0.5593. Acc.: 61.94%\nEpoch: 1048. Loss: 0.5698. Acc.: 63.39%\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"5\">**Compute result on testing data**</font><br>"},{"metadata":{"_uuid":"d0f483d5-5542-4d65-a70f-1aa7b3e13c3c","_cell_guid":"05b2b8d9-2b63-436e-a906-a839fb008c0e","trusted":true},"cell_type":"code","source":"test_results = []\nmodel.load_state_dict(torch.load('best.pth'))\nmodel.eval()\n\n# for batch in val_dl:\n#     x_raw, y_batch = [t.to(device) for t in batch]\n#     out = model(x_raw)\n#     preds = F.log_softmax(out, dim=1).argmax(dim=1)\n#     total += y_batch.size(0)\n#     correct += (preds == y_batch).sum().item()\n#     acc = correct / total\n#     acc_history.append(acc)\n\n# for batch in tst_dl:\n#     x, y_batche = [t.to(device) for t in (batch)]\n#     out = model(x)\n#     y_hat = F.log_softmax(out, dim=1).argmax(dim=1)\n#     test_results.extend(y_hat.tolist())\n    \nfor batch in tst_dl:\n        x_raw, y_batch = [t.to(device) for t in batch]\n        out = model(x_channel_0, x_channel_1, x_channel_2, x_channel_3, x_channel_4, x_channel_5, x_channel_6, x_channel_7, x_channel_8)\n        preds = F.log_softmax(out, dim=1).argmax(dim=1)\n        total += y_batch.size(0)\n        correct += (preds == y_batch).sum().item()\n    \nacc = correct / total\nprint(\"accurancy on test data: \"+str(acc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"5\">**Store final result**</font><br>"},{"metadata":{"_uuid":"7a49a38e-e203-4884-ba78-33fe8aa02426","_cell_guid":"f4bd2628-c801-42d5-9af6-11bbd89f2352","trusted":true},"cell_type":"code","source":"'''\nsubmit = pd.read_csv(sample)\nenc = joblib.load(ROOT/'encoder.model')\nsubmit['surface'] = enc.inverse_transform(test_results)\nsubmit.to_csv('submit_base.csv', index=None)\nprint(\"store result successfully!\")\n'''","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}