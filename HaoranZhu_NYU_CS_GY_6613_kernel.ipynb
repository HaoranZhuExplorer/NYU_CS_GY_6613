{"cells":[{"metadata":{},"cell_type":"markdown","source":"<font size=\"5\">**Project Summary**</font><br>\nIn this competition, the main task is to do surface time series classification. 1d convolution is a good technique when doing the time series classification job. So I apply 1d convolutional neural network to solve this problem. The whole code is written in Pytorch."},{"metadata":{},"cell_type":"markdown","source":"<font size=\"5\">**External libs we are using**</font><br>"},{"metadata":{"_uuid":"a6ed998e-ee9a-4333-8332-8fa753b7f0d4","_cell_guid":"e6e0bc2b-9cba-4688-9e0b-15f11d219c2d","trusted":true},"cell_type":"code","source":"# libs we are using\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport torch\nfrom torch import nn\nfrom torch import optim\nfrom torch.nn import functional as F\nfrom torch.utils.data import TensorDataset, DataLoader","execution_count":14,"outputs":[]},{"metadata":{"_uuid":"b8906e48-2384-4bb3-bba7-722160428e39","_cell_guid":"295d38d1-6663-4575-93ba-c6cfac9f6129","trusted":true},"cell_type":"markdown","source":"<font size=\"5\">**Data preparations**</font><br>\n\nIn this project, I use the raw data as input of the network. I concatenated all datasets into one single numpy array. The first 3810 rows are training data with given labels, the rest 3816 rows are testing data where labels are not given. In total raw data has 7816 rows. In addition to that, we split the raw training data into training data and validation data by the ratio of 80/20.\n\n\nI create three datasets and data loaders for them to make the data ready for model's training. The process is straightforward. I split the labelled data into two subsets, and keep testing data as is. Also, I convert Numpy arrays into `torch.tensor` objects of proper type (float for samples, and long - for targets).\n"},{"metadata":{"_uuid":"94c41c8d-0c91-4aa1-83a4-ff3f518f35f4","_cell_guid":"88b9ec00-fc64-4040-896b-895d06cd58f2","trusted":true},"cell_type":"code","source":"def create_datasets(data, target, train_size, valid_pct=0.2, seed=None):\n    \"\"\"Converts NumPy arrays into PyTorch datsets.\n    \n    Three datasets are created in total:\n        * training dataset\n        * validation dataset\n        * testing (un-labelled) dataset\n\n    \"\"\"\n    raw =  data\n    sz = train_size\n    idx = np.arange(sz)\n    trn_idx, val_idx = train_test_split(\n        idx, test_size=valid_pct, random_state=seed)\n     \n    trn_ds = TensorDataset(\n        torch.tensor(raw[:sz][trn_idx]).float(), \n        torch.tensor(target[:sz][trn_idx]).long())\n    val_ds = TensorDataset(\n        torch.tensor(raw[:sz][val_idx]).float(), \n        torch.tensor(target[:sz][val_idx]).long())\n    tst_ds = TensorDataset(\n        torch.tensor(raw[sz:]).float(), \n        torch.tensor(target[sz:]).long())\n    return trn_ds, val_ds, tst_ds\n\n# We use this function to divide the data\ndef create_datasets2(data, train_size, valid_pct=0.2, seed=None):\n    \"\"\"Converts NumPy arrays into PyTorch datsets.\n    \n    Three datasets are created in total:\n        * training dataset\n        * validation dataset\n        * testing (un-labelled) dataset\n\n    \"\"\"\n    raw =  data\n    sz = train_size\n  \n    idx = np.arange(sz)\n    print(\"total data length\")\n    print(len(idx))\n    trn_idx, tst_idx = train_test_split(idx, test_size=0.2, random_state=1)\n    trn_idx, val_idx = train_test_split(trn_idx, test_size=0.25, random_state=1)\n    \n    print(\"training data length\")\n    print(len(trn_idx))\n    print(\"validation data length\")\n    print(len(val_idx))\n    print(\"testing data length\")\n    print(len(tst_idx))\n    \n    trn_ds = TensorDataset(\n        torch.tensor(raw[:sz][trn_idx]).float(), \n        torch.tensor(target[:sz][trn_idx]).long())\n    val_ds = TensorDataset(\n        torch.tensor(raw[:sz][val_idx]).float(), \n        torch.tensor(target[:sz][val_idx]).long())\n    tst_ds = TensorDataset(\n        torch.tensor(raw[:sz][tst_idx]).float(), \n        torch.tensor(target[:sz][tst_idx]).long())\n    \n    return trn_ds, val_ds, tst_ds\n\ndef create_loaders(data, bs=128, jobs=0):\n    \"\"\"Wraps the datasets returned by create_datasets function with data loaders.\"\"\"\n    \n    trn_ds, val_ds, tst_ds = data\n    trn_dl = DataLoader(trn_ds, batch_size=bs, shuffle=True, num_workers=jobs)\n    val_dl = DataLoader(val_ds, batch_size=bs, shuffle=False, num_workers=jobs)\n    tst_dl = DataLoader(tst_ds, batch_size=bs, shuffle=False, num_workers=jobs)\n    return trn_dl, val_dl, tst_dl","execution_count":15,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"5\">**Neural Network Architecture**</font><br>"},{"metadata":{"_uuid":"f144e459-53bb-4b5a-860d-abec117fc2cc","_cell_guid":"07870588-c88f-42ff-b782-5ae3c5e6e568","trusted":true},"cell_type":"code","source":"class Surface_Classifier(nn.Module):\n    def __init__(self, raw_ni, no, drop=.5):\n        super().__init__()\n        \n        self.conv1d = nn.Sequential(\n            nn.Conv1d(raw_ni, 32, 8, 2, 3),\n            nn.BatchNorm1d(32),\n            nn.PReLU(),\n            nn.Dropout(),\n            nn.Conv1d(    32,  64, 8, 4, 2),\n            nn.BatchNorm1d(64),\n            nn.PReLU(),\n            nn.Dropout(),\n            nn.Conv1d(    64, 128, 8, 4, 2),\n            nn.BatchNorm1d(128),\n            nn.PReLU(),\n            nn.Dropout(),\n            nn.Conv1d(   128, 256, 8, 4, 2),\n            nn.BatchNorm1d(256),\n            nn.PReLU(),\n            nn.Dropout()\n        )\n        \n        self.dense = nn.Sequential(\n            nn.Linear(256, 128), nn.ReLU(), nn.Dropout(),\n            nn.Linear(128, 64), nn.Softmax(dim=1), \n            nn.Linear(64, no)\n        )\n            \n\n    def forward(self, t_raw):\n        conv1d_out = self.conv1d(t_raw)\n        res = conv1d_out.view(conv1d_out.size(0), -1)        \n        out = self.dense(res)\n        return out","execution_count":16,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"5\">**Initial setup for random seed and project path**</font><br>"},{"metadata":{"_uuid":"b4b74c47-df3d-4201-93a7-f8cb79d6a652","_cell_guid":"aa2ea0fc-dfc2-436e-9a73-aab6bfb63d20","trusted":true},"cell_type":"code","source":"\n# set up the seed\nseed = 1\nnp.random.seed(seed)\n\n# path to sample submission\nsample = Path.cwd().parent/'input'/'career-con-2019'/'sample_submission.csv'\n\nROOT = Path.cwd().parent/'input'/'career-con-2019-preprocessed-data'\nraw_arr = np.load(ROOT/'feat.npy').transpose(0, 2, 1)\ntarget = np.load(ROOT/'target.npy')\n\nprint(\"raw array shape\")\nprint(raw_arr.shape)\nprint(\"label array shape\")\nprint(target.shape)\n\ntrn_sz = 3810  # only the first `trn_sz` rows in each array include labelled data\n#datasets = create_datasets((raw_arr), target, trn_sz, seed=seed)\ndatasets = create_datasets2((raw_arr), trn_sz, seed=seed)\n\n\n# make sure that we run on a proper device (not relevant for Kaggle kernels but helpful in Jupyter sessions)\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\n\ntrn_dl, val_dl, tst_dl = create_loaders(datasets, bs=256)","execution_count":17,"outputs":[{"output_type":"stream","text":"raw array shape\n(7626, 9, 128)\nlabel array shape\n(7626,)\n2286\n762\n762\n","name":"stdout"}]},{"metadata":{"_uuid":"b9026a32-1355-4689-8bf9-581a5228af69","_cell_guid":"5db66d39-80aa-48f4-9aab-4f44eace0192","trusted":true},"cell_type":"markdown","source":"<font size=\"5\">**Training model**</font><br>\nNow everything is ready to create a training loop and see if our model works. For each training epoch, the loop performs the following actions:\n1. train model on the `trn_ds` dataset;\n2. verify quality on the `val_ds` dataset;\n3. check if the quality improved since previous epoch, and if so, save the model's weights onto disk;\n4. in case if the model's quality isn't impoving for `patience` epochs, the training is stopped.\nAlso, the code tracks loss and accuracy history, and prints current scores with exponentially increasing logging frequency, i.e., only at 1, 2, 4, 8... epochs."},{"metadata":{"_uuid":"dee51d07-1cff-4db3-a87d-bebc4bf5bcc0","_cell_guid":"72095ada-fe94-4757-9a8a-918481b04336","trusted":true},"cell_type":"code","source":"raw_feat = raw_arr.shape[1]\n\n\nlr = 0.001\nn_epochs = 10000\niterations_per_epoch = len(trn_dl)\nnum_classes = 9\nbest_acc = 0\npatience, trials = 500, 0\nbase = 1\nstep = 2\nloss_history = []\nacc_history = []\n\nmodel = Surface_Classifier(raw_feat, num_classes).to(device)\ncriterion = nn.CrossEntropyLoss(reduction='sum')\nopt = optim.Adam(model.parameters(), lr=lr)\n\nprint('Start model training')\n\nfor epoch in range(1, n_epochs + 1):\n    \n    model.train()\n    epoch_loss = 0\n    for i, batch in enumerate(trn_dl):\n        x_raw, y_batch = [t.to(device) for t in batch]\n        opt.zero_grad()\n        out = model(x_raw)\n        loss = criterion(out, y_batch)\n        epoch_loss += loss.item()\n        loss.backward()\n        opt.step()\n        \n    epoch_loss /= trn_sz\n    loss_history.append(epoch_loss)\n    \n    model.eval()\n    correct, total = 0, 0\n    for batch in val_dl:\n        x_raw, y_batch = [t.to(device) for t in batch]\n        out = model(x_raw)\n        preds = F.log_softmax(out, dim=1).argmax(dim=1)\n        total += y_batch.size(0)\n        correct += (preds == y_batch).sum().item()\n    \n    acc = correct / total\n    acc_history.append(acc)\n\n    if epoch % base == 0:\n        print(f'Epoch: {epoch:3d}. Loss: {epoch_loss:.4f}. Acc.: {acc:2.2%}')\n        base *= step\n    print(f'Epoch: {epoch:3d}. Loss: {epoch_loss:.4f}. Acc.: {acc:2.2%}')\n\n    \n    if acc > best_acc:\n        trials = 0\n        best_acc = acc\n        torch.save(model.state_dict(), 'best.pth')\n        print(f'Epoch {epoch} best model saved with accuracy: {best_acc:2.2%}')\n    else:\n        trials += 1\n        if trials >= patience:\n            print(f'Early stopping on epoch {epoch}')\n            break\n            \nprint('Done!')","execution_count":null,"outputs":[{"output_type":"stream","text":"Start model training\nEpoch:   1. Loss: 1.3095. Acc.: 13.65%\nEpoch:   1. Loss: 1.3095. Acc.: 13.65%\nEpoch 1 best model saved with accuracy: 13.65%\nEpoch:   2. Loss: 1.3030. Acc.: 21.78%\nEpoch:   2. Loss: 1.3030. Acc.: 21.78%\nEpoch 2 best model saved with accuracy: 21.78%\nEpoch:   3. Loss: 1.2953. Acc.: 21.78%\nEpoch:   4. Loss: 1.2853. Acc.: 21.78%\nEpoch:   4. Loss: 1.2853. Acc.: 21.78%\nEpoch:   5. Loss: 1.2728. Acc.: 29.79%\nEpoch 5 best model saved with accuracy: 29.79%\nEpoch:   6. Loss: 1.2605. Acc.: 29.66%\nEpoch:   7. Loss: 1.2494. Acc.: 29.40%\nEpoch:   8. Loss: 1.2405. Acc.: 29.66%\nEpoch:   8. Loss: 1.2405. Acc.: 29.66%\nEpoch:   9. Loss: 1.2323. Acc.: 29.92%\nEpoch 9 best model saved with accuracy: 29.92%\nEpoch:  10. Loss: 1.2242. Acc.: 30.58%\nEpoch 10 best model saved with accuracy: 30.58%\nEpoch:  11. Loss: 1.2163. Acc.: 31.23%\nEpoch 11 best model saved with accuracy: 31.23%\nEpoch:  12. Loss: 1.2083. Acc.: 32.41%\nEpoch 12 best model saved with accuracy: 32.41%\nEpoch:  13. Loss: 1.2012. Acc.: 32.28%\nEpoch:  14. Loss: 1.1934. Acc.: 33.73%\nEpoch 14 best model saved with accuracy: 33.73%\nEpoch:  15. Loss: 1.1874. Acc.: 33.73%\nEpoch:  16. Loss: 1.1812. Acc.: 33.33%\nEpoch:  16. Loss: 1.1812. Acc.: 33.33%\nEpoch:  17. Loss: 1.1750. Acc.: 35.70%\nEpoch 17 best model saved with accuracy: 35.70%\nEpoch:  18. Loss: 1.1680. Acc.: 34.91%\nEpoch:  19. Loss: 1.1618. Acc.: 36.48%\nEpoch 19 best model saved with accuracy: 36.48%\nEpoch:  20. Loss: 1.1559. Acc.: 36.22%\nEpoch:  21. Loss: 1.1520. Acc.: 37.14%\nEpoch 21 best model saved with accuracy: 37.14%\nEpoch:  22. Loss: 1.1453. Acc.: 36.61%\nEpoch:  23. Loss: 1.1404. Acc.: 34.12%\nEpoch:  24. Loss: 1.1367. Acc.: 36.88%\nEpoch:  25. Loss: 1.1357. Acc.: 37.27%\nEpoch 25 best model saved with accuracy: 37.27%\nEpoch:  26. Loss: 1.1290. Acc.: 34.25%\nEpoch:  27. Loss: 1.1229. Acc.: 35.04%\nEpoch:  28. Loss: 1.1150. Acc.: 37.14%\nEpoch:  29. Loss: 1.1127. Acc.: 38.58%\nEpoch 29 best model saved with accuracy: 38.58%\nEpoch:  30. Loss: 1.1091. Acc.: 37.53%\nEpoch:  31. Loss: 1.1032. Acc.: 39.50%\nEpoch 31 best model saved with accuracy: 39.50%\nEpoch:  32. Loss: 1.1028. Acc.: 40.16%\nEpoch:  32. Loss: 1.1028. Acc.: 40.16%\nEpoch 32 best model saved with accuracy: 40.16%\nEpoch:  33. Loss: 1.0959. Acc.: 38.85%\nEpoch:  34. Loss: 1.0924. Acc.: 38.71%\nEpoch:  35. Loss: 1.0881. Acc.: 39.90%\nEpoch:  36. Loss: 1.0883. Acc.: 39.24%\nEpoch:  37. Loss: 1.0815. Acc.: 39.63%\nEpoch:  38. Loss: 1.0803. Acc.: 38.71%\nEpoch:  39. Loss: 1.0755. Acc.: 39.76%\nEpoch:  40. Loss: 1.0713. Acc.: 38.19%\nEpoch:  41. Loss: 1.0715. Acc.: 37.93%\nEpoch:  42. Loss: 1.0664. Acc.: 37.53%\nEpoch:  43. Loss: 1.0616. Acc.: 38.85%\nEpoch:  44. Loss: 1.0539. Acc.: 39.24%\nEpoch:  45. Loss: 1.0544. Acc.: 39.11%\nEpoch:  46. Loss: 1.0513. Acc.: 38.32%\nEpoch:  47. Loss: 1.0492. Acc.: 38.19%\nEpoch:  48. Loss: 1.0498. Acc.: 38.58%\nEpoch:  49. Loss: 1.0452. Acc.: 37.80%\nEpoch:  50. Loss: 1.0448. Acc.: 37.53%\nEpoch:  51. Loss: 1.0397. Acc.: 39.11%\nEpoch:  52. Loss: 1.0389. Acc.: 38.19%\nEpoch:  53. Loss: 1.0326. Acc.: 39.76%\nEpoch:  54. Loss: 1.0306. Acc.: 39.90%\nEpoch:  55. Loss: 1.0281. Acc.: 38.58%\nEpoch:  56. Loss: 1.0276. Acc.: 39.37%\nEpoch:  57. Loss: 1.0261. Acc.: 40.16%\nEpoch:  58. Loss: 1.0211. Acc.: 39.63%\nEpoch:  59. Loss: 1.0190. Acc.: 39.37%\nEpoch:  60. Loss: 1.0183. Acc.: 39.24%\nEpoch:  61. Loss: 1.0166. Acc.: 39.37%\nEpoch:  62. Loss: 1.0175. Acc.: 39.63%\nEpoch:  63. Loss: 1.0098. Acc.: 39.76%\nEpoch:  64. Loss: 1.0111. Acc.: 40.29%\nEpoch:  64. Loss: 1.0111. Acc.: 40.29%\nEpoch 64 best model saved with accuracy: 40.29%\nEpoch:  65. Loss: 1.0062. Acc.: 38.32%\nEpoch:  66. Loss: 0.9991. Acc.: 37.53%\nEpoch:  67. Loss: 1.0067. Acc.: 38.45%\nEpoch:  68. Loss: 1.0041. Acc.: 38.71%\nEpoch:  69. Loss: 0.9975. Acc.: 39.37%\nEpoch:  70. Loss: 1.0029. Acc.: 39.24%\nEpoch:  71. Loss: 0.9983. Acc.: 39.63%\nEpoch:  72. Loss: 0.9941. Acc.: 39.76%\nEpoch:  73. Loss: 0.9945. Acc.: 39.50%\nEpoch:  74. Loss: 0.9943. Acc.: 38.85%\nEpoch:  75. Loss: 0.9931. Acc.: 39.76%\nEpoch:  76. Loss: 0.9879. Acc.: 40.68%\nEpoch 76 best model saved with accuracy: 40.68%\nEpoch:  77. Loss: 0.9879. Acc.: 39.90%\nEpoch:  78. Loss: 0.9861. Acc.: 38.85%\nEpoch:  79. Loss: 0.9793. Acc.: 38.85%\nEpoch:  80. Loss: 0.9784. Acc.: 38.85%\nEpoch:  81. Loss: 0.9798. Acc.: 39.37%\nEpoch:  82. Loss: 0.9828. Acc.: 39.90%\nEpoch:  83. Loss: 0.9807. Acc.: 41.21%\nEpoch 83 best model saved with accuracy: 41.21%\nEpoch:  84. Loss: 0.9797. Acc.: 40.68%\nEpoch:  85. Loss: 0.9801. Acc.: 40.03%\nEpoch:  86. Loss: 0.9722. Acc.: 38.32%\nEpoch:  87. Loss: 0.9746. Acc.: 39.11%\nEpoch:  88. Loss: 0.9716. Acc.: 39.63%\nEpoch:  89. Loss: 0.9669. Acc.: 39.90%\nEpoch:  90. Loss: 0.9638. Acc.: 38.98%\nEpoch:  91. Loss: 0.9607. Acc.: 38.45%\nEpoch:  92. Loss: 0.9601. Acc.: 38.32%\nEpoch:  93. Loss: 0.9620. Acc.: 40.03%\nEpoch:  94. Loss: 0.9643. Acc.: 39.90%\nEpoch:  95. Loss: 0.9563. Acc.: 40.16%\nEpoch:  96. Loss: 0.9566. Acc.: 40.29%\nEpoch:  97. Loss: 0.9601. Acc.: 40.16%\nEpoch:  98. Loss: 0.9548. Acc.: 41.34%\nEpoch 98 best model saved with accuracy: 41.34%\nEpoch:  99. Loss: 0.9571. Acc.: 40.68%\nEpoch: 100. Loss: 0.9594. Acc.: 40.94%\nEpoch: 101. Loss: 0.9494. Acc.: 41.86%\nEpoch 101 best model saved with accuracy: 41.86%\nEpoch: 102. Loss: 0.9535. Acc.: 41.86%\nEpoch: 103. Loss: 0.9485. Acc.: 40.94%\nEpoch: 104. Loss: 0.9585. Acc.: 41.21%\nEpoch: 105. Loss: 0.9520. Acc.: 41.21%\nEpoch: 106. Loss: 0.9454. Acc.: 39.90%\nEpoch: 107. Loss: 0.9468. Acc.: 40.55%\nEpoch: 108. Loss: 0.9429. Acc.: 40.81%\nEpoch: 109. Loss: 0.9495. Acc.: 40.29%\nEpoch: 110. Loss: 0.9454. Acc.: 40.16%\nEpoch: 111. Loss: 0.9412. Acc.: 40.42%\nEpoch: 112. Loss: 0.9491. Acc.: 40.68%\nEpoch: 113. Loss: 0.9422. Acc.: 41.47%\nEpoch: 114. Loss: 0.9367. Acc.: 41.99%\nEpoch 114 best model saved with accuracy: 41.99%\nEpoch: 115. Loss: 0.9416. Acc.: 41.08%\nEpoch: 116. Loss: 0.9403. Acc.: 40.16%\nEpoch: 117. Loss: 0.9348. Acc.: 40.42%\nEpoch: 118. Loss: 0.9409. Acc.: 41.21%\nEpoch: 119. Loss: 0.9328. Acc.: 41.34%\nEpoch: 120. Loss: 0.9308. Acc.: 41.47%\nEpoch: 121. Loss: 0.9315. Acc.: 41.21%\nEpoch: 122. Loss: 0.9321. Acc.: 41.34%\nEpoch: 123. Loss: 0.9295. Acc.: 41.47%\nEpoch: 124. Loss: 0.9336. Acc.: 41.60%\nEpoch: 125. Loss: 0.9356. Acc.: 40.81%\nEpoch: 126. Loss: 0.9341. Acc.: 39.90%\nEpoch: 127. Loss: 0.9276. Acc.: 41.60%\nEpoch: 128. Loss: 0.9267. Acc.: 41.86%\nEpoch: 128. Loss: 0.9267. Acc.: 41.86%\nEpoch: 129. Loss: 0.9267. Acc.: 41.47%\nEpoch: 130. Loss: 0.9249. Acc.: 41.08%\nEpoch: 131. Loss: 0.9352. Acc.: 41.99%\nEpoch: 132. Loss: 0.9222. Acc.: 41.21%\nEpoch: 133. Loss: 0.9220. Acc.: 42.13%\nEpoch 133 best model saved with accuracy: 42.13%\nEpoch: 134. Loss: 0.9163. Acc.: 41.34%\nEpoch: 135. Loss: 0.9132. Acc.: 40.55%\nEpoch: 136. Loss: 0.9182. Acc.: 41.21%\nEpoch: 137. Loss: 0.9144. Acc.: 41.73%\nEpoch: 138. Loss: 0.9112. Acc.: 39.76%\nEpoch: 139. Loss: 0.9140. Acc.: 40.55%\nEpoch: 140. Loss: 0.9120. Acc.: 41.86%\nEpoch: 141. Loss: 0.9068. Acc.: 41.86%\nEpoch: 142. Loss: 0.9093. Acc.: 41.86%\nEpoch: 143. Loss: 0.9104. Acc.: 41.99%\nEpoch: 144. Loss: 0.9090. Acc.: 41.99%\nEpoch: 145. Loss: 0.9057. Acc.: 41.73%\nEpoch: 146. Loss: 0.9091. Acc.: 42.13%\nEpoch: 147. Loss: 0.9048. Acc.: 42.39%\nEpoch 147 best model saved with accuracy: 42.39%\nEpoch: 148. Loss: 0.9052. Acc.: 41.60%\nEpoch: 149. Loss: 0.9042. Acc.: 41.73%\nEpoch: 150. Loss: 0.9031. Acc.: 43.04%\nEpoch 150 best model saved with accuracy: 43.04%\nEpoch: 151. Loss: 0.8984. Acc.: 42.91%\nEpoch: 152. Loss: 0.9041. Acc.: 41.34%\nEpoch: 153. Loss: 0.9034. Acc.: 40.81%\nEpoch: 154. Loss: 0.9029. Acc.: 41.08%\nEpoch: 155. Loss: 0.9023. Acc.: 42.13%\nEpoch: 156. Loss: 0.8926. Acc.: 42.26%\nEpoch: 157. Loss: 0.9011. Acc.: 43.31%\nEpoch 157 best model saved with accuracy: 43.31%\nEpoch: 158. Loss: 0.8990. Acc.: 43.04%\nEpoch: 159. Loss: 0.8960. Acc.: 43.31%\nEpoch: 160. Loss: 0.8927. Acc.: 43.18%\nEpoch: 161. Loss: 0.8913. Acc.: 41.99%\nEpoch: 162. Loss: 0.8951. Acc.: 41.60%\nEpoch: 163. Loss: 0.8901. Acc.: 41.86%\nEpoch: 164. Loss: 0.8923. Acc.: 41.60%\nEpoch: 165. Loss: 0.8882. Acc.: 42.13%\nEpoch: 166. Loss: 0.8970. Acc.: 41.47%\nEpoch: 167. Loss: 0.8862. Acc.: 42.91%\nEpoch: 168. Loss: 0.8963. Acc.: 43.04%\nEpoch: 169. Loss: 0.8846. Acc.: 41.86%\nEpoch: 170. Loss: 0.8885. Acc.: 42.39%\nEpoch: 171. Loss: 0.8811. Acc.: 42.65%\nEpoch: 172. Loss: 0.8757. Acc.: 41.86%\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 173. Loss: 0.8828. Acc.: 41.99%\nEpoch: 174. Loss: 0.8830. Acc.: 42.78%\nEpoch: 175. Loss: 0.8756. Acc.: 41.99%\nEpoch: 176. Loss: 0.8803. Acc.: 42.13%\nEpoch: 177. Loss: 0.8774. Acc.: 42.13%\nEpoch: 178. Loss: 0.8734. Acc.: 42.91%\nEpoch: 179. Loss: 0.8697. Acc.: 42.65%\nEpoch: 180. Loss: 0.8702. Acc.: 42.39%\nEpoch: 181. Loss: 0.8723. Acc.: 43.18%\nEpoch: 182. Loss: 0.8646. Acc.: 42.65%\nEpoch: 183. Loss: 0.8624. Acc.: 42.52%\nEpoch: 184. Loss: 0.8780. Acc.: 43.44%\nEpoch 184 best model saved with accuracy: 43.44%\nEpoch: 185. Loss: 0.8672. Acc.: 42.65%\nEpoch: 186. Loss: 0.8634. Acc.: 42.13%\nEpoch: 187. Loss: 0.8677. Acc.: 43.04%\nEpoch: 188. Loss: 0.8628. Acc.: 43.04%\nEpoch: 189. Loss: 0.8578. Acc.: 42.26%\nEpoch: 190. Loss: 0.8615. Acc.: 42.52%\nEpoch: 191. Loss: 0.8602. Acc.: 42.52%\nEpoch: 192. Loss: 0.8668. Acc.: 42.39%\nEpoch: 193. Loss: 0.8571. Acc.: 42.13%\nEpoch: 194. Loss: 0.8611. Acc.: 42.65%\nEpoch: 195. Loss: 0.8542. Acc.: 42.39%\nEpoch: 196. Loss: 0.8559. Acc.: 43.83%\nEpoch 196 best model saved with accuracy: 43.83%\nEpoch: 197. Loss: 0.8451. Acc.: 43.04%\nEpoch: 198. Loss: 0.8544. Acc.: 43.04%\nEpoch: 199. Loss: 0.8504. Acc.: 42.65%\nEpoch: 200. Loss: 0.8519. Acc.: 43.44%\nEpoch: 201. Loss: 0.8605. Acc.: 42.52%\nEpoch: 202. Loss: 0.8542. Acc.: 42.78%\nEpoch: 203. Loss: 0.8511. Acc.: 42.65%\nEpoch: 204. Loss: 0.8574. Acc.: 42.78%\nEpoch: 205. Loss: 0.8535. Acc.: 43.70%\nEpoch: 206. Loss: 0.8541. Acc.: 43.57%\nEpoch: 207. Loss: 0.8457. Acc.: 43.04%\nEpoch: 208. Loss: 0.8361. Acc.: 43.04%\nEpoch: 209. Loss: 0.8481. Acc.: 43.44%\nEpoch: 210. Loss: 0.8440. Acc.: 43.44%\nEpoch: 211. Loss: 0.8501. Acc.: 43.44%\nEpoch: 212. Loss: 0.8451. Acc.: 43.44%\nEpoch: 213. Loss: 0.8421. Acc.: 43.04%\nEpoch: 214. Loss: 0.8412. Acc.: 43.18%\nEpoch: 215. Loss: 0.8400. Acc.: 42.91%\nEpoch: 216. Loss: 0.8530. Acc.: 43.04%\nEpoch: 217. Loss: 0.8407. Acc.: 42.78%\nEpoch: 218. Loss: 0.8436. Acc.: 43.83%\nEpoch: 219. Loss: 0.8347. Acc.: 43.44%\nEpoch: 220. Loss: 0.8321. Acc.: 43.31%\nEpoch: 221. Loss: 0.8395. Acc.: 43.31%\nEpoch: 222. Loss: 0.8485. Acc.: 42.78%\nEpoch: 223. Loss: 0.8433. Acc.: 43.44%\nEpoch: 224. Loss: 0.8296. Acc.: 43.83%\nEpoch: 225. Loss: 0.8295. Acc.: 43.18%\nEpoch: 226. Loss: 0.8381. Acc.: 43.18%\nEpoch: 227. Loss: 0.8292. Acc.: 43.04%\nEpoch: 228. Loss: 0.8275. Acc.: 43.18%\nEpoch: 229. Loss: 0.8269. Acc.: 44.09%\nEpoch 229 best model saved with accuracy: 44.09%\nEpoch: 230. Loss: 0.8345. Acc.: 43.44%\nEpoch: 231. Loss: 0.8307. Acc.: 43.31%\nEpoch: 232. Loss: 0.8252. Acc.: 43.70%\nEpoch: 233. Loss: 0.8345. Acc.: 43.70%\nEpoch: 234. Loss: 0.8308. Acc.: 44.09%\nEpoch: 235. Loss: 0.8329. Acc.: 43.44%\nEpoch: 236. Loss: 0.8226. Acc.: 44.36%\nEpoch 236 best model saved with accuracy: 44.36%\nEpoch: 237. Loss: 0.8310. Acc.: 43.31%\nEpoch: 238. Loss: 0.8244. Acc.: 44.36%\nEpoch: 239. Loss: 0.8248. Acc.: 43.70%\nEpoch: 240. Loss: 0.8242. Acc.: 42.39%\nEpoch: 241. Loss: 0.8269. Acc.: 43.70%\nEpoch: 242. Loss: 0.8235. Acc.: 43.04%\nEpoch: 243. Loss: 0.8260. Acc.: 44.09%\nEpoch: 244. Loss: 0.8264. Acc.: 43.57%\nEpoch: 245. Loss: 0.8236. Acc.: 44.23%\nEpoch: 246. Loss: 0.8203. Acc.: 43.70%\nEpoch: 247. Loss: 0.8128. Acc.: 44.36%\nEpoch: 248. Loss: 0.8284. Acc.: 43.96%\nEpoch: 249. Loss: 0.8191. Acc.: 43.96%\nEpoch: 250. Loss: 0.8176. Acc.: 43.57%\nEpoch: 251. Loss: 0.8250. Acc.: 43.96%\nEpoch: 252. Loss: 0.8138. Acc.: 43.83%\nEpoch: 253. Loss: 0.8246. Acc.: 43.96%\nEpoch: 254. Loss: 0.8148. Acc.: 43.31%\nEpoch: 255. Loss: 0.8077. Acc.: 44.36%\nEpoch: 256. Loss: 0.8162. Acc.: 44.49%\nEpoch: 256. Loss: 0.8162. Acc.: 44.49%\nEpoch 256 best model saved with accuracy: 44.49%\nEpoch: 257. Loss: 0.8110. Acc.: 43.44%\nEpoch: 258. Loss: 0.8147. Acc.: 44.23%\nEpoch: 259. Loss: 0.8187. Acc.: 43.83%\nEpoch: 260. Loss: 0.8157. Acc.: 43.70%\nEpoch: 261. Loss: 0.8164. Acc.: 44.23%\nEpoch: 262. Loss: 0.8096. Acc.: 44.49%\nEpoch: 263. Loss: 0.8096. Acc.: 44.09%\nEpoch: 264. Loss: 0.8057. Acc.: 44.36%\nEpoch: 265. Loss: 0.7994. Acc.: 43.70%\nEpoch: 266. Loss: 0.8042. Acc.: 44.23%\nEpoch: 267. Loss: 0.7983. Acc.: 44.23%\nEpoch: 268. Loss: 0.7968. Acc.: 44.23%\nEpoch: 269. Loss: 0.8042. Acc.: 44.75%\nEpoch 269 best model saved with accuracy: 44.75%\nEpoch: 270. Loss: 0.8085. Acc.: 43.44%\nEpoch: 271. Loss: 0.8106. Acc.: 43.83%\nEpoch: 272. Loss: 0.8094. Acc.: 43.96%\nEpoch: 273. Loss: 0.7985. Acc.: 43.96%\nEpoch: 274. Loss: 0.7975. Acc.: 43.57%\nEpoch: 275. Loss: 0.8007. Acc.: 44.36%\nEpoch: 276. Loss: 0.7942. Acc.: 44.62%\nEpoch: 277. Loss: 0.7913. Acc.: 44.75%\nEpoch: 278. Loss: 0.7954. Acc.: 45.14%\nEpoch 278 best model saved with accuracy: 45.14%\nEpoch: 279. Loss: 0.7954. Acc.: 44.62%\nEpoch: 280. Loss: 0.7909. Acc.: 45.28%\nEpoch 280 best model saved with accuracy: 45.28%\nEpoch: 281. Loss: 0.7971. Acc.: 44.75%\nEpoch: 282. Loss: 0.7895. Acc.: 44.23%\nEpoch: 283. Loss: 0.7848. Acc.: 45.14%\nEpoch: 284. Loss: 0.7968. Acc.: 45.01%\nEpoch: 285. Loss: 0.7936. Acc.: 48.82%\nEpoch 285 best model saved with accuracy: 48.82%\nEpoch: 286. Loss: 0.7919. Acc.: 49.08%\nEpoch 286 best model saved with accuracy: 49.08%\nEpoch: 287. Loss: 0.7921. Acc.: 50.66%\nEpoch 287 best model saved with accuracy: 50.66%\nEpoch: 288. Loss: 0.7881. Acc.: 49.61%\nEpoch: 289. Loss: 0.7946. Acc.: 48.69%\nEpoch: 290. Loss: 0.7857. Acc.: 48.16%\nEpoch: 291. Loss: 0.7806. Acc.: 47.64%\nEpoch: 292. Loss: 0.7883. Acc.: 49.48%\nEpoch: 293. Loss: 0.7839. Acc.: 48.16%\nEpoch: 294. Loss: 0.7911. Acc.: 49.74%\nEpoch: 295. Loss: 0.7847. Acc.: 49.61%\nEpoch: 296. Loss: 0.7788. Acc.: 48.16%\nEpoch: 297. Loss: 0.7843. Acc.: 49.74%\nEpoch: 298. Loss: 0.7818. Acc.: 48.03%\nEpoch: 299. Loss: 0.7766. Acc.: 48.56%\nEpoch: 300. Loss: 0.7817. Acc.: 48.03%\nEpoch: 301. Loss: 0.7831. Acc.: 48.03%\nEpoch: 302. Loss: 0.7706. Acc.: 47.64%\nEpoch: 303. Loss: 0.7630. Acc.: 46.46%\nEpoch: 304. Loss: 0.7648. Acc.: 49.61%\nEpoch: 305. Loss: 0.7773. Acc.: 46.85%\nEpoch: 306. Loss: 0.7654. Acc.: 48.43%\nEpoch: 307. Loss: 0.7664. Acc.: 49.74%\nEpoch: 308. Loss: 0.7725. Acc.: 47.77%\nEpoch: 309. Loss: 0.7658. Acc.: 48.82%\nEpoch: 310. Loss: 0.7743. Acc.: 49.61%\nEpoch: 311. Loss: 0.7756. Acc.: 48.43%\nEpoch: 312. Loss: 0.7745. Acc.: 49.74%\nEpoch: 313. Loss: 0.7719. Acc.: 49.87%\nEpoch: 314. Loss: 0.7674. Acc.: 49.61%\nEpoch: 315. Loss: 0.7577. Acc.: 49.74%\nEpoch: 316. Loss: 0.7648. Acc.: 49.21%\nEpoch: 317. Loss: 0.7637. Acc.: 49.74%\nEpoch: 318. Loss: 0.7594. Acc.: 48.69%\nEpoch: 319. Loss: 0.7649. Acc.: 48.69%\nEpoch: 320. Loss: 0.7537. Acc.: 50.26%\nEpoch: 321. Loss: 0.7674. Acc.: 48.03%\nEpoch: 322. Loss: 0.7621. Acc.: 48.56%\nEpoch: 323. Loss: 0.7616. Acc.: 48.29%\nEpoch: 324. Loss: 0.7470. Acc.: 48.82%\nEpoch: 325. Loss: 0.7508. Acc.: 49.48%\nEpoch: 326. Loss: 0.7407. Acc.: 50.92%\nEpoch 326 best model saved with accuracy: 50.92%\nEpoch: 327. Loss: 0.7508. Acc.: 47.24%\nEpoch: 328. Loss: 0.7512. Acc.: 48.82%\nEpoch: 329. Loss: 0.7495. Acc.: 47.11%\nEpoch: 330. Loss: 0.7619. Acc.: 49.48%\nEpoch: 331. Loss: 0.7590. Acc.: 45.54%\nEpoch: 332. Loss: 0.7534. Acc.: 49.48%\nEpoch: 333. Loss: 0.7452. Acc.: 48.16%\nEpoch: 334. Loss: 0.7366. Acc.: 49.34%\nEpoch: 335. Loss: 0.7369. Acc.: 48.43%\nEpoch: 336. Loss: 0.7425. Acc.: 48.56%\nEpoch: 337. Loss: 0.7371. Acc.: 49.48%\nEpoch: 338. Loss: 0.7310. Acc.: 49.74%\nEpoch: 339. Loss: 0.7338. Acc.: 48.95%\nEpoch: 340. Loss: 0.7389. Acc.: 48.69%\nEpoch: 341. Loss: 0.7259. Acc.: 49.08%\nEpoch: 342. Loss: 0.7268. Acc.: 50.00%\nEpoch: 343. Loss: 0.7210. Acc.: 49.48%\nEpoch: 344. Loss: 0.7324. Acc.: 48.82%\nEpoch: 345. Loss: 0.7226. Acc.: 49.48%\nEpoch: 346. Loss: 0.7276. Acc.: 48.56%\nEpoch: 347. Loss: 0.7223. Acc.: 50.00%\nEpoch: 348. Loss: 0.7228. Acc.: 49.61%\nEpoch: 349. Loss: 0.7334. Acc.: 49.34%\nEpoch: 350. Loss: 0.7358. Acc.: 49.08%\nEpoch: 351. Loss: 0.7242. Acc.: 48.43%\nEpoch: 352. Loss: 0.7209. Acc.: 49.48%\nEpoch: 353. Loss: 0.7218. Acc.: 49.87%\nEpoch: 354. Loss: 0.7184. Acc.: 48.95%\nEpoch: 355. Loss: 0.7063. Acc.: 50.66%\nEpoch: 356. Loss: 0.7092. Acc.: 49.61%\nEpoch: 357. Loss: 0.7085. Acc.: 49.34%\nEpoch: 358. Loss: 0.7133. Acc.: 49.61%\nEpoch: 359. Loss: 0.6994. Acc.: 48.56%\nEpoch: 360. Loss: 0.7145. Acc.: 49.61%\nEpoch: 361. Loss: 0.7074. Acc.: 49.74%\nEpoch: 362. Loss: 0.7027. Acc.: 49.74%\nEpoch: 363. Loss: 0.7021. Acc.: 49.61%\nEpoch: 364. Loss: 0.6903. Acc.: 50.26%\nEpoch: 365. Loss: 0.6935. Acc.: 48.95%\nEpoch: 366. Loss: 0.6973. Acc.: 49.21%\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 367. Loss: 0.6979. Acc.: 50.52%\nEpoch: 368. Loss: 0.6864. Acc.: 47.90%\nEpoch: 369. Loss: 0.6999. Acc.: 48.03%\nEpoch: 370. Loss: 0.6919. Acc.: 47.64%\nEpoch: 371. Loss: 0.6879. Acc.: 49.87%\nEpoch: 372. Loss: 0.6955. Acc.: 46.33%\nEpoch: 373. Loss: 0.6918. Acc.: 49.87%\nEpoch: 374. Loss: 0.6925. Acc.: 48.82%\nEpoch: 375. Loss: 0.6854. Acc.: 46.72%\nEpoch: 376. Loss: 0.6942. Acc.: 49.34%\nEpoch: 377. Loss: 0.6807. Acc.: 49.34%\nEpoch: 378. Loss: 0.6968. Acc.: 49.74%\nEpoch: 379. Loss: 0.6788. Acc.: 48.95%\nEpoch: 380. Loss: 0.6856. Acc.: 49.48%\nEpoch: 381. Loss: 0.6893. Acc.: 49.08%\nEpoch: 382. Loss: 0.6814. Acc.: 50.00%\nEpoch: 383. Loss: 0.6793. Acc.: 48.56%\nEpoch: 384. Loss: 0.6902. Acc.: 49.34%\nEpoch: 385. Loss: 0.6813. Acc.: 47.90%\nEpoch: 386. Loss: 0.6752. Acc.: 48.69%\nEpoch: 387. Loss: 0.6658. Acc.: 50.79%\nEpoch: 388. Loss: 0.6680. Acc.: 49.74%\nEpoch: 389. Loss: 0.6729. Acc.: 49.34%\nEpoch: 390. Loss: 0.6822. Acc.: 50.13%\nEpoch: 391. Loss: 0.6666. Acc.: 49.48%\nEpoch: 392. Loss: 0.6685. Acc.: 48.43%\nEpoch: 393. Loss: 0.6809. Acc.: 51.05%\nEpoch 393 best model saved with accuracy: 51.05%\nEpoch: 394. Loss: 0.6775. Acc.: 49.34%\nEpoch: 395. Loss: 0.6759. Acc.: 48.95%\nEpoch: 396. Loss: 0.6650. Acc.: 49.08%\nEpoch: 397. Loss: 0.6683. Acc.: 50.00%\nEpoch: 398. Loss: 0.6643. Acc.: 50.39%\nEpoch: 399. Loss: 0.6779. Acc.: 47.64%\nEpoch: 400. Loss: 0.6620. Acc.: 48.56%\nEpoch: 401. Loss: 0.6609. Acc.: 48.56%\nEpoch: 402. Loss: 0.6553. Acc.: 48.82%\nEpoch: 403. Loss: 0.6695. Acc.: 48.43%\nEpoch: 404. Loss: 0.6654. Acc.: 49.61%\nEpoch: 405. Loss: 0.6584. Acc.: 47.77%\nEpoch: 406. Loss: 0.6542. Acc.: 48.16%\nEpoch: 407. Loss: 0.6515. Acc.: 50.26%\nEpoch: 408. Loss: 0.6567. Acc.: 49.34%\nEpoch: 409. Loss: 0.6480. Acc.: 49.08%\nEpoch: 410. Loss: 0.6482. Acc.: 48.95%\nEpoch: 411. Loss: 0.6621. Acc.: 48.56%\nEpoch: 412. Loss: 0.6545. Acc.: 48.43%\nEpoch: 413. Loss: 0.6544. Acc.: 48.03%\nEpoch: 414. Loss: 0.6600. Acc.: 47.24%\nEpoch: 415. Loss: 0.6533. Acc.: 49.34%\nEpoch: 416. Loss: 0.6503. Acc.: 48.03%\nEpoch: 417. Loss: 0.6529. Acc.: 49.74%\nEpoch: 418. Loss: 0.6452. Acc.: 48.82%\nEpoch: 419. Loss: 0.6474. Acc.: 49.48%\nEpoch: 420. Loss: 0.6407. Acc.: 49.87%\nEpoch: 421. Loss: 0.6410. Acc.: 50.39%\nEpoch: 422. Loss: 0.6453. Acc.: 48.43%\nEpoch: 423. Loss: 0.6459. Acc.: 49.48%\nEpoch: 424. Loss: 0.6356. Acc.: 49.61%\nEpoch: 425. Loss: 0.6485. Acc.: 50.66%\nEpoch: 426. Loss: 0.6309. Acc.: 48.29%\nEpoch: 427. Loss: 0.6542. Acc.: 49.21%\nEpoch: 428. Loss: 0.6464. Acc.: 49.48%\nEpoch: 429. Loss: 0.6416. Acc.: 49.21%\nEpoch: 430. Loss: 0.6492. Acc.: 49.34%\nEpoch: 431. Loss: 0.6454. Acc.: 49.74%\nEpoch: 432. Loss: 0.6228. Acc.: 49.48%\nEpoch: 433. Loss: 0.6330. Acc.: 49.08%\nEpoch: 434. Loss: 0.6422. Acc.: 49.48%\nEpoch: 435. Loss: 0.6300. Acc.: 48.82%\nEpoch: 436. Loss: 0.6396. Acc.: 47.90%\nEpoch: 437. Loss: 0.6335. Acc.: 52.76%\nEpoch 437 best model saved with accuracy: 52.76%\nEpoch: 438. Loss: 0.6320. Acc.: 51.84%\nEpoch: 439. Loss: 0.6372. Acc.: 51.18%\nEpoch: 440. Loss: 0.6238. Acc.: 52.10%\nEpoch: 441. Loss: 0.6370. Acc.: 52.76%\nEpoch: 442. Loss: 0.6405. Acc.: 53.41%\nEpoch 442 best model saved with accuracy: 53.41%\nEpoch: 443. Loss: 0.6335. Acc.: 54.46%\nEpoch 443 best model saved with accuracy: 54.46%\nEpoch: 444. Loss: 0.6268. Acc.: 53.54%\nEpoch: 445. Loss: 0.6296. Acc.: 51.71%\nEpoch: 446. Loss: 0.6373. Acc.: 52.76%\nEpoch: 447. Loss: 0.6239. Acc.: 54.33%\nEpoch: 448. Loss: 0.6247. Acc.: 54.59%\nEpoch 448 best model saved with accuracy: 54.59%\nEpoch: 449. Loss: 0.6175. Acc.: 53.41%\nEpoch: 450. Loss: 0.6344. Acc.: 55.77%\nEpoch 450 best model saved with accuracy: 55.77%\nEpoch: 451. Loss: 0.6241. Acc.: 54.72%\nEpoch: 452. Loss: 0.6185. Acc.: 55.12%\nEpoch: 453. Loss: 0.6218. Acc.: 55.51%\nEpoch: 454. Loss: 0.6209. Acc.: 56.17%\nEpoch 454 best model saved with accuracy: 56.17%\nEpoch: 455. Loss: 0.6245. Acc.: 53.81%\nEpoch: 456. Loss: 0.6156. Acc.: 52.89%\nEpoch: 457. Loss: 0.6138. Acc.: 52.89%\nEpoch: 458. Loss: 0.6301. Acc.: 54.86%\nEpoch: 459. Loss: 0.6174. Acc.: 54.72%\nEpoch: 460. Loss: 0.6189. Acc.: 55.12%\nEpoch: 461. Loss: 0.6096. Acc.: 54.99%\nEpoch: 462. Loss: 0.6269. Acc.: 53.94%\nEpoch: 463. Loss: 0.6128. Acc.: 54.99%\nEpoch: 464. Loss: 0.6218. Acc.: 54.99%\nEpoch: 465. Loss: 0.6163. Acc.: 53.94%\nEpoch: 466. Loss: 0.6132. Acc.: 55.25%\nEpoch: 467. Loss: 0.6138. Acc.: 54.86%\nEpoch: 468. Loss: 0.6240. Acc.: 55.12%\nEpoch: 469. Loss: 0.6097. Acc.: 55.25%\nEpoch: 470. Loss: 0.6101. Acc.: 53.81%\nEpoch: 471. Loss: 0.6185. Acc.: 54.46%\nEpoch: 472. Loss: 0.6120. Acc.: 55.25%\nEpoch: 473. Loss: 0.5944. Acc.: 55.91%\nEpoch: 474. Loss: 0.5991. Acc.: 54.59%\nEpoch: 475. Loss: 0.6134. Acc.: 54.46%\nEpoch: 476. Loss: 0.6026. Acc.: 55.51%\nEpoch: 477. Loss: 0.5973. Acc.: 54.86%\nEpoch: 478. Loss: 0.6049. Acc.: 55.12%\nEpoch: 479. Loss: 0.6083. Acc.: 54.07%\nEpoch: 480. Loss: 0.6082. Acc.: 54.99%\nEpoch: 481. Loss: 0.5960. Acc.: 55.25%\nEpoch: 482. Loss: 0.6087. Acc.: 55.64%\nEpoch: 483. Loss: 0.5898. Acc.: 56.17%\nEpoch: 484. Loss: 0.5981. Acc.: 56.43%\nEpoch 484 best model saved with accuracy: 56.43%\nEpoch: 485. Loss: 0.6023. Acc.: 57.22%\nEpoch 485 best model saved with accuracy: 57.22%\nEpoch: 486. Loss: 0.5946. Acc.: 56.17%\nEpoch: 487. Loss: 0.5936. Acc.: 56.30%\nEpoch: 488. Loss: 0.5979. Acc.: 57.09%\nEpoch: 489. Loss: 0.5883. Acc.: 55.77%\nEpoch: 490. Loss: 0.5982. Acc.: 55.25%\nEpoch: 491. Loss: 0.5925. Acc.: 56.04%\nEpoch: 492. Loss: 0.6101. Acc.: 56.96%\nEpoch: 493. Loss: 0.5991. Acc.: 56.56%\nEpoch: 494. Loss: 0.5905. Acc.: 56.82%\nEpoch: 495. Loss: 0.6058. Acc.: 56.82%\nEpoch: 496. Loss: 0.5931. Acc.: 56.17%\nEpoch: 497. Loss: 0.5834. Acc.: 56.96%\nEpoch: 498. Loss: 0.5856. Acc.: 56.82%\nEpoch: 499. Loss: 0.5867. Acc.: 57.09%\nEpoch: 500. Loss: 0.5940. Acc.: 57.35%\nEpoch 500 best model saved with accuracy: 57.35%\nEpoch: 501. Loss: 0.5873. Acc.: 55.77%\nEpoch: 502. Loss: 0.5895. Acc.: 56.56%\nEpoch: 503. Loss: 0.5886. Acc.: 56.96%\nEpoch: 504. Loss: 0.5774. Acc.: 55.77%\nEpoch: 505. Loss: 0.5767. Acc.: 57.48%\nEpoch 505 best model saved with accuracy: 57.48%\nEpoch: 506. Loss: 0.5662. Acc.: 56.82%\nEpoch: 507. Loss: 0.5710. Acc.: 56.96%\nEpoch: 508. Loss: 0.5895. Acc.: 57.09%\nEpoch: 509. Loss: 0.5781. Acc.: 56.17%\nEpoch: 510. Loss: 0.5752. Acc.: 56.56%\nEpoch: 511. Loss: 0.5853. Acc.: 55.91%\nEpoch: 512. Loss: 0.5801. Acc.: 55.91%\nEpoch: 512. Loss: 0.5801. Acc.: 55.91%\nEpoch: 513. Loss: 0.5712. Acc.: 56.69%\nEpoch: 514. Loss: 0.5708. Acc.: 56.17%\nEpoch: 515. Loss: 0.5852. Acc.: 56.04%\nEpoch: 516. Loss: 0.5852. Acc.: 57.61%\nEpoch 516 best model saved with accuracy: 57.61%\nEpoch: 517. Loss: 0.5902. Acc.: 55.12%\nEpoch: 518. Loss: 0.5653. Acc.: 55.51%\nEpoch: 519. Loss: 0.5746. Acc.: 55.77%\nEpoch: 520. Loss: 0.5786. Acc.: 55.51%\nEpoch: 521. Loss: 0.5718. Acc.: 56.30%\nEpoch: 522. Loss: 0.5817. Acc.: 55.91%\nEpoch: 523. Loss: 0.5731. Acc.: 56.43%\nEpoch: 524. Loss: 0.5577. Acc.: 55.38%\nEpoch: 525. Loss: 0.5735. Acc.: 55.51%\nEpoch: 526. Loss: 0.5728. Acc.: 55.25%\nEpoch: 527. Loss: 0.5683. Acc.: 55.64%\nEpoch: 528. Loss: 0.5688. Acc.: 56.17%\nEpoch: 529. Loss: 0.5614. Acc.: 56.04%\nEpoch: 530. Loss: 0.5795. Acc.: 53.94%\nEpoch: 531. Loss: 0.5719. Acc.: 55.38%\nEpoch: 532. Loss: 0.5568. Acc.: 56.04%\nEpoch: 533. Loss: 0.5614. Acc.: 56.17%\nEpoch: 534. Loss: 0.5647. Acc.: 57.35%\nEpoch: 535. Loss: 0.5578. Acc.: 56.56%\nEpoch: 536. Loss: 0.5604. Acc.: 56.56%\nEpoch: 537. Loss: 0.5667. Acc.: 56.96%\nEpoch: 538. Loss: 0.5688. Acc.: 56.69%\nEpoch: 539. Loss: 0.5706. Acc.: 56.82%\nEpoch: 540. Loss: 0.5530. Acc.: 56.56%\nEpoch: 541. Loss: 0.5646. Acc.: 57.22%\nEpoch: 542. Loss: 0.5770. Acc.: 57.48%\nEpoch: 543. Loss: 0.5775. Acc.: 56.43%\nEpoch: 544. Loss: 0.5695. Acc.: 57.87%\nEpoch 544 best model saved with accuracy: 57.87%\nEpoch: 545. Loss: 0.5682. Acc.: 56.56%\nEpoch: 546. Loss: 0.5638. Acc.: 56.43%\nEpoch: 547. Loss: 0.5570. Acc.: 56.56%\nEpoch: 548. Loss: 0.5582. Acc.: 56.56%\nEpoch: 549. Loss: 0.5589. Acc.: 57.09%\nEpoch: 550. Loss: 0.5671. Acc.: 56.69%\nEpoch: 551. Loss: 0.5618. Acc.: 56.69%\nEpoch: 552. Loss: 0.5684. Acc.: 55.91%\nEpoch: 553. Loss: 0.5600. Acc.: 55.77%\nEpoch: 554. Loss: 0.5599. Acc.: 56.56%\nEpoch: 555. Loss: 0.5528. Acc.: 57.61%\nEpoch: 556. Loss: 0.5656. Acc.: 58.27%\nEpoch 556 best model saved with accuracy: 58.27%\nEpoch: 557. Loss: 0.5523. Acc.: 56.96%\nEpoch: 558. Loss: 0.5548. Acc.: 56.96%\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 559. Loss: 0.5568. Acc.: 56.82%\nEpoch: 560. Loss: 0.5532. Acc.: 56.04%\nEpoch: 561. Loss: 0.5528. Acc.: 58.01%\nEpoch: 562. Loss: 0.5524. Acc.: 58.14%\nEpoch: 563. Loss: 0.5490. Acc.: 57.74%\nEpoch: 564. Loss: 0.5593. Acc.: 57.22%\nEpoch: 565. Loss: 0.5558. Acc.: 57.48%\nEpoch: 566. Loss: 0.5568. Acc.: 57.35%\nEpoch: 567. Loss: 0.5582. Acc.: 56.17%\nEpoch: 568. Loss: 0.5528. Acc.: 56.43%\nEpoch: 569. Loss: 0.5514. Acc.: 57.74%\nEpoch: 570. Loss: 0.5561. Acc.: 57.61%\nEpoch: 571. Loss: 0.5535. Acc.: 57.87%\nEpoch: 572. Loss: 0.5437. Acc.: 58.53%\nEpoch 572 best model saved with accuracy: 58.53%\nEpoch: 573. Loss: 0.5425. Acc.: 56.30%\nEpoch: 574. Loss: 0.5549. Acc.: 56.82%\nEpoch: 575. Loss: 0.5521. Acc.: 56.69%\nEpoch: 576. Loss: 0.5508. Acc.: 56.04%\nEpoch: 577. Loss: 0.5476. Acc.: 57.35%\nEpoch: 578. Loss: 0.5598. Acc.: 57.35%\nEpoch: 579. Loss: 0.5555. Acc.: 57.87%\nEpoch: 580. Loss: 0.5508. Acc.: 58.14%\nEpoch: 581. Loss: 0.5462. Acc.: 58.40%\nEpoch: 582. Loss: 0.5440. Acc.: 57.48%\nEpoch: 583. Loss: 0.5489. Acc.: 57.48%\nEpoch: 584. Loss: 0.5432. Acc.: 57.61%\nEpoch: 585. Loss: 0.5390. Acc.: 58.01%\nEpoch: 586. Loss: 0.5544. Acc.: 58.66%\nEpoch 586 best model saved with accuracy: 58.66%\nEpoch: 587. Loss: 0.5447. Acc.: 57.74%\nEpoch: 588. Loss: 0.5530. Acc.: 56.82%\nEpoch: 589. Loss: 0.5412. Acc.: 57.87%\nEpoch: 590. Loss: 0.5389. Acc.: 57.35%\nEpoch: 591. Loss: 0.5388. Acc.: 58.53%\nEpoch: 592. Loss: 0.5346. Acc.: 58.66%\nEpoch: 593. Loss: 0.5371. Acc.: 56.82%\nEpoch: 594. Loss: 0.5335. Acc.: 57.35%\nEpoch: 595. Loss: 0.5369. Acc.: 56.30%\nEpoch: 596. Loss: 0.5345. Acc.: 58.53%\nEpoch: 597. Loss: 0.5308. Acc.: 58.27%\nEpoch: 598. Loss: 0.5377. Acc.: 57.74%\nEpoch: 599. Loss: 0.5384. Acc.: 57.61%\nEpoch: 600. Loss: 0.5192. Acc.: 56.17%\nEpoch: 601. Loss: 0.5297. Acc.: 56.69%\nEpoch: 602. Loss: 0.5334. Acc.: 58.53%\nEpoch: 603. Loss: 0.5341. Acc.: 59.19%\nEpoch 603 best model saved with accuracy: 59.19%\nEpoch: 604. Loss: 0.5224. Acc.: 57.61%\nEpoch: 605. Loss: 0.5365. Acc.: 58.53%\nEpoch: 606. Loss: 0.5216. Acc.: 57.74%\nEpoch: 607. Loss: 0.5382. Acc.: 58.66%\nEpoch: 608. Loss: 0.5275. Acc.: 58.27%\nEpoch: 609. Loss: 0.5286. Acc.: 58.27%\nEpoch: 610. Loss: 0.5340. Acc.: 59.97%\nEpoch 610 best model saved with accuracy: 59.97%\nEpoch: 611. Loss: 0.5324. Acc.: 58.01%\nEpoch: 612. Loss: 0.5255. Acc.: 58.01%\nEpoch: 613. Loss: 0.5326. Acc.: 58.92%\nEpoch: 614. Loss: 0.5358. Acc.: 59.45%\nEpoch: 615. Loss: 0.5322. Acc.: 59.19%\nEpoch: 616. Loss: 0.5312. Acc.: 61.55%\nEpoch 616 best model saved with accuracy: 61.55%\nEpoch: 617. Loss: 0.5272. Acc.: 63.25%\nEpoch 617 best model saved with accuracy: 63.25%\nEpoch: 618. Loss: 0.5244. Acc.: 65.09%\nEpoch 618 best model saved with accuracy: 65.09%\nEpoch: 619. Loss: 0.5171. Acc.: 63.39%\nEpoch: 620. Loss: 0.5088. Acc.: 64.30%\nEpoch: 621. Loss: 0.5088. Acc.: 62.34%\nEpoch: 622. Loss: 0.5140. Acc.: 62.86%\nEpoch: 623. Loss: 0.5131. Acc.: 62.60%\nEpoch: 624. Loss: 0.5012. Acc.: 64.96%\nEpoch: 625. Loss: 0.5117. Acc.: 63.78%\nEpoch: 626. Loss: 0.5153. Acc.: 65.88%\nEpoch 626 best model saved with accuracy: 65.88%\nEpoch: 627. Loss: 0.5067. Acc.: 63.65%\nEpoch: 628. Loss: 0.4957. Acc.: 63.78%\nEpoch: 629. Loss: 0.4970. Acc.: 64.96%\nEpoch: 630. Loss: 0.4962. Acc.: 63.78%\nEpoch: 631. Loss: 0.4876. Acc.: 64.44%\nEpoch: 632. Loss: 0.5015. Acc.: 63.65%\nEpoch: 633. Loss: 0.4955. Acc.: 63.39%\nEpoch: 634. Loss: 0.5032. Acc.: 63.65%\nEpoch: 635. Loss: 0.4848. Acc.: 63.25%\nEpoch: 636. Loss: 0.4941. Acc.: 64.44%\nEpoch: 637. Loss: 0.4858. Acc.: 64.57%\nEpoch: 638. Loss: 0.5036. Acc.: 62.73%\nEpoch: 639. Loss: 0.4829. Acc.: 63.65%\nEpoch: 640. Loss: 0.5039. Acc.: 64.17%\nEpoch: 641. Loss: 0.4752. Acc.: 63.78%\nEpoch: 642. Loss: 0.4850. Acc.: 63.39%\nEpoch: 643. Loss: 0.4890. Acc.: 62.99%\nEpoch: 644. Loss: 0.4888. Acc.: 64.17%\nEpoch: 645. Loss: 0.4709. Acc.: 63.52%\nEpoch: 646. Loss: 0.4699. Acc.: 62.99%\nEpoch: 647. Loss: 0.4757. Acc.: 62.99%\nEpoch: 648. Loss: 0.4755. Acc.: 63.78%\nEpoch: 649. Loss: 0.4876. Acc.: 64.44%\nEpoch: 650. Loss: 0.4865. Acc.: 63.78%\nEpoch: 651. Loss: 0.4951. Acc.: 64.30%\nEpoch: 652. Loss: 0.4701. Acc.: 64.17%\nEpoch: 653. Loss: 0.4662. Acc.: 64.70%\nEpoch: 654. Loss: 0.4716. Acc.: 63.65%\nEpoch: 655. Loss: 0.4729. Acc.: 64.70%\nEpoch: 656. Loss: 0.4781. Acc.: 64.57%\nEpoch: 657. Loss: 0.4697. Acc.: 65.88%\nEpoch: 658. Loss: 0.4672. Acc.: 65.75%\nEpoch: 659. Loss: 0.4725. Acc.: 64.57%\nEpoch: 660. Loss: 0.4680. Acc.: 65.75%\nEpoch: 661. Loss: 0.4758. Acc.: 64.96%\nEpoch: 662. Loss: 0.4591. Acc.: 63.65%\nEpoch: 663. Loss: 0.4770. Acc.: 64.57%\nEpoch: 664. Loss: 0.4617. Acc.: 65.35%\nEpoch: 665. Loss: 0.4646. Acc.: 63.12%\nEpoch: 666. Loss: 0.4660. Acc.: 64.04%\nEpoch: 667. Loss: 0.4667. Acc.: 64.30%\nEpoch: 668. Loss: 0.4707. Acc.: 65.75%\nEpoch: 669. Loss: 0.4628. Acc.: 64.44%\nEpoch: 670. Loss: 0.4683. Acc.: 63.65%\nEpoch: 671. Loss: 0.4667. Acc.: 65.09%\nEpoch: 672. Loss: 0.4674. Acc.: 64.57%\nEpoch: 673. Loss: 0.4525. Acc.: 65.75%\nEpoch: 674. Loss: 0.4620. Acc.: 65.09%\nEpoch: 675. Loss: 0.4576. Acc.: 65.49%\nEpoch: 676. Loss: 0.4642. Acc.: 63.52%\nEpoch: 677. Loss: 0.4554. Acc.: 63.52%\nEpoch: 678. Loss: 0.4488. Acc.: 63.65%\nEpoch: 679. Loss: 0.4634. Acc.: 63.25%\nEpoch: 680. Loss: 0.4548. Acc.: 64.30%\nEpoch: 681. Loss: 0.4428. Acc.: 65.09%\nEpoch: 682. Loss: 0.4537. Acc.: 64.57%\nEpoch: 683. Loss: 0.4541. Acc.: 64.17%\nEpoch: 684. Loss: 0.4618. Acc.: 64.70%\nEpoch: 685. Loss: 0.4498. Acc.: 64.70%\nEpoch: 686. Loss: 0.4402. Acc.: 64.57%\nEpoch: 687. Loss: 0.4541. Acc.: 64.83%\nEpoch: 688. Loss: 0.4564. Acc.: 65.35%\nEpoch: 689. Loss: 0.4523. Acc.: 64.96%\nEpoch: 690. Loss: 0.4487. Acc.: 65.62%\nEpoch: 691. Loss: 0.4510. Acc.: 65.22%\nEpoch: 692. Loss: 0.4396. Acc.: 65.49%\nEpoch: 693. Loss: 0.4540. Acc.: 64.17%\nEpoch: 694. Loss: 0.4423. Acc.: 63.78%\nEpoch: 695. Loss: 0.4534. Acc.: 63.91%\nEpoch: 696. Loss: 0.4398. Acc.: 65.49%\nEpoch: 697. Loss: 0.4379. Acc.: 64.57%\nEpoch: 698. Loss: 0.4611. Acc.: 66.27%\nEpoch 698 best model saved with accuracy: 66.27%\nEpoch: 699. Loss: 0.4570. Acc.: 65.09%\nEpoch: 700. Loss: 0.4383. Acc.: 65.22%\nEpoch: 701. Loss: 0.4528. Acc.: 65.88%\nEpoch: 702. Loss: 0.4457. Acc.: 64.44%\nEpoch: 703. Loss: 0.4367. Acc.: 65.09%\nEpoch: 704. Loss: 0.4444. Acc.: 64.57%\nEpoch: 705. Loss: 0.4464. Acc.: 65.35%\nEpoch: 706. Loss: 0.4340. Acc.: 65.88%\nEpoch: 707. Loss: 0.4513. Acc.: 64.57%\nEpoch: 708. Loss: 0.4307. Acc.: 65.09%\nEpoch: 709. Loss: 0.4497. Acc.: 65.88%\nEpoch: 710. Loss: 0.4420. Acc.: 64.70%\nEpoch: 711. Loss: 0.4284. Acc.: 65.09%\nEpoch: 712. Loss: 0.4267. Acc.: 64.44%\nEpoch: 713. Loss: 0.4435. Acc.: 64.44%\nEpoch: 714. Loss: 0.4338. Acc.: 64.96%\nEpoch: 715. Loss: 0.4297. Acc.: 64.70%\nEpoch: 716. Loss: 0.4256. Acc.: 65.22%\nEpoch: 717. Loss: 0.4342. Acc.: 66.01%\nEpoch: 718. Loss: 0.4405. Acc.: 64.83%\nEpoch: 719. Loss: 0.4413. Acc.: 64.96%\nEpoch: 720. Loss: 0.4648. Acc.: 64.83%\nEpoch: 721. Loss: 0.4355. Acc.: 64.57%\nEpoch: 722. Loss: 0.4361. Acc.: 65.75%\nEpoch: 723. Loss: 0.4322. Acc.: 64.83%\nEpoch: 724. Loss: 0.4329. Acc.: 64.17%\nEpoch: 725. Loss: 0.4441. Acc.: 64.04%\nEpoch: 726. Loss: 0.4247. Acc.: 64.96%\nEpoch: 727. Loss: 0.4373. Acc.: 64.57%\nEpoch: 728. Loss: 0.4343. Acc.: 65.62%\nEpoch: 729. Loss: 0.4192. Acc.: 65.35%\nEpoch: 730. Loss: 0.4319. Acc.: 66.14%\nEpoch: 731. Loss: 0.4174. Acc.: 64.83%\nEpoch: 732. Loss: 0.4360. Acc.: 64.44%\nEpoch: 733. Loss: 0.4349. Acc.: 64.30%\nEpoch: 734. Loss: 0.4343. Acc.: 64.30%\nEpoch: 735. Loss: 0.4307. Acc.: 64.30%\nEpoch: 736. Loss: 0.4223. Acc.: 65.62%\nEpoch: 737. Loss: 0.4140. Acc.: 65.88%\nEpoch: 738. Loss: 0.4326. Acc.: 65.22%\nEpoch: 739. Loss: 0.4235. Acc.: 65.49%\nEpoch: 740. Loss: 0.4226. Acc.: 65.49%\nEpoch: 741. Loss: 0.4275. Acc.: 65.09%\nEpoch: 742. Loss: 0.4168. Acc.: 65.35%\nEpoch: 743. Loss: 0.4356. Acc.: 65.35%\nEpoch: 744. Loss: 0.4307. Acc.: 64.44%\nEpoch: 745. Loss: 0.4163. Acc.: 63.65%\nEpoch: 746. Loss: 0.4342. Acc.: 66.01%\nEpoch: 747. Loss: 0.4282. Acc.: 66.14%\nEpoch: 748. Loss: 0.4182. Acc.: 66.01%\nEpoch: 749. Loss: 0.4338. Acc.: 66.80%\nEpoch 749 best model saved with accuracy: 66.80%\nEpoch: 750. Loss: 0.4116. Acc.: 65.49%\nEpoch: 751. Loss: 0.4134. Acc.: 66.67%\nEpoch: 752. Loss: 0.4117. Acc.: 65.75%\nEpoch: 753. Loss: 0.4247. Acc.: 65.35%\nEpoch: 754. Loss: 0.4193. Acc.: 66.93%\nEpoch 754 best model saved with accuracy: 66.93%\nEpoch: 755. Loss: 0.4288. Acc.: 66.54%\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 756. Loss: 0.4355. Acc.: 64.96%\nEpoch: 757. Loss: 0.4290. Acc.: 66.14%\nEpoch: 758. Loss: 0.4124. Acc.: 65.09%\nEpoch: 759. Loss: 0.4290. Acc.: 65.62%\nEpoch: 760. Loss: 0.4261. Acc.: 66.01%\nEpoch: 761. Loss: 0.4168. Acc.: 66.14%\nEpoch: 762. Loss: 0.4229. Acc.: 65.09%\nEpoch: 763. Loss: 0.4116. Acc.: 66.80%\nEpoch: 764. Loss: 0.4195. Acc.: 66.27%\nEpoch: 765. Loss: 0.4202. Acc.: 66.67%\nEpoch: 766. Loss: 0.4142. Acc.: 66.40%\nEpoch: 767. Loss: 0.4195. Acc.: 66.14%\nEpoch: 768. Loss: 0.4201. Acc.: 67.19%\nEpoch 768 best model saved with accuracy: 67.19%\nEpoch: 769. Loss: 0.4140. Acc.: 66.14%\nEpoch: 770. Loss: 0.4205. Acc.: 66.67%\nEpoch: 771. Loss: 0.4083. Acc.: 66.27%\nEpoch: 772. Loss: 0.4237. Acc.: 65.62%\nEpoch: 773. Loss: 0.4115. Acc.: 67.32%\nEpoch 773 best model saved with accuracy: 67.32%\nEpoch: 774. Loss: 0.4186. Acc.: 66.40%\nEpoch: 775. Loss: 0.4169. Acc.: 66.14%\nEpoch: 776. Loss: 0.4092. Acc.: 66.14%\nEpoch: 777. Loss: 0.4126. Acc.: 66.67%\nEpoch: 778. Loss: 0.4121. Acc.: 65.49%\nEpoch: 779. Loss: 0.4062. Acc.: 64.96%\nEpoch: 780. Loss: 0.4091. Acc.: 65.62%\nEpoch: 781. Loss: 0.4125. Acc.: 66.01%\nEpoch: 782. Loss: 0.4068. Acc.: 65.22%\nEpoch: 783. Loss: 0.4150. Acc.: 64.96%\nEpoch: 784. Loss: 0.4133. Acc.: 65.75%\nEpoch: 785. Loss: 0.4109. Acc.: 64.44%\nEpoch: 786. Loss: 0.4209. Acc.: 65.49%\nEpoch: 787. Loss: 0.4056. Acc.: 65.49%\nEpoch: 788. Loss: 0.4203. Acc.: 64.83%\nEpoch: 789. Loss: 0.4118. Acc.: 65.22%\nEpoch: 790. Loss: 0.3955. Acc.: 65.35%\nEpoch: 791. Loss: 0.4172. Acc.: 63.52%\nEpoch: 792. Loss: 0.4111. Acc.: 64.57%\nEpoch: 793. Loss: 0.4203. Acc.: 64.83%\nEpoch: 794. Loss: 0.4132. Acc.: 65.35%\nEpoch: 795. Loss: 0.4109. Acc.: 66.01%\nEpoch: 796. Loss: 0.4043. Acc.: 66.40%\nEpoch: 797. Loss: 0.4029. Acc.: 66.14%\nEpoch: 798. Loss: 0.4118. Acc.: 66.67%\nEpoch: 799. Loss: 0.4026. Acc.: 66.54%\nEpoch: 800. Loss: 0.4059. Acc.: 66.01%\nEpoch: 801. Loss: 0.4059. Acc.: 67.32%\nEpoch: 802. Loss: 0.4008. Acc.: 67.45%\nEpoch 802 best model saved with accuracy: 67.45%\nEpoch: 803. Loss: 0.4116. Acc.: 66.80%\nEpoch: 804. Loss: 0.4088. Acc.: 66.40%\nEpoch: 805. Loss: 0.4106. Acc.: 66.27%\nEpoch: 806. Loss: 0.4004. Acc.: 66.27%\nEpoch: 807. Loss: 0.4141. Acc.: 67.06%\nEpoch: 808. Loss: 0.3989. Acc.: 66.80%\nEpoch: 809. Loss: 0.4047. Acc.: 67.45%\nEpoch: 810. Loss: 0.4042. Acc.: 66.93%\nEpoch: 811. Loss: 0.4112. Acc.: 66.67%\nEpoch: 812. Loss: 0.3953. Acc.: 66.67%\nEpoch: 813. Loss: 0.4130. Acc.: 67.59%\nEpoch 813 best model saved with accuracy: 67.59%\nEpoch: 814. Loss: 0.4182. Acc.: 66.93%\nEpoch: 815. Loss: 0.3894. Acc.: 67.32%\nEpoch: 816. Loss: 0.4040. Acc.: 66.01%\nEpoch: 817. Loss: 0.3951. Acc.: 65.88%\nEpoch: 818. Loss: 0.3944. Acc.: 65.88%\nEpoch: 819. Loss: 0.3920. Acc.: 66.14%\nEpoch: 820. Loss: 0.4113. Acc.: 66.93%\nEpoch: 821. Loss: 0.3870. Acc.: 66.67%\nEpoch: 822. Loss: 0.4081. Acc.: 66.01%\nEpoch: 823. Loss: 0.4036. Acc.: 66.67%\nEpoch: 824. Loss: 0.4051. Acc.: 67.06%\nEpoch: 825. Loss: 0.4005. Acc.: 66.40%\nEpoch: 826. Loss: 0.4070. Acc.: 66.67%\nEpoch: 827. Loss: 0.3962. Acc.: 66.54%\nEpoch: 828. Loss: 0.3904. Acc.: 66.67%\nEpoch: 829. Loss: 0.3943. Acc.: 65.88%\nEpoch: 830. Loss: 0.4114. Acc.: 65.35%\nEpoch: 831. Loss: 0.3956. Acc.: 65.49%\nEpoch: 832. Loss: 0.4027. Acc.: 66.67%\nEpoch: 833. Loss: 0.4068. Acc.: 66.80%\nEpoch: 834. Loss: 0.4005. Acc.: 66.67%\nEpoch: 835. Loss: 0.3971. Acc.: 66.67%\nEpoch: 836. Loss: 0.3875. Acc.: 66.80%\nEpoch: 837. Loss: 0.3848. Acc.: 65.09%\nEpoch: 838. Loss: 0.3977. Acc.: 65.35%\nEpoch: 839. Loss: 0.4003. Acc.: 66.14%\nEpoch: 840. Loss: 0.3903. Acc.: 66.80%\nEpoch: 841. Loss: 0.3818. Acc.: 66.14%\nEpoch: 842. Loss: 0.3952. Acc.: 66.27%\nEpoch: 843. Loss: 0.3800. Acc.: 66.54%\nEpoch: 844. Loss: 0.4077. Acc.: 66.40%\nEpoch: 845. Loss: 0.3893. Acc.: 65.75%\nEpoch: 846. Loss: 0.3951. Acc.: 67.72%\nEpoch 846 best model saved with accuracy: 67.72%\nEpoch: 847. Loss: 0.4051. Acc.: 68.24%\nEpoch 847 best model saved with accuracy: 68.24%\nEpoch: 848. Loss: 0.4007. Acc.: 65.75%\nEpoch: 849. Loss: 0.4044. Acc.: 65.49%\nEpoch: 850. Loss: 0.3908. Acc.: 66.67%\nEpoch: 851. Loss: 0.3934. Acc.: 66.67%\nEpoch: 852. Loss: 0.3914. Acc.: 65.62%\nEpoch: 853. Loss: 0.3823. Acc.: 67.19%\nEpoch: 854. Loss: 0.3963. Acc.: 67.32%\nEpoch: 855. Loss: 0.3894. Acc.: 67.19%\nEpoch: 856. Loss: 0.4054. Acc.: 67.45%\nEpoch: 857. Loss: 0.3961. Acc.: 67.59%\nEpoch: 858. Loss: 0.3872. Acc.: 66.14%\nEpoch: 859. Loss: 0.3749. Acc.: 67.45%\nEpoch: 860. Loss: 0.4025. Acc.: 67.32%\nEpoch: 861. Loss: 0.3991. Acc.: 66.14%\nEpoch: 862. Loss: 0.3962. Acc.: 65.88%\nEpoch: 863. Loss: 0.3901. Acc.: 66.14%\nEpoch: 864. Loss: 0.3898. Acc.: 66.80%\nEpoch: 865. Loss: 0.3944. Acc.: 65.22%\nEpoch: 866. Loss: 0.3881. Acc.: 66.40%\nEpoch: 867. Loss: 0.3832. Acc.: 67.32%\nEpoch: 868. Loss: 0.3807. Acc.: 67.98%\nEpoch: 869. Loss: 0.3898. Acc.: 65.75%\nEpoch: 870. Loss: 0.3921. Acc.: 66.40%\nEpoch: 871. Loss: 0.3822. Acc.: 66.67%\nEpoch: 872. Loss: 0.3807. Acc.: 66.27%\nEpoch: 873. Loss: 0.4005. Acc.: 67.59%\nEpoch: 874. Loss: 0.3976. Acc.: 66.01%\nEpoch: 875. Loss: 0.3782. Acc.: 66.14%\nEpoch: 876. Loss: 0.3893. Acc.: 65.22%\nEpoch: 877. Loss: 0.3980. Acc.: 65.49%\nEpoch: 878. Loss: 0.3884. Acc.: 66.40%\nEpoch: 879. Loss: 0.3774. Acc.: 66.40%\nEpoch: 880. Loss: 0.3957. Acc.: 66.14%\nEpoch: 881. Loss: 0.3945. Acc.: 65.49%\nEpoch: 882. Loss: 0.3920. Acc.: 66.01%\nEpoch: 883. Loss: 0.3789. Acc.: 66.27%\nEpoch: 884. Loss: 0.3861. Acc.: 67.32%\nEpoch: 885. Loss: 0.3798. Acc.: 67.06%\nEpoch: 886. Loss: 0.3879. Acc.: 66.93%\nEpoch: 887. Loss: 0.3862. Acc.: 66.67%\nEpoch: 888. Loss: 0.3779. Acc.: 66.54%\nEpoch: 889. Loss: 0.3874. Acc.: 65.75%\nEpoch: 890. Loss: 0.3901. Acc.: 64.30%\nEpoch: 891. Loss: 0.3810. Acc.: 66.14%\nEpoch: 892. Loss: 0.3895. Acc.: 66.14%\nEpoch: 893. Loss: 0.3852. Acc.: 66.27%\nEpoch: 894. Loss: 0.3819. Acc.: 65.35%\nEpoch: 895. Loss: 0.3818. Acc.: 66.01%\nEpoch: 896. Loss: 0.3821. Acc.: 66.01%\nEpoch: 897. Loss: 0.3858. Acc.: 65.62%\nEpoch: 898. Loss: 0.3807. Acc.: 64.96%\nEpoch: 899. Loss: 0.3787. Acc.: 65.49%\nEpoch: 900. Loss: 0.3768. Acc.: 66.67%\nEpoch: 901. Loss: 0.3842. Acc.: 67.19%\nEpoch: 902. Loss: 0.3790. Acc.: 66.27%\nEpoch: 903. Loss: 0.3632. Acc.: 66.54%\nEpoch: 904. Loss: 0.3803. Acc.: 67.19%\nEpoch: 905. Loss: 0.3883. Acc.: 65.88%\nEpoch: 906. Loss: 0.3785. Acc.: 65.88%\nEpoch: 907. Loss: 0.3812. Acc.: 66.27%\nEpoch: 908. Loss: 0.3904. Acc.: 66.14%\nEpoch: 909. Loss: 0.3866. Acc.: 66.01%\nEpoch: 910. Loss: 0.3880. Acc.: 67.85%\nEpoch: 911. Loss: 0.3746. Acc.: 65.88%\nEpoch: 912. Loss: 0.3754. Acc.: 66.67%\nEpoch: 913. Loss: 0.3665. Acc.: 66.40%\nEpoch: 914. Loss: 0.3723. Acc.: 66.40%\nEpoch: 915. Loss: 0.3866. Acc.: 66.14%\nEpoch: 916. Loss: 0.3910. Acc.: 66.93%\nEpoch: 917. Loss: 0.3739. Acc.: 67.19%\nEpoch: 918. Loss: 0.3679. Acc.: 66.93%\nEpoch: 919. Loss: 0.3799. Acc.: 67.06%\nEpoch: 920. Loss: 0.3786. Acc.: 67.19%\nEpoch: 921. Loss: 0.3825. Acc.: 67.19%\nEpoch: 922. Loss: 0.3682. Acc.: 66.40%\nEpoch: 923. Loss: 0.3634. Acc.: 66.67%\nEpoch: 924. Loss: 0.3700. Acc.: 66.67%\nEpoch: 925. Loss: 0.3797. Acc.: 67.06%\nEpoch: 926. Loss: 0.3681. Acc.: 67.32%\nEpoch: 927. Loss: 0.3711. Acc.: 66.67%\nEpoch: 928. Loss: 0.3682. Acc.: 67.32%\nEpoch: 929. Loss: 0.3810. Acc.: 66.27%\nEpoch: 930. Loss: 0.3715. Acc.: 66.27%\nEpoch: 931. Loss: 0.3780. Acc.: 66.67%\nEpoch: 932. Loss: 0.3611. Acc.: 66.27%\nEpoch: 933. Loss: 0.3775. Acc.: 65.88%\nEpoch: 934. Loss: 0.3648. Acc.: 67.19%\nEpoch: 935. Loss: 0.3760. Acc.: 66.67%\nEpoch: 936. Loss: 0.3806. Acc.: 66.80%\nEpoch: 937. Loss: 0.3953. Acc.: 67.19%\nEpoch: 938. Loss: 0.3697. Acc.: 67.98%\nEpoch: 939. Loss: 0.3676. Acc.: 69.16%\nEpoch 939 best model saved with accuracy: 69.16%\nEpoch: 940. Loss: 0.3731. Acc.: 67.32%\nEpoch: 941. Loss: 0.3607. Acc.: 65.22%\nEpoch: 942. Loss: 0.3594. Acc.: 66.54%\nEpoch: 943. Loss: 0.3630. Acc.: 68.64%\nEpoch: 944. Loss: 0.3664. Acc.: 66.80%\nEpoch: 945. Loss: 0.3728. Acc.: 66.01%\nEpoch: 946. Loss: 0.3715. Acc.: 67.32%\nEpoch: 947. Loss: 0.3492. Acc.: 67.72%\nEpoch: 948. Loss: 0.3653. Acc.: 67.72%\nEpoch: 949. Loss: 0.3657. Acc.: 67.19%\nEpoch: 950. Loss: 0.3594. Acc.: 67.19%\nEpoch: 951. Loss: 0.3660. Acc.: 67.85%\nEpoch: 952. Loss: 0.3591. Acc.: 67.45%\nEpoch: 953. Loss: 0.3748. Acc.: 66.80%\nEpoch: 954. Loss: 0.3724. Acc.: 67.85%\nEpoch: 955. Loss: 0.3689. Acc.: 68.24%\nEpoch: 956. Loss: 0.3803. Acc.: 67.32%\nEpoch: 957. Loss: 0.3652. Acc.: 66.54%\nEpoch: 958. Loss: 0.3794. Acc.: 67.19%\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 959. Loss: 0.3731. Acc.: 66.80%\nEpoch: 960. Loss: 0.3664. Acc.: 66.80%\nEpoch: 961. Loss: 0.3449. Acc.: 66.14%\nEpoch: 962. Loss: 0.3675. Acc.: 66.93%\nEpoch: 963. Loss: 0.3588. Acc.: 67.06%\nEpoch: 964. Loss: 0.3678. Acc.: 66.54%\nEpoch: 965. Loss: 0.3544. Acc.: 66.93%\nEpoch: 966. Loss: 0.3639. Acc.: 66.54%\nEpoch: 967. Loss: 0.3509. Acc.: 66.67%\nEpoch: 968. Loss: 0.3705. Acc.: 65.75%\nEpoch: 969. Loss: 0.3772. Acc.: 65.88%\nEpoch: 970. Loss: 0.3526. Acc.: 66.27%\nEpoch: 971. Loss: 0.3840. Acc.: 66.93%\nEpoch: 972. Loss: 0.3545. Acc.: 67.19%\nEpoch: 973. Loss: 0.3716. Acc.: 67.45%\nEpoch: 974. Loss: 0.3620. Acc.: 67.32%\nEpoch: 975. Loss: 0.3634. Acc.: 66.40%\nEpoch: 976. Loss: 0.3626. Acc.: 66.67%\nEpoch: 977. Loss: 0.3714. Acc.: 67.98%\nEpoch: 978. Loss: 0.3564. Acc.: 66.67%\nEpoch: 979. Loss: 0.3646. Acc.: 66.01%\nEpoch: 980. Loss: 0.3694. Acc.: 66.80%\nEpoch: 981. Loss: 0.3519. Acc.: 67.59%\nEpoch: 982. Loss: 0.3559. Acc.: 67.59%\nEpoch: 983. Loss: 0.3651. Acc.: 65.62%\nEpoch: 984. Loss: 0.3649. Acc.: 66.80%\nEpoch: 985. Loss: 0.3735. Acc.: 67.72%\nEpoch: 986. Loss: 0.3534. Acc.: 67.45%\nEpoch: 987. Loss: 0.3699. Acc.: 67.45%\nEpoch: 988. Loss: 0.3704. Acc.: 67.19%\nEpoch: 989. Loss: 0.3550. Acc.: 66.54%\nEpoch: 990. Loss: 0.3651. Acc.: 67.06%\nEpoch: 991. Loss: 0.3543. Acc.: 67.19%\nEpoch: 992. Loss: 0.3599. Acc.: 66.40%\nEpoch: 993. Loss: 0.3630. Acc.: 67.19%\nEpoch: 994. Loss: 0.3601. Acc.: 67.45%\nEpoch: 995. Loss: 0.3550. Acc.: 67.72%\nEpoch: 996. Loss: 0.3523. Acc.: 66.40%\nEpoch: 997. Loss: 0.3599. Acc.: 65.75%\nEpoch: 998. Loss: 0.3532. Acc.: 67.59%\nEpoch: 999. Loss: 0.3687. Acc.: 69.55%\nEpoch 999 best model saved with accuracy: 69.55%\nEpoch: 1000. Loss: 0.3640. Acc.: 68.64%\nEpoch: 1001. Loss: 0.3700. Acc.: 67.45%\nEpoch: 1002. Loss: 0.3633. Acc.: 67.98%\nEpoch: 1003. Loss: 0.3562. Acc.: 67.45%\nEpoch: 1004. Loss: 0.3518. Acc.: 67.06%\nEpoch: 1005. Loss: 0.3469. Acc.: 67.85%\nEpoch: 1006. Loss: 0.3465. Acc.: 67.59%\nEpoch: 1007. Loss: 0.3505. Acc.: 66.80%\nEpoch: 1008. Loss: 0.3476. Acc.: 66.93%\nEpoch: 1009. Loss: 0.3562. Acc.: 68.11%\nEpoch: 1010. Loss: 0.3651. Acc.: 67.98%\nEpoch: 1011. Loss: 0.3566. Acc.: 67.06%\nEpoch: 1012. Loss: 0.3600. Acc.: 68.11%\nEpoch: 1013. Loss: 0.3567. Acc.: 68.50%\nEpoch: 1014. Loss: 0.3490. Acc.: 68.37%\nEpoch: 1015. Loss: 0.3668. Acc.: 67.72%\nEpoch: 1016. Loss: 0.3663. Acc.: 67.85%\nEpoch: 1017. Loss: 0.3451. Acc.: 66.01%\nEpoch: 1018. Loss: 0.3660. Acc.: 66.54%\nEpoch: 1019. Loss: 0.3501. Acc.: 66.54%\nEpoch: 1020. Loss: 0.3411. Acc.: 66.93%\nEpoch: 1021. Loss: 0.3525. Acc.: 66.14%\nEpoch: 1022. Loss: 0.3463. Acc.: 69.16%\nEpoch: 1023. Loss: 0.3510. Acc.: 67.85%\nEpoch: 1024. Loss: 0.3505. Acc.: 66.93%\nEpoch: 1024. Loss: 0.3505. Acc.: 66.93%\nEpoch: 1025. Loss: 0.3435. Acc.: 67.85%\nEpoch: 1026. Loss: 0.3532. Acc.: 68.90%\nEpoch: 1027. Loss: 0.3507. Acc.: 69.16%\nEpoch: 1028. Loss: 0.3522. Acc.: 68.11%\nEpoch: 1029. Loss: 0.3412. Acc.: 68.37%\nEpoch: 1030. Loss: 0.3424. Acc.: 68.90%\nEpoch: 1031. Loss: 0.3470. Acc.: 68.50%\nEpoch: 1032. Loss: 0.3446. Acc.: 67.98%\nEpoch: 1033. Loss: 0.3678. Acc.: 67.85%\nEpoch: 1034. Loss: 0.3534. Acc.: 66.67%\nEpoch: 1035. Loss: 0.3516. Acc.: 67.98%\nEpoch: 1036. Loss: 0.3610. Acc.: 68.77%\nEpoch: 1037. Loss: 0.3679. Acc.: 67.45%\nEpoch: 1038. Loss: 0.3498. Acc.: 67.32%\nEpoch: 1039. Loss: 0.3438. Acc.: 67.98%\nEpoch: 1040. Loss: 0.3714. Acc.: 67.06%\nEpoch: 1041. Loss: 0.3657. Acc.: 67.32%\nEpoch: 1042. Loss: 0.3561. Acc.: 66.93%\nEpoch: 1043. Loss: 0.3415. Acc.: 68.11%\nEpoch: 1044. Loss: 0.3499. Acc.: 68.37%\nEpoch: 1045. Loss: 0.3456. Acc.: 68.24%\nEpoch: 1046. Loss: 0.3565. Acc.: 67.19%\nEpoch: 1047. Loss: 0.3558. Acc.: 66.80%\nEpoch: 1048. Loss: 0.3423. Acc.: 67.98%\nEpoch: 1049. Loss: 0.3527. Acc.: 68.90%\nEpoch: 1050. Loss: 0.3282. Acc.: 68.50%\nEpoch: 1051. Loss: 0.3510. Acc.: 69.29%\nEpoch: 1052. Loss: 0.3461. Acc.: 67.98%\nEpoch: 1053. Loss: 0.3526. Acc.: 67.59%\nEpoch: 1054. Loss: 0.3584. Acc.: 68.64%\nEpoch: 1055. Loss: 0.3604. Acc.: 69.69%\nEpoch 1055 best model saved with accuracy: 69.69%\nEpoch: 1056. Loss: 0.3484. Acc.: 69.16%\nEpoch: 1057. Loss: 0.3516. Acc.: 68.37%\nEpoch: 1058. Loss: 0.3552. Acc.: 69.03%\nEpoch: 1059. Loss: 0.3430. Acc.: 68.37%\nEpoch: 1060. Loss: 0.3419. Acc.: 69.03%\nEpoch: 1061. Loss: 0.3583. Acc.: 69.42%\nEpoch: 1062. Loss: 0.3500. Acc.: 68.90%\nEpoch: 1063. Loss: 0.3446. Acc.: 68.24%\nEpoch: 1064. Loss: 0.3452. Acc.: 67.85%\nEpoch: 1065. Loss: 0.3432. Acc.: 67.59%\nEpoch: 1066. Loss: 0.3360. Acc.: 68.77%\nEpoch: 1067. Loss: 0.3532. Acc.: 68.37%\nEpoch: 1068. Loss: 0.3333. Acc.: 68.50%\nEpoch: 1069. Loss: 0.3395. Acc.: 68.37%\nEpoch: 1070. Loss: 0.3424. Acc.: 67.98%\nEpoch: 1071. Loss: 0.3405. Acc.: 66.67%\nEpoch: 1072. Loss: 0.3314. Acc.: 68.64%\nEpoch: 1073. Loss: 0.3381. Acc.: 68.64%\nEpoch: 1074. Loss: 0.3489. Acc.: 68.24%\nEpoch: 1075. Loss: 0.3365. Acc.: 68.50%\nEpoch: 1076. Loss: 0.3365. Acc.: 68.64%\nEpoch: 1077. Loss: 0.3460. Acc.: 67.98%\nEpoch: 1078. Loss: 0.3451. Acc.: 68.77%\nEpoch: 1079. Loss: 0.3638. Acc.: 69.16%\nEpoch: 1080. Loss: 0.3493. Acc.: 68.50%\nEpoch: 1081. Loss: 0.3402. Acc.: 68.64%\nEpoch: 1082. Loss: 0.3336. Acc.: 66.54%\nEpoch: 1083. Loss: 0.3467. Acc.: 68.11%\nEpoch: 1084. Loss: 0.3475. Acc.: 68.24%\nEpoch: 1085. Loss: 0.3369. Acc.: 67.32%\nEpoch: 1086. Loss: 0.3400. Acc.: 66.67%\nEpoch: 1087. Loss: 0.3416. Acc.: 67.19%\nEpoch: 1088. Loss: 0.3440. Acc.: 67.85%\nEpoch: 1089. Loss: 0.3248. Acc.: 68.37%\nEpoch: 1090. Loss: 0.3496. Acc.: 69.42%\nEpoch: 1091. Loss: 0.3414. Acc.: 68.11%\nEpoch: 1092. Loss: 0.3374. Acc.: 68.50%\nEpoch: 1093. Loss: 0.3292. Acc.: 69.03%\nEpoch: 1094. Loss: 0.3289. Acc.: 68.24%\nEpoch: 1095. Loss: 0.3338. Acc.: 68.11%\nEpoch: 1096. Loss: 0.3302. Acc.: 69.42%\nEpoch: 1097. Loss: 0.3349. Acc.: 68.77%\nEpoch: 1098. Loss: 0.3357. Acc.: 67.59%\nEpoch: 1099. Loss: 0.3335. Acc.: 69.82%\nEpoch 1099 best model saved with accuracy: 69.82%\nEpoch: 1100. Loss: 0.3521. Acc.: 68.64%\nEpoch: 1101. Loss: 0.3429. Acc.: 68.24%\nEpoch: 1102. Loss: 0.3425. Acc.: 68.77%\nEpoch: 1103. Loss: 0.3365. Acc.: 68.90%\nEpoch: 1104. Loss: 0.3422. Acc.: 69.95%\nEpoch 1104 best model saved with accuracy: 69.95%\nEpoch: 1105. Loss: 0.3390. Acc.: 69.69%\nEpoch: 1106. Loss: 0.3170. Acc.: 68.24%\nEpoch: 1107. Loss: 0.3253. Acc.: 69.03%\nEpoch: 1108. Loss: 0.3259. Acc.: 69.29%\nEpoch: 1109. Loss: 0.3302. Acc.: 69.95%\nEpoch: 1110. Loss: 0.3494. Acc.: 67.72%\nEpoch: 1111. Loss: 0.3378. Acc.: 66.40%\nEpoch: 1112. Loss: 0.3335. Acc.: 67.32%\nEpoch: 1113. Loss: 0.3409. Acc.: 69.42%\nEpoch: 1114. Loss: 0.3364. Acc.: 68.77%\nEpoch: 1115. Loss: 0.3488. Acc.: 69.29%\nEpoch: 1116. Loss: 0.3509. Acc.: 67.59%\nEpoch: 1117. Loss: 0.3345. Acc.: 68.64%\nEpoch: 1118. Loss: 0.3270. Acc.: 69.42%\nEpoch: 1119. Loss: 0.3277. Acc.: 69.03%\nEpoch: 1120. Loss: 0.3347. Acc.: 67.98%\nEpoch: 1121. Loss: 0.3374. Acc.: 68.24%\nEpoch: 1122. Loss: 0.3426. Acc.: 68.64%\nEpoch: 1123. Loss: 0.3328. Acc.: 68.24%\nEpoch: 1124. Loss: 0.3435. Acc.: 69.42%\nEpoch: 1125. Loss: 0.3415. Acc.: 67.85%\nEpoch: 1126. Loss: 0.3273. Acc.: 68.24%\nEpoch: 1127. Loss: 0.3384. Acc.: 68.64%\nEpoch: 1128. Loss: 0.3320. Acc.: 68.37%\nEpoch: 1129. Loss: 0.3387. Acc.: 69.03%\nEpoch: 1130. Loss: 0.3221. Acc.: 69.69%\nEpoch: 1131. Loss: 0.3531. Acc.: 67.85%\nEpoch: 1132. Loss: 0.3313. Acc.: 67.19%\nEpoch: 1133. Loss: 0.3270. Acc.: 68.37%\nEpoch: 1134. Loss: 0.3298. Acc.: 67.72%\nEpoch: 1135. Loss: 0.3287. Acc.: 67.85%\nEpoch: 1136. Loss: 0.3292. Acc.: 68.90%\nEpoch: 1137. Loss: 0.3536. Acc.: 69.69%\nEpoch: 1138. Loss: 0.3370. Acc.: 68.77%\nEpoch: 1139. Loss: 0.3346. Acc.: 68.64%\nEpoch: 1140. Loss: 0.3283. Acc.: 68.50%\nEpoch: 1141. Loss: 0.3379. Acc.: 69.42%\nEpoch: 1142. Loss: 0.3204. Acc.: 69.55%\nEpoch: 1143. Loss: 0.3271. Acc.: 69.16%\nEpoch: 1144. Loss: 0.3262. Acc.: 69.82%\nEpoch: 1145. Loss: 0.3228. Acc.: 69.55%\nEpoch: 1146. Loss: 0.3250. Acc.: 68.90%\nEpoch: 1147. Loss: 0.3246. Acc.: 68.64%\nEpoch: 1148. Loss: 0.3216. Acc.: 70.08%\nEpoch 1148 best model saved with accuracy: 70.08%\nEpoch: 1149. Loss: 0.3267. Acc.: 69.69%\nEpoch: 1150. Loss: 0.3234. Acc.: 69.55%\nEpoch: 1151. Loss: 0.3388. Acc.: 69.29%\nEpoch: 1152. Loss: 0.3362. Acc.: 67.59%\nEpoch: 1153. Loss: 0.3290. Acc.: 67.72%\nEpoch: 1154. Loss: 0.3365. Acc.: 68.37%\nEpoch: 1155. Loss: 0.3443. Acc.: 67.98%\nEpoch: 1156. Loss: 0.3309. Acc.: 68.77%\nEpoch: 1157. Loss: 0.3304. Acc.: 69.69%\nEpoch: 1158. Loss: 0.3285. Acc.: 69.55%\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 1159. Loss: 0.3308. Acc.: 68.64%\nEpoch: 1160. Loss: 0.3434. Acc.: 68.77%\nEpoch: 1161. Loss: 0.3329. Acc.: 68.64%\nEpoch: 1162. Loss: 0.3239. Acc.: 69.55%\nEpoch: 1163. Loss: 0.3255. Acc.: 68.37%\nEpoch: 1164. Loss: 0.3230. Acc.: 69.55%\nEpoch: 1165. Loss: 0.3371. Acc.: 70.08%\nEpoch: 1166. Loss: 0.3142. Acc.: 67.98%\nEpoch: 1167. Loss: 0.3286. Acc.: 68.37%\nEpoch: 1168. Loss: 0.3172. Acc.: 68.24%\nEpoch: 1169. Loss: 0.3217. Acc.: 67.85%\nEpoch: 1170. Loss: 0.3211. Acc.: 67.72%\nEpoch: 1171. Loss: 0.3463. Acc.: 69.55%\nEpoch: 1172. Loss: 0.3277. Acc.: 69.42%\nEpoch: 1173. Loss: 0.3355. Acc.: 68.90%\nEpoch: 1174. Loss: 0.3299. Acc.: 69.42%\nEpoch: 1175. Loss: 0.3187. Acc.: 68.90%\nEpoch: 1176. Loss: 0.3269. Acc.: 69.03%\nEpoch: 1177. Loss: 0.3311. Acc.: 69.69%\nEpoch: 1178. Loss: 0.3148. Acc.: 68.77%\nEpoch: 1179. Loss: 0.3266. Acc.: 69.55%\nEpoch: 1180. Loss: 0.3279. Acc.: 69.42%\nEpoch: 1181. Loss: 0.3175. Acc.: 68.64%\nEpoch: 1182. Loss: 0.3345. Acc.: 69.03%\nEpoch: 1183. Loss: 0.3197. Acc.: 69.42%\nEpoch: 1184. Loss: 0.3232. Acc.: 69.29%\nEpoch: 1185. Loss: 0.3315. Acc.: 71.00%\nEpoch 1185 best model saved with accuracy: 71.00%\nEpoch: 1186. Loss: 0.3356. Acc.: 70.34%\nEpoch: 1187. Loss: 0.3157. Acc.: 68.24%\nEpoch: 1188. Loss: 0.3240. Acc.: 70.08%\nEpoch: 1189. Loss: 0.3398. Acc.: 70.08%\nEpoch: 1190. Loss: 0.3201. Acc.: 69.55%\nEpoch: 1191. Loss: 0.3321. Acc.: 69.95%\nEpoch: 1192. Loss: 0.3206. Acc.: 69.95%\nEpoch: 1193. Loss: 0.3163. Acc.: 69.55%\nEpoch: 1194. Loss: 0.3236. Acc.: 70.34%\nEpoch: 1195. Loss: 0.3255. Acc.: 70.21%\nEpoch: 1196. Loss: 0.3292. Acc.: 69.16%\nEpoch: 1197. Loss: 0.3220. Acc.: 69.29%\nEpoch: 1198. Loss: 0.3202. Acc.: 69.16%\nEpoch: 1199. Loss: 0.3127. Acc.: 68.90%\nEpoch: 1200. Loss: 0.3213. Acc.: 69.55%\nEpoch: 1201. Loss: 0.3180. Acc.: 69.82%\nEpoch: 1202. Loss: 0.3028. Acc.: 69.03%\nEpoch: 1203. Loss: 0.3138. Acc.: 69.03%\nEpoch: 1204. Loss: 0.3202. Acc.: 69.69%\nEpoch: 1205. Loss: 0.3205. Acc.: 69.03%\nEpoch: 1206. Loss: 0.3159. Acc.: 69.55%\nEpoch: 1207. Loss: 0.3230. Acc.: 68.24%\nEpoch: 1208. Loss: 0.3311. Acc.: 67.72%\nEpoch: 1209. Loss: 0.3270. Acc.: 69.42%\nEpoch: 1210. Loss: 0.3111. Acc.: 68.50%\nEpoch: 1211. Loss: 0.3210. Acc.: 69.16%\nEpoch: 1212. Loss: 0.3247. Acc.: 67.98%\nEpoch: 1213. Loss: 0.3073. Acc.: 67.72%\nEpoch: 1214. Loss: 0.3155. Acc.: 68.50%\nEpoch: 1215. Loss: 0.3263. Acc.: 69.42%\nEpoch: 1216. Loss: 0.3214. Acc.: 68.77%\nEpoch: 1217. Loss: 0.3288. Acc.: 68.37%\nEpoch: 1218. Loss: 0.3287. Acc.: 67.59%\nEpoch: 1219. Loss: 0.3019. Acc.: 69.03%\nEpoch: 1220. Loss: 0.3088. Acc.: 69.95%\nEpoch: 1221. Loss: 0.3143. Acc.: 70.47%\nEpoch: 1222. Loss: 0.3309. Acc.: 69.42%\nEpoch: 1223. Loss: 0.3259. Acc.: 70.47%\nEpoch: 1224. Loss: 0.3098. Acc.: 70.47%\nEpoch: 1225. Loss: 0.3095. Acc.: 69.42%\nEpoch: 1226. Loss: 0.3159. Acc.: 68.64%\nEpoch: 1227. Loss: 0.3192. Acc.: 68.64%\nEpoch: 1228. Loss: 0.3184. Acc.: 69.95%\nEpoch: 1229. Loss: 0.3173. Acc.: 70.87%\nEpoch: 1230. Loss: 0.3074. Acc.: 70.08%\nEpoch: 1231. Loss: 0.3082. Acc.: 68.64%\nEpoch: 1232. Loss: 0.3357. Acc.: 68.50%\nEpoch: 1233. Loss: 0.3210. Acc.: 68.37%\nEpoch: 1234. Loss: 0.3139. Acc.: 68.77%\nEpoch: 1235. Loss: 0.3204. Acc.: 68.24%\nEpoch: 1236. Loss: 0.3115. Acc.: 68.50%\nEpoch: 1237. Loss: 0.3183. Acc.: 69.03%\nEpoch: 1238. Loss: 0.3158. Acc.: 69.16%\nEpoch: 1239. Loss: 0.3237. Acc.: 70.47%\nEpoch: 1240. Loss: 0.3147. Acc.: 70.60%\nEpoch: 1241. Loss: 0.3090. Acc.: 70.47%\nEpoch: 1242. Loss: 0.3098. Acc.: 68.77%\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"5\">**Compute result on testing data**</font><br>"},{"metadata":{"_uuid":"d0f483d5-5542-4d65-a70f-1aa7b3e13c3c","_cell_guid":"05b2b8d9-2b63-436e-a906-a839fb008c0e","trusted":true},"cell_type":"code","source":"test_results = []\nmodel.load_state_dict(torch.load('best.pth'))\nmodel.eval()\n\n# for batch in val_dl:\n#     x_raw, y_batch = [t.to(device) for t in batch]\n#     out = model(x_raw)\n#     preds = F.log_softmax(out, dim=1).argmax(dim=1)\n#     total += y_batch.size(0)\n#     correct += (preds == y_batch).sum().item()\n#     acc = correct / total\n#     acc_history.append(acc)\n\n# for batch in tst_dl:\n#     x, y_batche = [t.to(device) for t in (batch)]\n#     out = model(x)\n#     y_hat = F.log_softmax(out, dim=1).argmax(dim=1)\n#     test_results.extend(y_hat.tolist())\n    \nfor batch in tst_dl:\n        x_raw, y_batch = [t.to(device) for t in batch]\n        out = model(x_raw)\n        preds = F.log_softmax(out, dim=1).argmax(dim=1)\n        total += y_batch.size(0)\n        correct += (preds == y_batch).sum().item()\n    \nacc = correct / total\nprint(\"accurancy on test data: \"+str(acc))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"5\">**Store final result**</font><br>"},{"metadata":{"_uuid":"7a49a38e-e203-4884-ba78-33fe8aa02426","_cell_guid":"f4bd2628-c801-42d5-9af6-11bbd89f2352","trusted":true},"cell_type":"code","source":"submit = pd.read_csv(sample)\nenc = joblib.load(ROOT/'encoder.model')\nsubmit['surface'] = enc.inverse_transform(test_results)\nsubmit.to_csv('submit_base.csv', index=None)\nprint(\"store result successfully!\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}