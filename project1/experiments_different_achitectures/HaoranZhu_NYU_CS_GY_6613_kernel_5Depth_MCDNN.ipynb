{"cells":[{"metadata":{},"cell_type":"markdown","source":"<font size=\"5\">**Project Summary**</font><br>\nIn this competition, the main task is to do surface time series classification. 1d convolution is a good technique when doing the time series classification job. So I apply 1d convolutional neural network to solve this problem. The whole code is written in Pytorch."},{"metadata":{},"cell_type":"markdown","source":"<font size=\"5\">**External libs we are using**</font><br>"},{"metadata":{"_uuid":"a6ed998e-ee9a-4333-8332-8fa753b7f0d4","_cell_guid":"e6e0bc2b-9cba-4688-9e0b-15f11d219c2d","trusted":true},"cell_type":"code","source":"# libs we are using\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport torch\nfrom torch import nn\nfrom torch import optim\nfrom torch.nn import functional as F\nfrom torch.utils.data import TensorDataset, DataLoader","execution_count":939,"outputs":[]},{"metadata":{"_uuid":"b8906e48-2384-4bb3-bba7-722160428e39","_cell_guid":"295d38d1-6663-4575-93ba-c6cfac9f6129","trusted":true},"cell_type":"markdown","source":"<font size=\"5\">**Data preparations**</font><br>\n\nIn this project, I use the raw data as input of the network. I concatenated all datasets into one single numpy array. The first 3810 rows are training data with given labels, the rest 3816 rows are testing data where labels are not given. In total raw data has 7816 rows. In addition to that, we split the raw training data into training data and validation data by the ratio of 80/20.\n\n\nI create three datasets and data loaders for them to make the data ready for model's training. The process is straightforward. I split the labelled data into two subsets, and keep testing data as is. Also, I convert Numpy arrays into `torch.tensor` objects of proper type (float for samples, and long - for targets).\n"},{"metadata":{"_uuid":"94c41c8d-0c91-4aa1-83a4-ff3f518f35f4","_cell_guid":"88b9ec00-fc64-4040-896b-895d06cd58f2","trusted":true},"cell_type":"code","source":"def create_datasets(data, target, train_size, valid_pct=0.2, seed=None):\n    \"\"\"Converts NumPy arrays into PyTorch datsets.\n    \n    Three datasets are created in total:\n        * training dataset\n        * validation dataset\n        * testing (un-labelled) dataset\n\n    \"\"\"\n    raw =  data\n    sz = train_size\n    idx = np.arange(sz)\n    trn_idx, val_idx = train_test_split(\n        idx, test_size=valid_pct, random_state=seed)\n     \n    trn_ds = TensorDataset(\n        torch.tensor(raw[:sz][trn_idx]).float(), \n        torch.tensor(target[:sz][trn_idx]).long())\n    val_ds = TensorDataset(\n        torch.tensor(raw[:sz][val_idx]).float(), \n        torch.tensor(target[:sz][val_idx]).long())\n    tst_ds = TensorDataset(\n        torch.tensor(raw[sz:]).float(), \n        torch.tensor(target[sz:]).long())\n    return trn_ds, val_ds, tst_ds\n\n# We use this function to divide the data\ndef create_datasets2(data_arr, train_size, valid_pct=0.2, seed=None):\n    \"\"\"Converts NumPy arrays into PyTorch datsets.\n    \n    Three datasets are created in total:\n        * training dataset\n        * validation dataset\n        * testing (un-labelled) dataset\n\n    \"\"\"\n    raw =  data_arr\n    sz = train_size\n\n    idx = np.arange(sz)\n    print(\"total data length\")\n    print(len(idx))\n    trn_idx, tst_idx = train_test_split(idx, test_size=0.2, random_state=1)\n    trn_idx, val_idx = train_test_split(trn_idx, test_size=0.25, random_state=1)\n    \n    print(\"input one dimension shape\")\n    print(raw[0].shape)\n    print(\"training data length\")\n    print(len(trn_idx))\n    print(\"validation data le\")\n    print(len(val_idx))\n    print(\"testing d\")\n    print(len(tst_idx))\n    \n    trn_ds = TensorDataset(\n        torch.tensor(raw[0][:sz][trn_idx]).float(),\n        torch.tensor(raw[1][:sz][trn_idx]).float(),\n        torch.tensor(raw[2][:sz][trn_idx]).float(),\n        torch.tensor(raw[3][:sz][trn_idx]).float(),\n        torch.tensor(raw[4][:sz][trn_idx]).float(),\n        torch.tensor(raw[5][:sz][trn_idx]).float(),\n        torch.tensor(raw[6][:sz][trn_idx]).float(),\n        torch.tensor(raw[7][:sz][trn_idx]).float(),\n        torch.tensor(raw[8][:sz][trn_idx]).float(),\n        torch.tensor(target[:sz][trn_idx]).long())\n    \n    val_ds = TensorDataset(\n        torch.tensor(raw[0][:sz][val_idx]).float(),\n        torch.tensor(raw[1][:sz][val_idx]).float(), \n        torch.tensor(raw[2][:sz][val_idx]).float(), \n        torch.tensor(raw[3][:sz][val_idx]).float(), \n        torch.tensor(raw[4][:sz][val_idx]).float(), \n        torch.tensor(raw[5][:sz][val_idx]).float(), \n        torch.tensor(raw[6][:sz][val_idx]).float(), \n        torch.tensor(raw[7][:sz][val_idx]).float(), \n        torch.tensor(raw[8][:sz][val_idx]).float(), \n        torch.tensor(target[:sz][val_idx]).long())\n    \n    tst_ds = TensorDataset(\n        torch.tensor(raw[0][:sz][tst_idx]).float(),\n        torch.tensor(raw[1][:sz][tst_idx]).float(),\n        torch.tensor(raw[2][:sz][tst_idx]).float(),\n        torch.tensor(raw[3][:sz][tst_idx]).float(),\n        torch.tensor(raw[4][:sz][tst_idx]).float(),\n        torch.tensor(raw[5][:sz][tst_idx]).float(),\n        torch.tensor(raw[6][:sz][tst_idx]).float(),\n        torch.tensor(raw[7][:sz][tst_idx]).float(),\n        torch.tensor(raw[8][:sz][tst_idx]).float(),\n        torch.tensor(target[:sz][tst_idx]).long())\n    \n    return trn_ds, val_ds, tst_ds\n\ndef create_loaders(data, bs=128, jobs=0):\n    \"\"\"Wraps the datasets returned by create_datasets function with data loaders.\"\"\"\n    \n    trn_ds, val_ds, tst_ds = data\n    trn_dl = DataLoader(trn_ds, batch_size=bs, shuffle=True, num_workers=jobs)\n    val_dl = DataLoader(val_ds, batch_size=bs, shuffle=False, num_workers=jobs)\n    tst_dl = DataLoader(tst_ds, batch_size=bs, shuffle=False, num_workers=jobs)\n    return trn_dl, val_dl, tst_dl","execution_count":940,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"5\">**Neural Network Architecture**</font><br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Flatten(nn.Module):\n    \"\"\"Converts N-dimensional tensor into 'flat' one.\"\"\"\n\n    def __init__(self, keep_batch_dim=True):\n        super().__init__()\n        self.keep_batch_dim = keep_batch_dim\n\n    def forward(self, x):\n        if self.keep_batch_dim:\n            return x.view(x.size(0), -1)\n        return x.view(-1)","execution_count":941,"outputs":[]},{"metadata":{"_uuid":"f144e459-53bb-4b5a-860d-abec117fc2cc","_cell_guid":"07870588-c88f-42ff-b782-5ae3c5e6e568","trusted":true},"cell_type":"code","source":"class Surface_Classifier(nn.Module):\n    def __init__(self, raw_ni, no, drop=.5):\n        super().__init__()\n        \n        self.conv1d_channel_0 = nn.Sequential(\n            nn.Conv1d(1, 6, 16, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            nn.MaxPool1d(kernel_size=3),\n            \n            nn.BatchNorm1d(6),\n            \n            nn.Conv1d(6,10, 8, 2, 1),\n            nn.ReLU(),\n            nn.Dropout(),\n            nn.MaxPool1d(kernel_size=2),\n            \n            nn.BatchNorm1d(10),\n            \n            nn.Conv1d(10, 16, 4, 2, 1),\n            nn.ReLU(), \n            nn.Dropout(),\n            \n            \n            nn.Conv1d(16, 32, 3, 2, 1),\n            nn.ReLU(), \n            nn.Dropout(),\n            \n            nn.Conv1d(32, 64, 3, 2, 1),\n            nn.ReLU(), \n            nn.Dropout(),\n        )\n            \n            \n            \n           \n        \n        self.conv1d_channel_1 = nn.Sequential(\n            nn.Conv1d(1, 6, 16, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=3),\n            \n            nn.BatchNorm1d(6),\n            \n            nn.Conv1d(6,10, 8, 2, 1),\n            nn.ReLU(),\n            nn.Dropout(),\n            nn.MaxPool1d(kernel_size=2),\n            \n            nn.BatchNorm1d(10),\n            \n            nn.Conv1d(10, 16, 4, 2, 1),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            \n            nn.Conv1d(16, 32, 3, 2, 1),\n            nn.ReLU(), \n            nn.Dropout(),\n            \n            nn.Conv1d(32, 64, 3, 2, 1),\n            nn.ReLU(), \n            nn.Dropout(),\n            \n            \n            \n        )\n        \n        self.conv1d_channel_2 = nn.Sequential(\n            nn.Conv1d(1, 6, 16, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=3),\n            \n            nn.BatchNorm1d(6),\n            \n            nn.Conv1d(6,10, 8, 2, 1),\n            nn.ReLU(),\n            nn.Dropout(),\n            nn.MaxPool1d(kernel_size=2),\n            \n            nn.BatchNorm1d(10),\n            \n            nn.Conv1d(10, 16, 4, 2, 1),\n            nn.ReLU(),\n            nn.Dropout(),\n        \n            nn.Conv1d(16, 32, 3, 2, 1),\n            nn.ReLU(), \n            nn.Dropout(),\n            \n            nn.Conv1d(32, 64, 3, 2, 1),\n            nn.ReLU(), \n            nn.Dropout(),\n            \n         \n           \n        )\n        \n        self.conv1d_channel_3 = nn.Sequential(\n            nn.Conv1d(1, 6, 16, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            nn.MaxPool1d(kernel_size=3),\n            nn.BatchNorm1d(6),\n            \n            nn.Conv1d(6,10, 8, 2, 1),\n            nn.ReLU(),\n            nn.Dropout(),\n            nn.MaxPool1d(kernel_size=2),\n            nn.BatchNorm1d(10),\n            \n            nn.Conv1d(10, 16, 4, 2, 1),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.Conv1d(16, 32, 3, 2, 1),\n            nn.ReLU(), \n            nn.Dropout(),\n            \n            nn.Conv1d(32, 64, 3, 2, 1),\n            nn.ReLU(), \n            nn.Dropout(),\n           \n               \n        )\n        \n        self.conv1d_channel_4 = nn.Sequential(\n            nn.Conv1d(1, 6, 16, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            nn.MaxPool1d(kernel_size=3),\n            nn.BatchNorm1d(6),\n            \n            nn.Conv1d(6,10, 8, 2, 1),\n            nn.ReLU(),\n            nn.Dropout(),\n            nn.MaxPool1d(kernel_size=2),\n            nn.BatchNorm1d(10),\n            \n            nn.Conv1d(10, 16, 4, 2, 1),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.Conv1d(16, 32, 3, 2, 1),\n            nn.ReLU(), \n            nn.Dropout(),\n            \n            nn.Conv1d(32, 64, 3, 2, 1),\n            nn.ReLU(), \n            nn.Dropout(),\n            \n            \n            \n           \n            \n           \n        )\n        \n        self.conv1d_channel_5 = nn.Sequential(\n            nn.Conv1d(1, 6, 16, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            nn.MaxPool1d(kernel_size=3),\n            nn.BatchNorm1d(6),\n        \n            nn.Conv1d(6,10, 8, 2, 1),\n            nn.ReLU(),\n            nn.Dropout(),\n            nn.MaxPool1d(kernel_size=2),\n            nn.BatchNorm1d(10),\n            \n            nn.Conv1d(10, 16, 4, 2, 1),\n            nn.ReLU(),\n            nn.Dropout(),\n           \n            nn.Conv1d(16, 32, 3, 2, 1),\n            nn.ReLU(), \n            nn.Dropout(),\n            \n            nn.Conv1d(32, 64, 3, 2, 1),\n            nn.ReLU(), \n            nn.Dropout(),\n         \n            \n        )\n        \n        self.conv1d_channel_6 = nn.Sequential(\n            nn.Conv1d(1, 6, 16, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            nn.MaxPool1d(kernel_size=3),\n            nn.BatchNorm1d(6),\n            \n            nn.Conv1d(6,10, 8, 2, 1),\n            nn.ReLU(),\n            nn.Dropout(),\n            nn.MaxPool1d(kernel_size=2),\n            nn.BatchNorm1d(10),\n            \n            nn.Conv1d(10, 16, 4, 2, 1),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.Conv1d(16, 32, 3, 2, 1),\n            nn.ReLU(), \n            nn.Dropout(),\n            \n            nn.Conv1d(32, 64, 3, 2, 1),\n            nn.ReLU(), \n            nn.Dropout(),\n            \n            \n           \n           \n        )\n        \n        self.conv1d_channel_7 = nn.Sequential(\n            nn.Conv1d(1, 6, 16, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            nn.MaxPool1d(kernel_size=3),\n            nn.BatchNorm1d(6),\n            nn.Conv1d(6,10, 8, 2, 1),\n            nn.ReLU(),\n            nn.Dropout(),\n            nn.MaxPool1d(kernel_size=2),\n            nn.BatchNorm1d(10),\n            \n            nn.Conv1d(10, 16, 4, 2, 1),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.Conv1d(16, 32, 3, 2, 1),\n            nn.ReLU(), \n            nn.Dropout(),\n            \n            nn.Conv1d(32, 64, 3, 2, 1),\n            nn.ReLU(), \n            nn.Dropout(),\n            \n        )\n            \n\n       \n        \n        self.conv1d_channel_8 = nn.Sequential(\n            nn.Conv1d(1, 6, 16, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            nn.MaxPool1d(kernel_size=3),\n            nn.BatchNorm1d(6),\n            \n            nn.Conv1d(6,10, 8, 2, 1),\n            nn.ReLU(),\n            nn.Dropout(),\n            nn.BatchNorm1d(10),\n            \n            nn.MaxPool1d(kernel_size=2),\n            \n            nn.Conv1d(10, 16, 4, 2, 1),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.Conv1d(16, 32, 3, 2, 1),\n            nn.ReLU(), \n            nn.Dropout(),\n            \n            nn.Conv1d(32, 64, 3, 2, 1),\n            nn.ReLU(), \n            nn.Dropout(),\n            \n        )\n\n            \n\n        self.dense = nn.Sequential(\n            nn.Linear(576, 288),  nn.ReLU(),nn.Dropout(),\n            nn.Linear(288, 64),  nn.ReLU(),nn.Dropout(),\n            nn.Linear(64, no),  nn.ReLU(),\n        )\n        \n            \n\n    def forward(self, t_channel_0, t_channel_1, t_channel_2, t_channel_3, t_channel_4, t_channel_5, t_channel_6, t_channel_7, t_channel_8):\n        conv1d_out_channel_0 = self.conv1d_channel_0(t_channel_0)\n        conv1d_out_channel_1 = self.conv1d_channel_1(t_channel_1)\n        conv1d_out_channel_2 = self.conv1d_channel_2(t_channel_2)\n        conv1d_out_channel_3 = self.conv1d_channel_3(t_channel_3)\n        conv1d_out_channel_4 = self.conv1d_channel_4(t_channel_4)\n        conv1d_out_channel_5 = self.conv1d_channel_5(t_channel_5)\n        conv1d_out_channel_6 = self.conv1d_channel_6(t_channel_6)\n        conv1d_out_channel_7 = self.conv1d_channel_7(t_channel_7)\n        conv1d_out_channel_8 = self.conv1d_channel_8(t_channel_8)\n        \n        t_in = torch.cat([conv1d_out_channel_0,conv1d_out_channel_1, conv1d_out_channel_2, conv1d_out_channel_3, conv1d_out_channel_4, conv1d_out_channel_5, conv1d_out_channel_6, conv1d_out_channel_7, conv1d_out_channel_8], dim=1)\n        res = t_in.view(t_in.size(0), -1)\n        out = self.dense(res)\n        return out\n        ","execution_count":942,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"5\">**Initial setup for random seed and project path**</font><br>"},{"metadata":{"_uuid":"b4b74c47-df3d-4201-93a7-f8cb79d6a652","_cell_guid":"aa2ea0fc-dfc2-436e-9a73-aab6bfb63d20","trusted":true},"cell_type":"code","source":"\n# set up the seed\nseed = 1\nnp.random.seed(seed)\n\n# path to sample submission\nsample = Path.cwd().parent/'input'/'career-con-2019'/'sample_submission.csv'\n\nROOT = Path.cwd().parent/'input'/'career-con-2019-preprocessed-data'\nraw_arr = np.load(ROOT/'feat.npy').transpose(0, 2, 1)\ntarget = np.load(ROOT/'target.npy')\n\nraw_dim_data = [None]*9\n#print(raw_dim_data)\nfor i in range(0, 9):\n    raw_dim_data[i] = raw_arr[:,i,:]\n#    print(\"raw data shape\")\n    \n    raw_dim_data[i] = raw_dim_data[i].reshape([7626,1,128])\n#    print(raw_dim_data[i].shape)\n    \n# print(\"raw array shape\")\n# print(raw_arr.shape)\n# print(\"label array shape\")\n# print(target.shape)\n\ntrn_sz = 3810  # only the first `trn_sz` rows in each array include labelled data\n#datasets = create_datasets((raw_arr), target, trn_sz, seed=seed)\ndatasets = create_datasets2((raw_dim_data), trn_sz, seed=seed)\n\n\n# make sure that we run on a proper device (not relevant for Kaggle kernels but helpful in Jupyter sessions)\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\n\ntrn_dl, val_dl, tst_dl = create_loaders(datasets, bs=256)","execution_count":943,"outputs":[{"output_type":"stream","text":"total data length\n3810\ninput one dimension shape\n(7626, 1, 128)\ntraining data length\n2286\nvalidation data le\n762\ntesting d\n762\n","name":"stdout"}]},{"metadata":{"_uuid":"b9026a32-1355-4689-8bf9-581a5228af69","_cell_guid":"5db66d39-80aa-48f4-9aab-4f44eace0192","trusted":true},"cell_type":"markdown","source":"<font size=\"5\">**Training model**</font><br>\nNow everything is ready to create a training loop and see if our model works. For each training epoch, the loop performs the following actions:\n1. train model on the `trn_ds` dataset;\n2. verify quality on the `val_ds` dataset;\n3. check if the quality improved since previous epoch, and if so, save the model's weights onto disk;\n4. in case if the model's quality isn't impoving for `patience` epochs, the training is stopped.\nAlso, the code tracks loss and accuracy history, and prints current scores with exponentially increasing logging frequency, i.e., only at 1, 2, 4, 8... epochs."},{"metadata":{"_uuid":"dee51d07-1cff-4db3-a87d-bebc4bf5bcc0","_cell_guid":"72095ada-fe94-4757-9a8a-918481b04336","trusted":true},"cell_type":"code","source":"raw_feat = raw_arr.shape[1]\n\n\nlr = 0.001\nn_epochs = 10000\niterations_per_epoch = len(trn_dl)\nnum_classes = 9\nbest_acc = 0\npatience, trials = 500, 0\nbase = 1\nstep = 2\nloss_history = []\nacc_history = []\n\nmodel = Surface_Classifier(raw_feat, num_classes).to(device)\nmodel.cuda()\ncriterion = nn.CrossEntropyLoss(reduction='sum')\nopt = optim.Adam(model.parameters(), lr=lr)\n\nprint('Start model training')\n\nfor epoch in range(1, n_epochs + 1):\n    \n    model.train()\n    epoch_loss = 0\n    for i, batch in enumerate(trn_dl):\n        #x_raw, y_batch = [t.to(device) for t in batch]\n        x_channel_0, x_channel_1,  x_channel_2,  x_channel_3,  x_channel_4,  x_channel_5,  x_channel_6,  x_channel_7,  x_channel_8, y_batch = [t.to(device) for t in batch]\n        opt.zero_grad()\n        \n#         print(\"channel 0 type\")\n#         print(type(x_channel_0))\n#         print(\"channel 0 shape\")\n#         print(x_channel_0.shape)\n#         print(\"batch type\")\n#         print(type(batch))\n#         print(len(batch))\n#         print(batch[0].shape)\n#         print(batch[9].shape)\n\n        out = model(x_channel_0, x_channel_1, x_channel_2, x_channel_3, x_channel_4, x_channel_5, x_channel_6, x_channel_7, x_channel_8)\n        \n        \n#         out = []\n#         with torch.no_grad():\n#             for x_channel_0, x_channel_1,  x_channel_2,  x_channel_3,  x_channel_4,  x_channel_5,  x_channel_6,  x_channel_7,  x_channel_8, y_batch in batch:\n#                 output = model(x_channel_0, x_channel_1, x_channel_2, x_channel_3, x_channel_4, x_channel_5, x_channel_6, x_channel_7, x_channel_8)\n#                 out.append(output.detach())\n#         out = torch.cat(out)\n        \n\n    \n        loss = criterion(out, y_batch)\n        epoch_loss += loss.item()\n        loss.backward()\n        opt.step()\n        \n    epoch_loss /= trn_sz\n    loss_history.append(epoch_loss)\n    \n    model.eval()\n    correct, total = 0, 0\n    \n    for batch in val_dl:\n        x_channel_0, x_channel_1,  x_channel_2,  x_channel_3,  x_channel_4,  x_channel_5,  x_channel_6,  x_channel_7,  x_channel_8, y_batch = [t.to(device) for t in batch]\n        \n        out = model(x_channel_0, x_channel_1, x_channel_2, x_channel_3, x_channel_4, x_channel_5, x_channel_6, x_channel_7, x_channel_8)\n        preds = F.log_softmax(out, dim=1).argmax(dim=1)\n        total += y_batch.size(0)\n        correct += (preds == y_batch).sum().item()\n    \n    acc = correct / total\n    acc_history.append(acc)\n\n    if epoch % base == 0:\n        print(f'Epoch: {epoch:3d}. Loss: {epoch_loss:.4f}. Acc.: {acc:2.2%}')\n        base *= step\n    print(f'Epoch: {epoch:3d}. Loss: {epoch_loss:.4f}. Acc.: {acc:2.2%}')\n\n    \n    if acc > best_acc:\n        trials = 0\n        best_acc = acc\n        torch.save(model.state_dict(), 'best.pth')\n        print(f'Epoch {epoch} best model saved with accuracy: {best_acc:2.2%}')\n    else:\n        trials += 1\n        if trials >= patience:\n            print(f'Early stopping on epoch {epoch}')\n            break\n            \nprint('Done!')","execution_count":null,"outputs":[{"output_type":"stream","text":"Start model training\nEpoch:   1. Loss: 1.3192. Acc.: 15.09%\nEpoch:   1. Loss: 1.3192. Acc.: 15.09%\nEpoch 1 best model saved with accuracy: 15.09%\nEpoch:   2. Loss: 1.3147. Acc.: 15.09%\nEpoch:   2. Loss: 1.3147. Acc.: 15.09%\nEpoch:   3. Loss: 1.3015. Acc.: 15.09%\nEpoch:   4. Loss: 1.2751. Acc.: 21.78%\nEpoch:   4. Loss: 1.2751. Acc.: 21.78%\nEpoch 4 best model saved with accuracy: 21.78%\nEpoch:   5. Loss: 1.2563. Acc.: 22.05%\nEpoch 5 best model saved with accuracy: 22.05%\nEpoch:   6. Loss: 1.2592. Acc.: 21.78%\nEpoch:   7. Loss: 1.2455. Acc.: 21.78%\nEpoch:   8. Loss: 1.2435. Acc.: 23.62%\nEpoch:   8. Loss: 1.2435. Acc.: 23.62%\nEpoch 8 best model saved with accuracy: 23.62%\nEpoch:   9. Loss: 1.2330. Acc.: 27.69%\nEpoch 9 best model saved with accuracy: 27.69%\nEpoch:  10. Loss: 1.2118. Acc.: 27.82%\nEpoch 10 best model saved with accuracy: 27.82%\nEpoch:  11. Loss: 1.1742. Acc.: 33.60%\nEpoch 11 best model saved with accuracy: 33.60%\nEpoch:  12. Loss: 1.1397. Acc.: 32.28%\nEpoch:  13. Loss: 1.1176. Acc.: 33.73%\nEpoch 13 best model saved with accuracy: 33.73%\nEpoch:  14. Loss: 1.1165. Acc.: 34.38%\nEpoch 14 best model saved with accuracy: 34.38%\nEpoch:  15. Loss: 1.0974. Acc.: 33.86%\nEpoch:  16. Loss: 1.0957. Acc.: 35.30%\nEpoch:  16. Loss: 1.0957. Acc.: 35.30%\nEpoch 16 best model saved with accuracy: 35.30%\nEpoch:  17. Loss: 1.0946. Acc.: 34.12%\nEpoch:  18. Loss: 1.0764. Acc.: 34.78%\nEpoch:  19. Loss: 1.0645. Acc.: 35.56%\nEpoch 19 best model saved with accuracy: 35.56%\nEpoch:  20. Loss: 1.0675. Acc.: 35.43%\nEpoch:  21. Loss: 1.0665. Acc.: 35.56%\nEpoch:  22. Loss: 1.0590. Acc.: 35.43%\nEpoch:  23. Loss: 1.0599. Acc.: 34.78%\nEpoch:  24. Loss: 1.0422. Acc.: 34.91%\nEpoch:  25. Loss: 1.0378. Acc.: 38.71%\nEpoch 25 best model saved with accuracy: 38.71%\nEpoch:  26. Loss: 1.0416. Acc.: 39.24%\nEpoch 26 best model saved with accuracy: 39.24%\nEpoch:  27. Loss: 1.0433. Acc.: 37.93%\nEpoch:  28. Loss: 1.0278. Acc.: 38.71%\nEpoch:  29. Loss: 1.0212. Acc.: 39.90%\nEpoch 29 best model saved with accuracy: 39.90%\nEpoch:  30. Loss: 1.0098. Acc.: 39.90%\nEpoch:  31. Loss: 1.0154. Acc.: 40.81%\nEpoch 31 best model saved with accuracy: 40.81%\nEpoch:  32. Loss: 1.0041. Acc.: 40.55%\nEpoch:  32. Loss: 1.0041. Acc.: 40.55%\nEpoch:  33. Loss: 0.9975. Acc.: 39.63%\nEpoch:  34. Loss: 0.9957. Acc.: 40.55%\nEpoch:  35. Loss: 0.9809. Acc.: 42.78%\nEpoch 35 best model saved with accuracy: 42.78%\nEpoch:  36. Loss: 0.9806. Acc.: 41.73%\nEpoch:  37. Loss: 0.9755. Acc.: 43.04%\nEpoch 37 best model saved with accuracy: 43.04%\nEpoch:  38. Loss: 0.9739. Acc.: 40.29%\nEpoch:  39. Loss: 0.9673. Acc.: 43.57%\nEpoch 39 best model saved with accuracy: 43.57%\nEpoch:  40. Loss: 0.9553. Acc.: 40.42%\nEpoch:  41. Loss: 0.9754. Acc.: 42.91%\nEpoch:  42. Loss: 0.9649. Acc.: 41.47%\nEpoch:  43. Loss: 0.9633. Acc.: 41.99%\nEpoch:  44. Loss: 0.9768. Acc.: 43.04%\nEpoch:  45. Loss: 0.9695. Acc.: 43.31%\nEpoch:  46. Loss: 0.9465. Acc.: 40.68%\nEpoch:  47. Loss: 0.9466. Acc.: 41.34%\nEpoch:  48. Loss: 0.9495. Acc.: 43.83%\nEpoch 48 best model saved with accuracy: 43.83%\nEpoch:  49. Loss: 0.9405. Acc.: 42.78%\nEpoch:  50. Loss: 0.9378. Acc.: 43.18%\nEpoch:  51. Loss: 0.9425. Acc.: 42.91%\nEpoch:  52. Loss: 0.9463. Acc.: 43.44%\nEpoch:  53. Loss: 0.9361. Acc.: 40.42%\nEpoch:  54. Loss: 0.9386. Acc.: 41.99%\nEpoch:  55. Loss: 0.9518. Acc.: 44.09%\nEpoch 55 best model saved with accuracy: 44.09%\nEpoch:  56. Loss: 0.9402. Acc.: 42.52%\nEpoch:  57. Loss: 0.9355. Acc.: 44.49%\nEpoch 57 best model saved with accuracy: 44.49%\nEpoch:  58. Loss: 0.9288. Acc.: 45.28%\nEpoch 58 best model saved with accuracy: 45.28%\nEpoch:  59. Loss: 0.9283. Acc.: 41.86%\nEpoch:  60. Loss: 0.9276. Acc.: 44.49%\nEpoch:  61. Loss: 0.9194. Acc.: 44.88%\nEpoch:  62. Loss: 0.9314. Acc.: 43.70%\nEpoch:  63. Loss: 0.9284. Acc.: 41.34%\nEpoch:  64. Loss: 0.9259. Acc.: 42.26%\nEpoch:  64. Loss: 0.9259. Acc.: 42.26%\nEpoch:  65. Loss: 0.9305. Acc.: 42.78%\nEpoch:  66. Loss: 0.9205. Acc.: 45.28%\nEpoch:  67. Loss: 0.9214. Acc.: 45.14%\nEpoch:  68. Loss: 0.9118. Acc.: 44.75%\nEpoch:  69. Loss: 0.9151. Acc.: 43.83%\nEpoch:  70. Loss: 0.9128. Acc.: 45.14%\nEpoch:  71. Loss: 0.9018. Acc.: 43.44%\nEpoch:  72. Loss: 0.9015. Acc.: 44.62%\nEpoch:  73. Loss: 0.9019. Acc.: 43.44%\nEpoch:  74. Loss: 0.9038. Acc.: 45.41%\nEpoch 74 best model saved with accuracy: 45.41%\nEpoch:  75. Loss: 0.9042. Acc.: 44.23%\nEpoch:  76. Loss: 0.8865. Acc.: 45.14%\nEpoch:  77. Loss: 0.8953. Acc.: 44.75%\nEpoch:  78. Loss: 0.8867. Acc.: 43.83%\nEpoch:  79. Loss: 0.8644. Acc.: 45.14%\nEpoch:  80. Loss: 0.8856. Acc.: 45.14%\nEpoch:  81. Loss: 0.8788. Acc.: 45.01%\nEpoch:  82. Loss: 0.8710. Acc.: 45.14%\nEpoch:  83. Loss: 0.8728. Acc.: 45.54%\nEpoch 83 best model saved with accuracy: 45.54%\nEpoch:  84. Loss: 0.8722. Acc.: 46.46%\nEpoch 84 best model saved with accuracy: 46.46%\nEpoch:  85. Loss: 0.8774. Acc.: 47.11%\nEpoch 85 best model saved with accuracy: 47.11%\nEpoch:  86. Loss: 0.8801. Acc.: 45.14%\nEpoch:  87. Loss: 0.8714. Acc.: 47.64%\nEpoch 87 best model saved with accuracy: 47.64%\nEpoch:  88. Loss: 0.8714. Acc.: 46.46%\nEpoch:  89. Loss: 0.8615. Acc.: 47.90%\nEpoch 89 best model saved with accuracy: 47.90%\nEpoch:  90. Loss: 0.8558. Acc.: 46.85%\nEpoch:  91. Loss: 0.8600. Acc.: 48.16%\nEpoch 91 best model saved with accuracy: 48.16%\nEpoch:  92. Loss: 0.8647. Acc.: 46.98%\nEpoch:  93. Loss: 0.8605. Acc.: 48.43%\nEpoch 93 best model saved with accuracy: 48.43%\nEpoch:  94. Loss: 0.8562. Acc.: 48.43%\nEpoch:  95. Loss: 0.8608. Acc.: 48.69%\nEpoch 95 best model saved with accuracy: 48.69%\nEpoch:  96. Loss: 0.8670. Acc.: 47.11%\nEpoch:  97. Loss: 0.8640. Acc.: 50.39%\nEpoch 97 best model saved with accuracy: 50.39%\nEpoch:  98. Loss: 0.8518. Acc.: 48.56%\nEpoch:  99. Loss: 0.8360. Acc.: 48.56%\nEpoch: 100. Loss: 0.8452. Acc.: 47.64%\nEpoch: 101. Loss: 0.8344. Acc.: 49.74%\nEpoch: 102. Loss: 0.8416. Acc.: 48.03%\nEpoch: 103. Loss: 0.8310. Acc.: 49.48%\nEpoch: 104. Loss: 0.8398. Acc.: 48.16%\nEpoch: 105. Loss: 0.8313. Acc.: 51.44%\nEpoch 105 best model saved with accuracy: 51.44%\nEpoch: 106. Loss: 0.8289. Acc.: 49.21%\nEpoch: 107. Loss: 0.8280. Acc.: 51.31%\nEpoch: 108. Loss: 0.8238. Acc.: 50.13%\nEpoch: 109. Loss: 0.8197. Acc.: 49.48%\nEpoch: 110. Loss: 0.8224. Acc.: 51.31%\nEpoch: 111. Loss: 0.8248. Acc.: 50.66%\nEpoch: 112. Loss: 0.8234. Acc.: 50.52%\nEpoch: 113. Loss: 0.8172. Acc.: 51.18%\nEpoch: 114. Loss: 0.8132. Acc.: 50.92%\nEpoch: 115. Loss: 0.8255. Acc.: 49.48%\nEpoch: 116. Loss: 0.8223. Acc.: 50.52%\nEpoch: 117. Loss: 0.8085. Acc.: 50.52%\nEpoch: 118. Loss: 0.8138. Acc.: 50.79%\nEpoch: 119. Loss: 0.8061. Acc.: 48.95%\nEpoch: 120. Loss: 0.8154. Acc.: 52.89%\nEpoch 120 best model saved with accuracy: 52.89%\nEpoch: 121. Loss: 0.8050. Acc.: 51.05%\nEpoch: 122. Loss: 0.8074. Acc.: 51.97%\nEpoch: 123. Loss: 0.8079. Acc.: 50.00%\nEpoch: 124. Loss: 0.8046. Acc.: 51.05%\nEpoch: 125. Loss: 0.7952. Acc.: 51.97%\nEpoch: 126. Loss: 0.8119. Acc.: 50.66%\nEpoch: 127. Loss: 0.8129. Acc.: 51.71%\nEpoch: 128. Loss: 0.8141. Acc.: 48.95%\nEpoch: 128. Loss: 0.8141. Acc.: 48.95%\nEpoch: 129. Loss: 0.8098. Acc.: 51.05%\nEpoch: 130. Loss: 0.7947. Acc.: 50.26%\nEpoch: 131. Loss: 0.7985. Acc.: 50.00%\nEpoch: 132. Loss: 0.8115. Acc.: 51.84%\nEpoch: 133. Loss: 0.7884. Acc.: 49.87%\nEpoch: 134. Loss: 0.8072. Acc.: 50.26%\nEpoch: 135. Loss: 0.7971. Acc.: 49.48%\nEpoch: 136. Loss: 0.7925. Acc.: 51.31%\nEpoch: 137. Loss: 0.7909. Acc.: 51.71%\nEpoch: 138. Loss: 0.8030. Acc.: 50.13%\nEpoch: 139. Loss: 0.8091. Acc.: 50.26%\nEpoch: 140. Loss: 0.7974. Acc.: 51.18%\nEpoch: 141. Loss: 0.7882. Acc.: 49.08%\nEpoch: 142. Loss: 0.8019. Acc.: 51.44%\nEpoch: 143. Loss: 0.7946. Acc.: 50.79%\nEpoch: 144. Loss: 0.7939. Acc.: 51.44%\nEpoch: 145. Loss: 0.7887. Acc.: 49.87%\nEpoch: 146. Loss: 0.7826. Acc.: 52.62%\nEpoch: 147. Loss: 0.7896. Acc.: 48.69%\nEpoch: 148. Loss: 0.7866. Acc.: 48.03%\nEpoch: 149. Loss: 0.7869. Acc.: 52.10%\nEpoch: 150. Loss: 0.7874. Acc.: 52.62%\nEpoch: 151. Loss: 0.7768. Acc.: 49.61%\nEpoch: 152. Loss: 0.7879. Acc.: 50.39%\nEpoch: 153. Loss: 0.7838. Acc.: 51.57%\nEpoch: 154. Loss: 0.7782. Acc.: 53.15%\nEpoch 154 best model saved with accuracy: 53.15%\nEpoch: 155. Loss: 0.7976. Acc.: 51.57%\nEpoch: 156. Loss: 0.7873. Acc.: 52.10%\nEpoch: 157. Loss: 0.7903. Acc.: 51.31%\nEpoch: 158. Loss: 0.7818. Acc.: 51.84%\nEpoch: 159. Loss: 0.7690. Acc.: 53.41%\nEpoch 159 best model saved with accuracy: 53.41%\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 160. Loss: 0.7683. Acc.: 51.05%\nEpoch: 161. Loss: 0.7880. Acc.: 50.66%\nEpoch: 162. Loss: 0.7667. Acc.: 50.79%\nEpoch: 163. Loss: 0.7829. Acc.: 51.18%\nEpoch: 164. Loss: 0.7777. Acc.: 49.87%\nEpoch: 165. Loss: 0.7964. Acc.: 51.57%\nEpoch: 166. Loss: 0.7709. Acc.: 49.08%\nEpoch: 167. Loss: 0.7791. Acc.: 51.97%\nEpoch: 168. Loss: 0.7696. Acc.: 48.69%\nEpoch: 169. Loss: 0.7696. Acc.: 50.26%\nEpoch: 170. Loss: 0.7791. Acc.: 51.44%\nEpoch: 171. Loss: 0.7812. Acc.: 52.10%\nEpoch: 172. Loss: 0.7849. Acc.: 47.51%\nEpoch: 173. Loss: 0.7604. Acc.: 50.66%\nEpoch: 174. Loss: 0.7659. Acc.: 51.84%\nEpoch: 175. Loss: 0.7695. Acc.: 51.84%\nEpoch: 176. Loss: 0.7597. Acc.: 50.79%\nEpoch: 177. Loss: 0.7445. Acc.: 51.84%\nEpoch: 178. Loss: 0.7530. Acc.: 50.79%\nEpoch: 179. Loss: 0.7661. Acc.: 51.97%\nEpoch: 180. Loss: 0.7469. Acc.: 51.44%\nEpoch: 181. Loss: 0.7735. Acc.: 50.13%\nEpoch: 182. Loss: 0.7611. Acc.: 51.05%\nEpoch: 183. Loss: 0.7623. Acc.: 47.77%\nEpoch: 184. Loss: 0.7469. Acc.: 53.02%\nEpoch: 185. Loss: 0.7627. Acc.: 50.79%\nEpoch: 186. Loss: 0.7637. Acc.: 48.16%\nEpoch: 187. Loss: 0.7657. Acc.: 51.31%\nEpoch: 188. Loss: 0.7503. Acc.: 52.36%\nEpoch: 189. Loss: 0.7585. Acc.: 50.92%\nEpoch: 190. Loss: 0.7570. Acc.: 51.18%\nEpoch: 191. Loss: 0.7424. Acc.: 52.10%\nEpoch: 192. Loss: 0.7553. Acc.: 49.21%\nEpoch: 193. Loss: 0.7557. Acc.: 51.97%\nEpoch: 194. Loss: 0.7627. Acc.: 48.69%\nEpoch: 195. Loss: 0.7220. Acc.: 50.66%\nEpoch: 196. Loss: 0.7559. Acc.: 49.74%\nEpoch: 197. Loss: 0.7533. Acc.: 50.13%\nEpoch: 198. Loss: 0.7476. Acc.: 51.97%\nEpoch: 199. Loss: 0.7521. Acc.: 50.13%\nEpoch: 200. Loss: 0.7430. Acc.: 50.13%\nEpoch: 201. Loss: 0.7461. Acc.: 51.44%\nEpoch: 202. Loss: 0.7545. Acc.: 50.39%\nEpoch: 203. Loss: 0.7522. Acc.: 52.49%\nEpoch: 204. Loss: 0.7499. Acc.: 51.57%\nEpoch: 205. Loss: 0.7383. Acc.: 49.61%\nEpoch: 206. Loss: 0.7233. Acc.: 48.56%\nEpoch: 207. Loss: 0.7627. Acc.: 49.87%\nEpoch: 208. Loss: 0.7458. Acc.: 48.16%\nEpoch: 209. Loss: 0.7575. Acc.: 50.39%\nEpoch: 210. Loss: 0.7518. Acc.: 50.52%\nEpoch: 211. Loss: 0.7494. Acc.: 49.74%\nEpoch: 212. Loss: 0.7507. Acc.: 50.00%\nEpoch: 213. Loss: 0.7388. Acc.: 48.16%\nEpoch: 214. Loss: 0.7381. Acc.: 50.92%\nEpoch: 215. Loss: 0.7325. Acc.: 48.82%\nEpoch: 216. Loss: 0.7390. Acc.: 50.13%\nEpoch: 217. Loss: 0.7333. Acc.: 51.71%\nEpoch: 218. Loss: 0.7328. Acc.: 49.74%\nEpoch: 219. Loss: 0.7307. Acc.: 48.95%\nEpoch: 220. Loss: 0.7224. Acc.: 50.39%\nEpoch: 221. Loss: 0.7434. Acc.: 48.82%\nEpoch: 222. Loss: 0.7351. Acc.: 51.57%\nEpoch: 223. Loss: 0.7360. Acc.: 49.34%\nEpoch: 224. Loss: 0.7207. Acc.: 51.18%\nEpoch: 225. Loss: 0.7351. Acc.: 51.84%\nEpoch: 226. Loss: 0.7271. Acc.: 51.71%\nEpoch: 227. Loss: 0.7287. Acc.: 49.74%\nEpoch: 228. Loss: 0.7256. Acc.: 49.21%\nEpoch: 229. Loss: 0.7200. Acc.: 51.44%\nEpoch: 230. Loss: 0.7470. Acc.: 50.92%\nEpoch: 231. Loss: 0.7174. Acc.: 51.44%\nEpoch: 232. Loss: 0.7427. Acc.: 50.66%\nEpoch: 233. Loss: 0.7188. Acc.: 51.05%\nEpoch: 234. Loss: 0.7210. Acc.: 51.31%\nEpoch: 235. Loss: 0.7442. Acc.: 51.97%\nEpoch: 236. Loss: 0.7242. Acc.: 51.31%\nEpoch: 237. Loss: 0.7323. Acc.: 50.79%\nEpoch: 238. Loss: 0.7161. Acc.: 51.44%\nEpoch: 239. Loss: 0.7319. Acc.: 51.05%\nEpoch: 240. Loss: 0.7227. Acc.: 50.39%\nEpoch: 241. Loss: 0.7196. Acc.: 50.26%\nEpoch: 242. Loss: 0.7314. Acc.: 52.76%\nEpoch: 243. Loss: 0.7374. Acc.: 52.10%\nEpoch: 244. Loss: 0.7351. Acc.: 50.92%\nEpoch: 245. Loss: 0.7215. Acc.: 50.79%\nEpoch: 246. Loss: 0.7126. Acc.: 50.00%\nEpoch: 247. Loss: 0.7307. Acc.: 50.66%\nEpoch: 248. Loss: 0.7119. Acc.: 51.05%\nEpoch: 249. Loss: 0.7106. Acc.: 47.51%\nEpoch: 250. Loss: 0.7379. Acc.: 48.29%\nEpoch: 251. Loss: 0.7309. Acc.: 50.39%\nEpoch: 252. Loss: 0.7211. Acc.: 51.44%\nEpoch: 253. Loss: 0.7281. Acc.: 50.26%\nEpoch: 254. Loss: 0.7288. Acc.: 50.13%\nEpoch: 255. Loss: 0.7275. Acc.: 51.84%\nEpoch: 256. Loss: 0.7148. Acc.: 51.31%\nEpoch: 256. Loss: 0.7148. Acc.: 51.31%\nEpoch: 257. Loss: 0.7014. Acc.: 51.18%\nEpoch: 258. Loss: 0.7304. Acc.: 50.52%\nEpoch: 259. Loss: 0.7200. Acc.: 48.69%\nEpoch: 260. Loss: 0.7313. Acc.: 49.74%\nEpoch: 261. Loss: 0.7294. Acc.: 49.21%\nEpoch: 262. Loss: 0.7089. Acc.: 50.66%\nEpoch: 263. Loss: 0.7015. Acc.: 51.97%\nEpoch: 264. Loss: 0.7150. Acc.: 51.05%\nEpoch: 265. Loss: 0.7141. Acc.: 47.77%\nEpoch: 266. Loss: 0.7177. Acc.: 49.34%\nEpoch: 267. Loss: 0.7121. Acc.: 51.84%\nEpoch: 268. Loss: 0.7138. Acc.: 47.64%\nEpoch: 269. Loss: 0.7254. Acc.: 52.49%\nEpoch: 270. Loss: 0.7098. Acc.: 50.92%\nEpoch: 271. Loss: 0.7122. Acc.: 50.92%\nEpoch: 272. Loss: 0.7205. Acc.: 52.76%\nEpoch: 273. Loss: 0.7126. Acc.: 48.43%\nEpoch: 274. Loss: 0.7206. Acc.: 51.05%\nEpoch: 275. Loss: 0.7210. Acc.: 52.49%\nEpoch: 276. Loss: 0.7085. Acc.: 48.95%\nEpoch: 277. Loss: 0.7187. Acc.: 52.23%\nEpoch: 278. Loss: 0.7034. Acc.: 48.43%\nEpoch: 279. Loss: 0.7181. Acc.: 50.92%\nEpoch: 280. Loss: 0.7369. Acc.: 49.74%\nEpoch: 281. Loss: 0.7194. Acc.: 51.05%\nEpoch: 282. Loss: 0.6964. Acc.: 51.57%\nEpoch: 283. Loss: 0.7239. Acc.: 49.87%\nEpoch: 284. Loss: 0.7099. Acc.: 52.49%\nEpoch: 285. Loss: 0.7180. Acc.: 48.69%\nEpoch: 286. Loss: 0.7149. Acc.: 51.05%\nEpoch: 287. Loss: 0.7093. Acc.: 48.95%\nEpoch: 288. Loss: 0.7063. Acc.: 50.52%\nEpoch: 289. Loss: 0.7167. Acc.: 50.13%\nEpoch: 290. Loss: 0.7070. Acc.: 51.84%\nEpoch: 291. Loss: 0.6749. Acc.: 50.13%\nEpoch: 292. Loss: 0.7196. Acc.: 51.31%\nEpoch: 293. Loss: 0.7182. Acc.: 50.52%\nEpoch: 294. Loss: 0.7025. Acc.: 52.76%\nEpoch: 295. Loss: 0.6992. Acc.: 48.95%\nEpoch: 296. Loss: 0.7195. Acc.: 49.08%\nEpoch: 297. Loss: 0.7021. Acc.: 47.51%\nEpoch: 298. Loss: 0.7169. Acc.: 51.44%\nEpoch: 299. Loss: 0.7100. Acc.: 51.57%\nEpoch: 300. Loss: 0.7071. Acc.: 48.03%\nEpoch: 301. Loss: 0.6959. Acc.: 50.52%\nEpoch: 302. Loss: 0.7056. Acc.: 51.05%\nEpoch: 303. Loss: 0.6946. Acc.: 48.43%\nEpoch: 304. Loss: 0.6848. Acc.: 53.81%\nEpoch 304 best model saved with accuracy: 53.81%\nEpoch: 305. Loss: 0.7087. Acc.: 49.34%\nEpoch: 306. Loss: 0.7098. Acc.: 50.13%\nEpoch: 307. Loss: 0.7136. Acc.: 50.52%\nEpoch: 308. Loss: 0.7132. Acc.: 49.21%\nEpoch: 309. Loss: 0.6989. Acc.: 48.82%\nEpoch: 310. Loss: 0.7010. Acc.: 51.84%\nEpoch: 311. Loss: 0.7184. Acc.: 52.36%\nEpoch: 312. Loss: 0.6857. Acc.: 50.13%\nEpoch: 313. Loss: 0.7073. Acc.: 49.21%\nEpoch: 314. Loss: 0.6982. Acc.: 51.18%\nEpoch: 315. Loss: 0.7070. Acc.: 50.52%\nEpoch: 316. Loss: 0.7218. Acc.: 51.31%\nEpoch: 317. Loss: 0.6871. Acc.: 51.31%\nEpoch: 318. Loss: 0.6904. Acc.: 50.92%\nEpoch: 319. Loss: 0.7106. Acc.: 49.74%\nEpoch: 320. Loss: 0.6898. Acc.: 51.44%\nEpoch: 321. Loss: 0.7183. Acc.: 50.00%\nEpoch: 322. Loss: 0.6939. Acc.: 48.16%\nEpoch: 323. Loss: 0.7032. Acc.: 51.97%\nEpoch: 324. Loss: 0.6946. Acc.: 47.51%\nEpoch: 325. Loss: 0.6979. Acc.: 50.00%\nEpoch: 326. Loss: 0.7028. Acc.: 50.92%\nEpoch: 327. Loss: 0.6983. Acc.: 48.69%\nEpoch: 328. Loss: 0.6919. Acc.: 50.26%\nEpoch: 329. Loss: 0.7078. Acc.: 50.39%\nEpoch: 330. Loss: 0.7059. Acc.: 48.16%\nEpoch: 331. Loss: 0.7083. Acc.: 50.13%\nEpoch: 332. Loss: 0.7015. Acc.: 50.39%\nEpoch: 333. Loss: 0.6932. Acc.: 49.08%\nEpoch: 334. Loss: 0.6985. Acc.: 50.13%\nEpoch: 335. Loss: 0.6977. Acc.: 50.66%\nEpoch: 336. Loss: 0.6836. Acc.: 48.56%\nEpoch: 337. Loss: 0.6864. Acc.: 50.00%\nEpoch: 338. Loss: 0.6900. Acc.: 51.57%\nEpoch: 339. Loss: 0.6866. Acc.: 51.57%\nEpoch: 340. Loss: 0.6893. Acc.: 48.56%\nEpoch: 341. Loss: 0.6912. Acc.: 50.66%\nEpoch: 342. Loss: 0.6862. Acc.: 49.61%\nEpoch: 343. Loss: 0.6918. Acc.: 48.43%\nEpoch: 344. Loss: 0.7072. Acc.: 51.71%\nEpoch: 345. Loss: 0.6990. Acc.: 50.13%\nEpoch: 346. Loss: 0.6889. Acc.: 49.48%\nEpoch: 347. Loss: 0.6848. Acc.: 50.13%\nEpoch: 348. Loss: 0.6899. Acc.: 49.34%\nEpoch: 349. Loss: 0.6789. Acc.: 49.34%\nEpoch: 350. Loss: 0.6861. Acc.: 49.34%\nEpoch: 351. Loss: 0.6890. Acc.: 50.00%\nEpoch: 352. Loss: 0.6847. Acc.: 50.00%\nEpoch: 353. Loss: 0.6941. Acc.: 47.51%\nEpoch: 354. Loss: 0.7035. Acc.: 52.23%\nEpoch: 355. Loss: 0.6680. Acc.: 48.82%\nEpoch: 356. Loss: 0.7039. Acc.: 48.29%\nEpoch: 357. Loss: 0.6884. Acc.: 50.13%\nEpoch: 358. Loss: 0.6853. Acc.: 50.92%\nEpoch: 359. Loss: 0.6813. Acc.: 50.13%\nEpoch: 360. Loss: 0.7065. Acc.: 48.43%\nEpoch: 361. Loss: 0.6871. Acc.: 49.74%\nEpoch: 362. Loss: 0.6819. Acc.: 48.43%\nEpoch: 363. Loss: 0.6955. Acc.: 50.26%\nEpoch: 364. Loss: 0.6958. Acc.: 50.92%\nEpoch: 365. Loss: 0.6981. Acc.: 50.26%\nEpoch: 366. Loss: 0.6797. Acc.: 48.43%\nEpoch: 367. Loss: 0.6911. Acc.: 48.16%\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 368. Loss: 0.6666. Acc.: 49.34%\nEpoch: 369. Loss: 0.6840. Acc.: 49.48%\nEpoch: 370. Loss: 0.6780. Acc.: 48.29%\nEpoch: 371. Loss: 0.6962. Acc.: 46.98%\nEpoch: 372. Loss: 0.6833. Acc.: 48.43%\nEpoch: 373. Loss: 0.6968. Acc.: 50.66%\nEpoch: 374. Loss: 0.7084. Acc.: 49.48%\nEpoch: 375. Loss: 0.6778. Acc.: 50.00%\nEpoch: 376. Loss: 0.6893. Acc.: 50.00%\nEpoch: 377. Loss: 0.6827. Acc.: 50.92%\nEpoch: 378. Loss: 0.6782. Acc.: 49.74%\nEpoch: 379. Loss: 0.6784. Acc.: 49.21%\nEpoch: 380. Loss: 0.6926. Acc.: 50.00%\nEpoch: 381. Loss: 0.6795. Acc.: 50.13%\nEpoch: 382. Loss: 0.6949. Acc.: 51.05%\nEpoch: 383. Loss: 0.6778. Acc.: 49.08%\nEpoch: 384. Loss: 0.6704. Acc.: 49.74%\nEpoch: 385. Loss: 0.6899. Acc.: 49.48%\nEpoch: 386. Loss: 0.6799. Acc.: 51.05%\nEpoch: 387. Loss: 0.6717. Acc.: 49.21%\nEpoch: 388. Loss: 0.6852. Acc.: 48.43%\nEpoch: 389. Loss: 0.6590. Acc.: 48.69%\nEpoch: 390. Loss: 0.6745. Acc.: 46.85%\nEpoch: 391. Loss: 0.6687. Acc.: 49.21%\nEpoch: 392. Loss: 0.6756. Acc.: 51.57%\nEpoch: 393. Loss: 0.6882. Acc.: 50.00%\nEpoch: 394. Loss: 0.6787. Acc.: 51.57%\nEpoch: 395. Loss: 0.6826. Acc.: 48.69%\nEpoch: 396. Loss: 0.6790. Acc.: 50.92%\nEpoch: 397. Loss: 0.6917. Acc.: 51.57%\nEpoch: 398. Loss: 0.6709. Acc.: 48.95%\nEpoch: 399. Loss: 0.6897. Acc.: 48.29%\nEpoch: 400. Loss: 0.6659. Acc.: 52.76%\nEpoch: 401. Loss: 0.6884. Acc.: 47.11%\nEpoch: 402. Loss: 0.6692. Acc.: 50.39%\nEpoch: 403. Loss: 0.6916. Acc.: 47.90%\nEpoch: 404. Loss: 0.6631. Acc.: 48.03%\nEpoch: 405. Loss: 0.6676. Acc.: 48.95%\nEpoch: 406. Loss: 0.6697. Acc.: 45.93%\nEpoch: 407. Loss: 0.6886. Acc.: 48.69%\nEpoch: 408. Loss: 0.6817. Acc.: 50.79%\nEpoch: 409. Loss: 0.6802. Acc.: 49.34%\nEpoch: 410. Loss: 0.6888. Acc.: 47.51%\nEpoch: 411. Loss: 0.6781. Acc.: 52.10%\nEpoch: 412. Loss: 0.6586. Acc.: 48.16%\nEpoch: 413. Loss: 0.6695. Acc.: 49.21%\nEpoch: 414. Loss: 0.6850. Acc.: 52.49%\nEpoch: 415. Loss: 0.6777. Acc.: 50.52%\nEpoch: 416. Loss: 0.6738. Acc.: 48.43%\nEpoch: 417. Loss: 0.6735. Acc.: 51.05%\nEpoch: 418. Loss: 0.6751. Acc.: 49.34%\nEpoch: 419. Loss: 0.6857. Acc.: 49.61%\nEpoch: 420. Loss: 0.6774. Acc.: 51.71%\nEpoch: 421. Loss: 0.6730. Acc.: 50.39%\nEpoch: 422. Loss: 0.6864. Acc.: 47.77%\nEpoch: 423. Loss: 0.6520. Acc.: 52.23%\nEpoch: 424. Loss: 0.6766. Acc.: 50.00%\nEpoch: 425. Loss: 0.6760. Acc.: 48.29%\nEpoch: 426. Loss: 0.6681. Acc.: 51.71%\nEpoch: 427. Loss: 0.6622. Acc.: 49.21%\nEpoch: 428. Loss: 0.6910. Acc.: 49.87%\nEpoch: 429. Loss: 0.6965. Acc.: 51.05%\nEpoch: 430. Loss: 0.6769. Acc.: 48.29%\nEpoch: 431. Loss: 0.6874. Acc.: 49.61%\nEpoch: 432. Loss: 0.6690. Acc.: 48.16%\nEpoch: 433. Loss: 0.6785. Acc.: 51.44%\nEpoch: 434. Loss: 0.6668. Acc.: 50.00%\nEpoch: 435. Loss: 0.6673. Acc.: 48.43%\nEpoch: 436. Loss: 0.6593. Acc.: 50.13%\nEpoch: 437. Loss: 0.6747. Acc.: 50.13%\nEpoch: 438. Loss: 0.6868. Acc.: 50.00%\nEpoch: 439. Loss: 0.6668. Acc.: 50.13%\nEpoch: 440. Loss: 0.6891. Acc.: 49.48%\nEpoch: 441. Loss: 0.6890. Acc.: 49.21%\nEpoch: 442. Loss: 0.6845. Acc.: 51.71%\nEpoch: 443. Loss: 0.6783. Acc.: 49.74%\nEpoch: 444. Loss: 0.6804. Acc.: 50.26%\nEpoch: 445. Loss: 0.6818. Acc.: 50.13%\nEpoch: 446. Loss: 0.6730. Acc.: 49.61%\nEpoch: 447. Loss: 0.6747. Acc.: 48.16%\nEpoch: 448. Loss: 0.6657. Acc.: 50.79%\nEpoch: 449. Loss: 0.6742. Acc.: 52.10%\nEpoch: 450. Loss: 0.6618. Acc.: 46.46%\nEpoch: 451. Loss: 0.6708. Acc.: 50.52%\nEpoch: 452. Loss: 0.6663. Acc.: 50.13%\nEpoch: 453. Loss: 0.6554. Acc.: 50.92%\nEpoch: 454. Loss: 0.6865. Acc.: 48.82%\nEpoch: 455. Loss: 0.6613. Acc.: 50.92%\nEpoch: 456. Loss: 0.6673. Acc.: 49.61%\nEpoch: 457. Loss: 0.6734. Acc.: 47.77%\nEpoch: 458. Loss: 0.6809. Acc.: 51.31%\nEpoch: 459. Loss: 0.6809. Acc.: 50.13%\nEpoch: 460. Loss: 0.6683. Acc.: 47.38%\nEpoch: 461. Loss: 0.6658. Acc.: 48.29%\nEpoch: 462. Loss: 0.6715. Acc.: 51.05%\nEpoch: 463. Loss: 0.6748. Acc.: 50.52%\nEpoch: 464. Loss: 0.6577. Acc.: 50.13%\nEpoch: 465. Loss: 0.6589. Acc.: 48.43%\nEpoch: 466. Loss: 0.6576. Acc.: 49.61%\nEpoch: 467. Loss: 0.6645. Acc.: 48.82%\nEpoch: 468. Loss: 0.6570. Acc.: 51.84%\nEpoch: 469. Loss: 0.6678. Acc.: 49.74%\nEpoch: 470. Loss: 0.6688. Acc.: 47.90%\nEpoch: 471. Loss: 0.6703. Acc.: 51.18%\nEpoch: 472. Loss: 0.6559. Acc.: 49.87%\nEpoch: 473. Loss: 0.6811. Acc.: 49.87%\nEpoch: 474. Loss: 0.6742. Acc.: 50.52%\nEpoch: 475. Loss: 0.6707. Acc.: 48.95%\nEpoch: 476. Loss: 0.6676. Acc.: 49.08%\nEpoch: 477. Loss: 0.6777. Acc.: 51.97%\nEpoch: 478. Loss: 0.6692. Acc.: 50.26%\nEpoch: 479. Loss: 0.6743. Acc.: 49.21%\nEpoch: 480. Loss: 0.6701. Acc.: 50.52%\nEpoch: 481. Loss: 0.6495. Acc.: 49.08%\nEpoch: 482. Loss: 0.6662. Acc.: 48.56%\nEpoch: 483. Loss: 0.6746. Acc.: 51.71%\nEpoch: 484. Loss: 0.6875. Acc.: 47.24%\nEpoch: 485. Loss: 0.6700. Acc.: 47.24%\nEpoch: 486. Loss: 0.6586. Acc.: 50.13%\nEpoch: 487. Loss: 0.6626. Acc.: 50.13%\nEpoch: 488. Loss: 0.6727. Acc.: 45.41%\nEpoch: 489. Loss: 0.6703. Acc.: 48.56%\nEpoch: 490. Loss: 0.6599. Acc.: 47.77%\nEpoch: 491. Loss: 0.6654. Acc.: 47.64%\nEpoch: 492. Loss: 0.6470. Acc.: 49.08%\nEpoch: 493. Loss: 0.6674. Acc.: 47.51%\nEpoch: 494. Loss: 0.6603. Acc.: 49.21%\nEpoch: 495. Loss: 0.6505. Acc.: 45.80%\nEpoch: 496. Loss: 0.6577. Acc.: 51.05%\nEpoch: 497. Loss: 0.6568. Acc.: 48.29%\nEpoch: 498. Loss: 0.6576. Acc.: 48.82%\nEpoch: 499. Loss: 0.6608. Acc.: 47.38%\nEpoch: 500. Loss: 0.6668. Acc.: 51.57%\nEpoch: 501. Loss: 0.6598. Acc.: 46.33%\nEpoch: 502. Loss: 0.6604. Acc.: 48.56%\nEpoch: 503. Loss: 0.6631. Acc.: 50.26%\nEpoch: 504. Loss: 0.6717. Acc.: 48.95%\nEpoch: 505. Loss: 0.6374. Acc.: 49.48%\nEpoch: 506. Loss: 0.6445. Acc.: 51.84%\nEpoch: 507. Loss: 0.6655. Acc.: 48.82%\nEpoch: 508. Loss: 0.6648. Acc.: 50.13%\nEpoch: 509. Loss: 0.6675. Acc.: 47.51%\nEpoch: 510. Loss: 0.6709. Acc.: 46.46%\nEpoch: 511. Loss: 0.6656. Acc.: 51.84%\nEpoch: 512. Loss: 0.6810. Acc.: 48.95%\nEpoch: 512. Loss: 0.6810. Acc.: 48.95%\nEpoch: 513. Loss: 0.6562. Acc.: 49.87%\nEpoch: 514. Loss: 0.6780. Acc.: 50.13%\nEpoch: 515. Loss: 0.6706. Acc.: 50.00%\nEpoch: 516. Loss: 0.6490. Acc.: 48.95%\nEpoch: 517. Loss: 0.6627. Acc.: 48.03%\nEpoch: 518. Loss: 0.6495. Acc.: 47.77%\nEpoch: 519. Loss: 0.6631. Acc.: 48.56%\nEpoch: 520. Loss: 0.6716. Acc.: 47.51%\nEpoch: 521. Loss: 0.6599. Acc.: 50.39%\nEpoch: 522. Loss: 0.6451. Acc.: 48.95%\nEpoch: 523. Loss: 0.6473. Acc.: 48.16%\nEpoch: 524. Loss: 0.6476. Acc.: 48.29%\nEpoch: 525. Loss: 0.6563. Acc.: 48.95%\nEpoch: 526. Loss: 0.6487. Acc.: 48.16%\nEpoch: 527. Loss: 0.6518. Acc.: 49.61%\nEpoch: 528. Loss: 0.6692. Acc.: 50.39%\nEpoch: 529. Loss: 0.6688. Acc.: 49.61%\nEpoch: 530. Loss: 0.6587. Acc.: 50.39%\nEpoch: 531. Loss: 0.6607. Acc.: 51.18%\nEpoch: 532. Loss: 0.6578. Acc.: 49.08%\nEpoch: 533. Loss: 0.6712. Acc.: 48.16%\nEpoch: 534. Loss: 0.6459. Acc.: 51.84%\nEpoch: 535. Loss: 0.6476. Acc.: 48.56%\nEpoch: 536. Loss: 0.6465. Acc.: 50.92%\nEpoch: 537. Loss: 0.6439. Acc.: 50.13%\nEpoch: 538. Loss: 0.6484. Acc.: 48.95%\nEpoch: 539. Loss: 0.6686. Acc.: 50.39%\nEpoch: 540. Loss: 0.6781. Acc.: 50.39%\nEpoch: 541. Loss: 0.6580. Acc.: 47.24%\nEpoch: 542. Loss: 0.6452. Acc.: 49.08%\nEpoch: 543. Loss: 0.6612. Acc.: 51.18%\nEpoch: 544. Loss: 0.6676. Acc.: 48.82%\nEpoch: 545. Loss: 0.6430. Acc.: 49.21%\nEpoch: 546. Loss: 0.6581. Acc.: 48.95%\nEpoch: 547. Loss: 0.6500. Acc.: 49.08%\nEpoch: 548. Loss: 0.6487. Acc.: 48.16%\nEpoch: 549. Loss: 0.6677. Acc.: 48.69%\nEpoch: 550. Loss: 0.6729. Acc.: 50.66%\nEpoch: 551. Loss: 0.6492. Acc.: 49.08%\nEpoch: 552. Loss: 0.6461. Acc.: 47.24%\nEpoch: 553. Loss: 0.6502. Acc.: 47.38%\nEpoch: 554. Loss: 0.6577. Acc.: 49.08%\nEpoch: 555. Loss: 0.6367. Acc.: 49.74%\nEpoch: 556. Loss: 0.6426. Acc.: 48.69%\nEpoch: 557. Loss: 0.6473. Acc.: 49.87%\nEpoch: 558. Loss: 0.6640. Acc.: 51.31%\nEpoch: 559. Loss: 0.6535. Acc.: 51.97%\nEpoch: 560. Loss: 0.6580. Acc.: 46.72%\nEpoch: 561. Loss: 0.6490. Acc.: 50.52%\nEpoch: 562. Loss: 0.6531. Acc.: 49.34%\nEpoch: 563. Loss: 0.6538. Acc.: 47.11%\nEpoch: 564. Loss: 0.6433. Acc.: 47.77%\nEpoch: 565. Loss: 0.6358. Acc.: 49.61%\nEpoch: 566. Loss: 0.6495. Acc.: 49.74%\nEpoch: 567. Loss: 0.6474. Acc.: 51.71%\nEpoch: 568. Loss: 0.6396. Acc.: 49.34%\nEpoch: 569. Loss: 0.6414. Acc.: 50.79%\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"5\">**Compute result on testing data**</font><br>"},{"metadata":{"_uuid":"d0f483d5-5542-4d65-a70f-1aa7b3e13c3c","_cell_guid":"05b2b8d9-2b63-436e-a906-a839fb008c0e","trusted":true},"cell_type":"code","source":"test_results = []\nmodel.load_state_dict(torch.load('best.pth'))\nmodel.eval()\n\n# for batch in val_dl:\n#     x_raw, y_batch = [t.to(device) for t in batch]\n#     out = model(x_raw)\n#     preds = F.log_softmax(out, dim=1).argmax(dim=1)\n#     total += y_batch.size(0)\n#     correct += (preds == y_batch).sum().item()\n#     acc = correct / total\n#     acc_history.append(acc)\n\n# for batch in tst_dl:\n#     x, y_batche = [t.to(device) for t in (batch)]\n#     out = model(x)\n#     y_hat = F.log_softmax(out, dim=1).argmax(dim=1)\n#     test_results.extend(y_hat.tolist())\n    \nfor batch in tst_dl:\n        x_raw, y_batch = [t.to(device) for t in batch]\n        out = model(x_channel_0, x_channel_1, x_channel_2, x_channel_3, x_channel_4, x_channel_5, x_channel_6, x_channel_7, x_channel_8)\n        preds = F.log_softmax(out, dim=1).argmax(dim=1)\n        total += y_batch.size(0)\n        correct += (preds == y_batch).sum().item()\n    \nacc = correct / total\nprint(\"accurancy on test data: \"+str(acc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"5\">**Store final result**</font><br>"},{"metadata":{"_uuid":"7a49a38e-e203-4884-ba78-33fe8aa02426","_cell_guid":"f4bd2628-c801-42d5-9af6-11bbd89f2352","trusted":true},"cell_type":"code","source":"'''\nsubmit = pd.read_csv(sample)\nenc = joblib.load(ROOT/'encoder.model')\nsubmit['surface'] = enc.inverse_transform(test_results)\nsubmit.to_csv('submit_base.csv', index=None)\nprint(\"store result successfully!\")\n'''","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}