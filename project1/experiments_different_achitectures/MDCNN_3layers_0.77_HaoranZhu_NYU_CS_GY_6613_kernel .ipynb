{"cells":[{"metadata":{},"cell_type":"markdown","source":"<font size=\"5\">**Project Summary**</font><br>\nIn this competition, the main task is to do surface time series classification. 1d convolution is a good technique when doing the time series classification job. So I apply 1d convolutional neural network to solve this problem. The whole code is written in Pytorch."},{"metadata":{},"cell_type":"markdown","source":"<font size=\"5\">**External libs we are using**</font><br>"},{"metadata":{"_uuid":"a6ed998e-ee9a-4333-8332-8fa753b7f0d4","_cell_guid":"e6e0bc2b-9cba-4688-9e0b-15f11d219c2d","trusted":true},"cell_type":"code","source":"# libs we are using\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport torch\nfrom torch import nn\nfrom torch import optim\nfrom torch.nn import functional as F\nfrom torch.utils.data import TensorDataset, DataLoader","execution_count":53,"outputs":[]},{"metadata":{"_uuid":"b8906e48-2384-4bb3-bba7-722160428e39","_cell_guid":"295d38d1-6663-4575-93ba-c6cfac9f6129","trusted":true},"cell_type":"markdown","source":"<font size=\"5\">**Data preparations**</font><br>\n\nIn this project, I use the raw data as input of the network. I concatenated all datasets into one single numpy array. The first 3810 rows are training data with given labels, the rest 3816 rows are testing data where labels are not given. In total raw data has 7816 rows. In addition to that, we split the raw training data into training data and validation data by the ratio of 80/20.\n\n\nI create three datasets and data loaders for them to make the data ready for model's training. The process is straightforward. I split the labelled data into two subsets, and keep testing data as is. Also, I convert Numpy arrays into `torch.tensor` objects of proper type (float for samples, and long - for targets).\n"},{"metadata":{"_uuid":"94c41c8d-0c91-4aa1-83a4-ff3f518f35f4","_cell_guid":"88b9ec00-fc64-4040-896b-895d06cd58f2","trusted":true},"cell_type":"code","source":"def create_datasets(data, target, train_size, valid_pct=0.2, seed=None):\n    \"\"\"Converts NumPy arrays into PyTorch datsets.\n    \n    Three datasets are created in total:\n        * training dataset\n        * validation dataset\n        * testing (un-labelled) dataset\n\n    \"\"\"\n    raw =  data\n    sz = train_size\n    idx = np.arange(sz)\n    trn_idx, val_idx = train_test_split(\n        idx, test_size=valid_pct, random_state=seed)\n     \n    trn_ds = TensorDataset(\n        torch.tensor(raw[:sz][trn_idx]).float(), \n        torch.tensor(target[:sz][trn_idx]).long())\n    val_ds = TensorDataset(\n        torch.tensor(raw[:sz][val_idx]).float(), \n        torch.tensor(target[:sz][val_idx]).long())\n    tst_ds = TensorDataset(\n        torch.tensor(raw[sz:]).float(), \n        torch.tensor(target[sz:]).long())\n    return trn_ds, val_ds, tst_ds\n\n# We use this function to divide the data\ndef create_datasets2(data_arr, train_size, valid_pct=0.2, seed=None):\n    \"\"\"Converts NumPy arrays into PyTorch datsets.\n    \n    Three datasets are created in total:\n        * training dataset\n        * validation dataset\n        * testing (un-labelled) dataset\n\n    \"\"\"\n    raw =  data_arr\n    sz = train_size\n\n    idx = np.arange(sz)\n    print(\"total data length\")\n    print(len(idx))\n    trn_idx, tst_idx = train_test_split(idx, test_size=0.2, random_state=1)\n    trn_idx, val_idx = train_test_split(trn_idx, test_size=0.25, random_state=1)\n    \n    print(\"input one dimension shape\")\n    print(raw[0].shape)\n    print(\"training data length\")\n    print(len(trn_idx))\n    print(\"validation data le\")\n    print(len(val_idx))\n    print(\"testing d\")\n    print(len(tst_idx))\n    \n    trn_ds = TensorDataset(\n        torch.tensor(raw[0][:sz][trn_idx]).float(),\n        torch.tensor(raw[1][:sz][trn_idx]).float(),\n        torch.tensor(raw[2][:sz][trn_idx]).float(),\n        torch.tensor(raw[3][:sz][trn_idx]).float(),\n        torch.tensor(raw[4][:sz][trn_idx]).float(),\n        torch.tensor(raw[5][:sz][trn_idx]).float(),\n        torch.tensor(raw[6][:sz][trn_idx]).float(),\n        torch.tensor(raw[7][:sz][trn_idx]).float(),\n        torch.tensor(raw[8][:sz][trn_idx]).float(),\n        torch.tensor(target[:sz][trn_idx]).long())\n    \n    val_ds = TensorDataset(\n        torch.tensor(raw[0][:sz][val_idx]).float(),\n        torch.tensor(raw[1][:sz][val_idx]).float(), \n        torch.tensor(raw[2][:sz][val_idx]).float(), \n        torch.tensor(raw[3][:sz][val_idx]).float(), \n        torch.tensor(raw[4][:sz][val_idx]).float(), \n        torch.tensor(raw[5][:sz][val_idx]).float(), \n        torch.tensor(raw[6][:sz][val_idx]).float(), \n        torch.tensor(raw[7][:sz][val_idx]).float(), \n        torch.tensor(raw[8][:sz][val_idx]).float(), \n        torch.tensor(target[:sz][val_idx]).long())\n    \n    tst_ds = TensorDataset(\n        torch.tensor(raw[0][:sz][tst_idx]).float(),\n        torch.tensor(raw[1][:sz][tst_idx]).float(),\n        torch.tensor(raw[2][:sz][tst_idx]).float(),\n        torch.tensor(raw[3][:sz][tst_idx]).float(),\n        torch.tensor(raw[4][:sz][tst_idx]).float(),\n        torch.tensor(raw[5][:sz][tst_idx]).float(),\n        torch.tensor(raw[6][:sz][tst_idx]).float(),\n        torch.tensor(raw[7][:sz][tst_idx]).float(),\n        torch.tensor(raw[8][:sz][tst_idx]).float(),\n        torch.tensor(target[:sz][tst_idx]).long())\n    \n    return trn_ds, val_ds, tst_ds\n\ndef create_loaders(data, bs=128, jobs=0):\n    \"\"\"Wraps the datasets returned by create_datasets function with data loaders.\"\"\"\n    \n    trn_ds, val_ds, tst_ds = data\n    trn_dl = DataLoader(trn_ds, batch_size=bs, shuffle=True, num_workers=jobs)\n    val_dl = DataLoader(val_ds, batch_size=bs, shuffle=False, num_workers=jobs)\n    tst_dl = DataLoader(tst_ds, batch_size=bs, shuffle=False, num_workers=jobs)\n    return trn_dl, val_dl, tst_dl","execution_count":54,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"5\">**Neural Network Architecture**</font><br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Flatten(nn.Module):\n    \"\"\"Converts N-dimensional tensor into 'flat' one.\"\"\"\n\n    def __init__(self, keep_batch_dim=True):\n        super().__init__()\n        self.keep_batch_dim = keep_batch_dim\n\n    def forward(self, x):\n        if self.keep_batch_dim:\n            return x.view(x.size(0), -1)\n        return x.view(-1)","execution_count":55,"outputs":[]},{"metadata":{"_uuid":"f144e459-53bb-4b5a-860d-abec117fc2cc","_cell_guid":"07870588-c88f-42ff-b782-5ae3c5e6e568","trusted":true},"cell_type":"code","source":"class Surface_Classifier(nn.Module):\n    def __init__(self, raw_ni, no, drop=.5):\n        super().__init__()\n        \n        self.conv1d_channel_0 = nn.Sequential(\n            nn.Conv1d(1, 8, 16, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n\n            nn.Conv1d(8, 16, 11, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n            \n            nn.Conv1d(16, 24, 8, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            \n           \n        \n        )\n            \n            \n        \n        self.conv1d_channel_1 = nn.Sequential(\n            nn.Conv1d(1, 8, 16, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n\n            nn.Conv1d(8, 16, 11, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n            \n            nn.Conv1d(16, 24, 8, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n        )\n        \n        self.conv1d_channel_2 = nn.Sequential(\n            nn.Conv1d(1, 8, 16, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n\n            nn.Conv1d(8, 16, 11, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n            \n            nn.Conv1d(16, 24, 8, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n        )\n        \n        self.conv1d_channel_3 = nn.Sequential(\n             nn.Conv1d(1, 8, 16, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n\n            nn.Conv1d(8, 16, 11, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n            \n            nn.Conv1d(16, 24, 8, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n        )\n        \n        self.conv1d_channel_4 = nn.Sequential(\n            nn.Conv1d(1, 8, 16, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n\n            nn.Conv1d(8, 16, 11, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n            \n            nn.Conv1d(16, 24, 8, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n        )\n        \n        self.conv1d_channel_5 = nn.Sequential(\n            nn.Conv1d(1, 8, 16, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n\n            nn.Conv1d(8, 16, 11, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n            \n            nn.Conv1d(16, 24, 8, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n        )\n        \n        self.conv1d_channel_6 = nn.Sequential(\n            nn.Conv1d(1, 8, 16, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n\n            nn.Conv1d(8, 16, 11, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n            \n            nn.Conv1d(16, 24, 8, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n           \n        )\n        \n        self.conv1d_channel_7 = nn.Sequential(\n            nn.Conv1d(1, 8, 16, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n\n            nn.Conv1d(8, 16, 11, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n            \n            nn.Conv1d(16, 24, 8, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n        )\n            \n\n       \n        \n        self.conv1d_channel_8 = nn.Sequential(\n            nn.Conv1d(1, 8, 16, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n\n            nn.Conv1d(8, 16, 11, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n            \n            nn.Conv1d(16, 24, 8, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n        )\n\n            \n\n        self.dense = nn.Sequential(\n            nn.Linear(216, 84),  nn.ReLU(),nn.Dropout(),\n            nn.Linear(84, 36),  nn.ReLU(),nn.Dropout(),\n            nn.Linear(36, no),  nn.ReLU()\n        )\n        \n            \n\n    def forward(self, t_channel_0, t_channel_1, t_channel_2, t_channel_3, t_channel_4, t_channel_5, t_channel_6, t_channel_7, t_channel_8):\n        conv1d_out_channel_0 = self.conv1d_channel_0(t_channel_0)\n        conv1d_out_channel_1 = self.conv1d_channel_1(t_channel_1)\n        conv1d_out_channel_2 = self.conv1d_channel_2(t_channel_2)\n        conv1d_out_channel_3 = self.conv1d_channel_3(t_channel_3)\n        conv1d_out_channel_4 = self.conv1d_channel_4(t_channel_4)\n        conv1d_out_channel_5 = self.conv1d_channel_5(t_channel_5)\n        conv1d_out_channel_6 = self.conv1d_channel_6(t_channel_6)\n        conv1d_out_channel_7 = self.conv1d_channel_7(t_channel_7)\n        conv1d_out_channel_8 = self.conv1d_channel_8(t_channel_8)\n        \n        t_in = torch.cat([conv1d_out_channel_0,conv1d_out_channel_1, conv1d_out_channel_2, conv1d_out_channel_3, conv1d_out_channel_4, conv1d_out_channel_5, conv1d_out_channel_6, conv1d_out_channel_7, conv1d_out_channel_8], dim=1)\n        res = t_in.view(t_in.size(0), -1)\n        out = self.dense(res)\n        return out\n        ","execution_count":56,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"5\">**Initial setup for random seed and project path**</font><br>"},{"metadata":{"_uuid":"b4b74c47-df3d-4201-93a7-f8cb79d6a652","_cell_guid":"aa2ea0fc-dfc2-436e-9a73-aab6bfb63d20","trusted":true},"cell_type":"code","source":"\n# set up the seed\nseed = 1\nnp.random.seed(seed)\n\n# path to sample submission\nsample = Path.cwd().parent/'input'/'career-con-2019'/'sample_submission.csv'\n\nROOT = Path.cwd().parent/'input'/'career-con-2019-preprocessed-data'\nraw_arr = np.load(ROOT/'feat.npy').transpose(0, 2, 1)\ntarget = np.load(ROOT/'target.npy')\n\nraw_dim_data = [None]*9\n#print(raw_dim_data)\nfor i in range(0, 9):\n    raw_dim_data[i] = raw_arr[:,i,:]\n#    print(\"raw data shape\")\n    \n    raw_dim_data[i] = raw_dim_data[i].reshape([7626,1,128])\n#    print(raw_dim_data[i].shape)\n    \n# print(\"raw array shape\")\n# print(raw_arr.shape)\n# print(\"label array shape\")\n# print(target.shape)\n\ntrn_sz = 3810  # only the first `trn_sz` rows in each array include labelled data\n#datasets = create_datasets((raw_arr), target, trn_sz, seed=seed)\ndatasets = create_datasets2((raw_dim_data), trn_sz, seed=seed)\n\n\n# make sure that we run on a proper device (not relevant for Kaggle kernels but helpful in Jupyter sessions)\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\n\ntrn_dl, val_dl, tst_dl = create_loaders(datasets, bs=256)","execution_count":57,"outputs":[{"output_type":"stream","text":"total data length\n3810\ninput one dimension shape\n(7626, 1, 128)\ntraining data length\n2286\nvalidation data le\n762\ntesting d\n762\n","name":"stdout"}]},{"metadata":{"_uuid":"b9026a32-1355-4689-8bf9-581a5228af69","_cell_guid":"5db66d39-80aa-48f4-9aab-4f44eace0192","trusted":true},"cell_type":"markdown","source":"<font size=\"5\">**Training model**</font><br>\nNow everything is ready to create a training loop and see if our model works. For each training epoch, the loop performs the following actions:\n1. train model on the `trn_ds` dataset;\n2. verify quality on the `val_ds` dataset;\n3. check if the quality improved since previous epoch, and if so, save the model's weights onto disk;\n4. in case if the model's quality isn't impoving for `patience` epochs, the training is stopped.\nAlso, the code tracks loss and accuracy history, and prints current scores with exponentially increasing logging frequency, i.e., only at 1, 2, 4, 8... epochs."},{"metadata":{"_uuid":"dee51d07-1cff-4db3-a87d-bebc4bf5bcc0","_cell_guid":"72095ada-fe94-4757-9a8a-918481b04336","trusted":true},"cell_type":"code","source":"raw_feat = raw_arr.shape[1]\n\n\nlr = 0.002\nn_epochs = 10000\niterations_per_epoch = len(trn_dl)\nnum_classes = 9\nbest_acc = 0\npatience, trials = 500, 0\nbase = 1\nstep = 2\nloss_history = []\nacc_history = []\n\nmodel = Surface_Classifier(raw_feat, num_classes).to(device)\nmodel.cuda()\ncriterion = nn.CrossEntropyLoss(reduction='sum')\nopt = optim.Adam(model.parameters(), lr=lr)\n\nprint('Start model training')\n\nfor epoch in range(1, n_epochs + 1):\n    \n    model.train()\n    epoch_loss = 0\n    for i, batch in enumerate(trn_dl):\n        #x_raw, y_batch = [t.to(device) for t in batch]\n        x_channel_0, x_channel_1,  x_channel_2,  x_channel_3,  x_channel_4,  x_channel_5,  x_channel_6,  x_channel_7,  x_channel_8, y_batch = [t.to(device) for t in batch]\n        opt.zero_grad()\n        \n#         print(\"channel 0 type\")\n#         print(type(x_channel_0))\n#         print(\"channel 0 shape\")\n#         print(x_channel_0.shape)\n#         print(\"batch type\")\n#         print(type(batch))\n#         print(len(batch))\n#         print(batch[0].shape)\n#         print(batch[9].shape)\n\n        out = model(x_channel_0, x_channel_1, x_channel_2, x_channel_3, x_channel_4, x_channel_5, x_channel_6, x_channel_7, x_channel_8)\n        \n        \n#         out = []\n#         with torch.no_grad():\n#             for x_channel_0, x_channel_1,  x_channel_2,  x_channel_3,  x_channel_4,  x_channel_5,  x_channel_6,  x_channel_7,  x_channel_8, y_batch in batch:\n#                 output = model(x_channel_0, x_channel_1, x_channel_2, x_channel_3, x_channel_4, x_channel_5, x_channel_6, x_channel_7, x_channel_8)\n#                 out.append(output.detach())\n#         out = torch.cat(out)\n        \n\n    \n        loss = criterion(out, y_batch)\n        epoch_loss += loss.item()\n        loss.backward()\n        opt.step()\n        \n    epoch_loss /= trn_sz\n    loss_history.append(epoch_loss)\n    \n    model.eval()\n    correct, total = 0, 0\n    \n    for batch in val_dl:\n        x_channel_0, x_channel_1,  x_channel_2,  x_channel_3,  x_channel_4,  x_channel_5,  x_channel_6,  x_channel_7,  x_channel_8, y_batch = [t.to(device) for t in batch]\n        \n        out = model(x_channel_0, x_channel_1, x_channel_2, x_channel_3, x_channel_4, x_channel_5, x_channel_6, x_channel_7, x_channel_8)\n        preds = F.log_softmax(out, dim=1).argmax(dim=1)\n        total += y_batch.size(0)\n        correct += (preds == y_batch).sum().item()\n    \n    acc = correct / total\n    acc_history.append(acc)\n\n    if epoch % base == 0:\n        print(f'Epoch: {epoch:3d}. Loss: {epoch_loss:.4f}. Acc.: {acc:2.2%}')\n        base *= step\n    print(f'Epoch: {epoch:3d}. Loss: {epoch_loss:.4f}. Acc.: {acc:2.2%}')\n\n    \n    if acc > best_acc:\n        trials = 0\n        best_acc = acc\n        torch.save(model.state_dict(), 'best.pth')\n        print(f'Epoch {epoch} best model saved with accuracy: {best_acc:2.2%}')\n    else:\n        trials += 1\n        if trials >= patience:\n            print(f'Early stopping on epoch {epoch}')\n            break\n            \nprint('Done!')","execution_count":null,"outputs":[{"output_type":"stream","text":"Start model training\nEpoch:   1. Loss: 1.3133. Acc.: 15.22%\nEpoch:   1. Loss: 1.3133. Acc.: 15.22%\nEpoch 1 best model saved with accuracy: 15.22%\nEpoch:   2. Loss: 1.2846. Acc.: 30.05%\nEpoch:   2. Loss: 1.2846. Acc.: 30.05%\nEpoch 2 best model saved with accuracy: 30.05%\nEpoch:   3. Loss: 1.2596. Acc.: 29.00%\nEpoch:   4. Loss: 1.2368. Acc.: 27.17%\nEpoch:   4. Loss: 1.2368. Acc.: 27.17%\nEpoch:   5. Loss: 1.2173. Acc.: 28.22%\nEpoch:   6. Loss: 1.2050. Acc.: 33.99%\nEpoch 6 best model saved with accuracy: 33.99%\nEpoch:   7. Loss: 1.1831. Acc.: 35.30%\nEpoch 7 best model saved with accuracy: 35.30%\nEpoch:   8. Loss: 1.1767. Acc.: 36.22%\nEpoch:   8. Loss: 1.1767. Acc.: 36.22%\nEpoch 8 best model saved with accuracy: 36.22%\nEpoch:   9. Loss: 1.1548. Acc.: 35.43%\nEpoch:  10. Loss: 1.1363. Acc.: 37.53%\nEpoch 10 best model saved with accuracy: 37.53%\nEpoch:  11. Loss: 1.1213. Acc.: 36.48%\nEpoch:  12. Loss: 1.1060. Acc.: 35.96%\nEpoch:  13. Loss: 1.0919. Acc.: 37.93%\nEpoch 13 best model saved with accuracy: 37.93%\nEpoch:  14. Loss: 1.0746. Acc.: 38.06%\nEpoch 14 best model saved with accuracy: 38.06%\nEpoch:  15. Loss: 1.0766. Acc.: 39.11%\nEpoch 15 best model saved with accuracy: 39.11%\nEpoch:  16. Loss: 1.0548. Acc.: 42.78%\nEpoch:  16. Loss: 1.0548. Acc.: 42.78%\nEpoch 16 best model saved with accuracy: 42.78%\nEpoch:  17. Loss: 1.0472. Acc.: 43.18%\nEpoch 17 best model saved with accuracy: 43.18%\nEpoch:  18. Loss: 1.0368. Acc.: 43.96%\nEpoch 18 best model saved with accuracy: 43.96%\nEpoch:  19. Loss: 1.0251. Acc.: 42.26%\nEpoch:  20. Loss: 1.0247. Acc.: 44.23%\nEpoch 20 best model saved with accuracy: 44.23%\nEpoch:  21. Loss: 1.0192. Acc.: 40.55%\nEpoch:  22. Loss: 1.0101. Acc.: 43.44%\nEpoch:  23. Loss: 1.0029. Acc.: 45.14%\nEpoch 23 best model saved with accuracy: 45.14%\nEpoch:  24. Loss: 0.9898. Acc.: 44.62%\nEpoch:  25. Loss: 0.9915. Acc.: 43.57%\nEpoch:  26. Loss: 0.9874. Acc.: 46.19%\nEpoch 26 best model saved with accuracy: 46.19%\nEpoch:  27. Loss: 0.9815. Acc.: 43.44%\nEpoch:  28. Loss: 0.9850. Acc.: 46.72%\nEpoch 28 best model saved with accuracy: 46.72%\nEpoch:  29. Loss: 0.9843. Acc.: 43.44%\nEpoch:  30. Loss: 0.9744. Acc.: 43.18%\nEpoch:  31. Loss: 0.9827. Acc.: 43.83%\nEpoch:  32. Loss: 0.9660. Acc.: 45.41%\nEpoch:  32. Loss: 0.9660. Acc.: 45.41%\nEpoch:  33. Loss: 0.9677. Acc.: 42.65%\nEpoch:  34. Loss: 0.9641. Acc.: 44.88%\nEpoch:  35. Loss: 0.9523. Acc.: 46.85%\nEpoch 35 best model saved with accuracy: 46.85%\nEpoch:  36. Loss: 0.9492. Acc.: 46.46%\nEpoch:  37. Loss: 0.9526. Acc.: 47.11%\nEpoch 37 best model saved with accuracy: 47.11%\nEpoch:  38. Loss: 0.9496. Acc.: 45.28%\nEpoch:  39. Loss: 0.9381. Acc.: 45.01%\nEpoch:  40. Loss: 0.9475. Acc.: 47.77%\nEpoch 40 best model saved with accuracy: 47.77%\nEpoch:  41. Loss: 0.9312. Acc.: 45.67%\nEpoch:  42. Loss: 0.9291. Acc.: 47.24%\nEpoch:  43. Loss: 0.9303. Acc.: 45.28%\nEpoch:  44. Loss: 0.9246. Acc.: 45.28%\nEpoch:  45. Loss: 0.9221. Acc.: 47.11%\nEpoch:  46. Loss: 0.9384. Acc.: 45.01%\nEpoch:  47. Loss: 0.9332. Acc.: 48.56%\nEpoch 47 best model saved with accuracy: 48.56%\nEpoch:  48. Loss: 0.9281. Acc.: 45.28%\nEpoch:  49. Loss: 0.9203. Acc.: 45.67%\nEpoch:  50. Loss: 0.9155. Acc.: 46.72%\nEpoch:  51. Loss: 0.9188. Acc.: 45.28%\nEpoch:  52. Loss: 0.9125. Acc.: 48.56%\nEpoch:  53. Loss: 0.9083. Acc.: 48.43%\nEpoch:  54. Loss: 0.9196. Acc.: 47.38%\nEpoch:  55. Loss: 0.9183. Acc.: 48.43%\nEpoch:  56. Loss: 0.9109. Acc.: 48.43%\nEpoch:  57. Loss: 0.8978. Acc.: 47.38%\nEpoch:  58. Loss: 0.8948. Acc.: 48.43%\nEpoch:  59. Loss: 0.8944. Acc.: 46.85%\nEpoch:  60. Loss: 0.8996. Acc.: 47.38%\nEpoch:  61. Loss: 0.8940. Acc.: 48.69%\nEpoch 61 best model saved with accuracy: 48.69%\nEpoch:  62. Loss: 0.8875. Acc.: 47.51%\nEpoch:  63. Loss: 0.8846. Acc.: 48.03%\nEpoch:  64. Loss: 0.8842. Acc.: 49.87%\nEpoch:  64. Loss: 0.8842. Acc.: 49.87%\nEpoch 64 best model saved with accuracy: 49.87%\nEpoch:  65. Loss: 0.8813. Acc.: 48.29%\nEpoch:  66. Loss: 0.8793. Acc.: 49.08%\nEpoch:  67. Loss: 0.8992. Acc.: 48.82%\nEpoch:  68. Loss: 0.8727. Acc.: 48.82%\nEpoch:  69. Loss: 0.8701. Acc.: 46.98%\nEpoch:  70. Loss: 0.8797. Acc.: 50.00%\nEpoch 70 best model saved with accuracy: 50.00%\nEpoch:  71. Loss: 0.8795. Acc.: 48.82%\nEpoch:  72. Loss: 0.8728. Acc.: 51.18%\nEpoch 72 best model saved with accuracy: 51.18%\nEpoch:  73. Loss: 0.8750. Acc.: 48.03%\nEpoch:  74. Loss: 0.8785. Acc.: 50.26%\nEpoch:  75. Loss: 0.8592. Acc.: 50.13%\nEpoch:  76. Loss: 0.8729. Acc.: 49.21%\nEpoch:  77. Loss: 0.8666. Acc.: 49.61%\nEpoch:  78. Loss: 0.8549. Acc.: 50.13%\nEpoch:  79. Loss: 0.8644. Acc.: 50.52%\nEpoch:  80. Loss: 0.8540. Acc.: 49.34%\nEpoch:  81. Loss: 0.8384. Acc.: 51.57%\nEpoch 81 best model saved with accuracy: 51.57%\nEpoch:  82. Loss: 0.8454. Acc.: 50.92%\nEpoch:  83. Loss: 0.8601. Acc.: 51.31%\nEpoch:  84. Loss: 0.8583. Acc.: 51.57%\nEpoch:  85. Loss: 0.8423. Acc.: 51.44%\nEpoch:  86. Loss: 0.8371. Acc.: 51.84%\nEpoch 86 best model saved with accuracy: 51.84%\nEpoch:  87. Loss: 0.8422. Acc.: 51.57%\nEpoch:  88. Loss: 0.8274. Acc.: 52.62%\nEpoch 88 best model saved with accuracy: 52.62%\nEpoch:  89. Loss: 0.8421. Acc.: 52.23%\nEpoch:  90. Loss: 0.8366. Acc.: 52.76%\nEpoch 90 best model saved with accuracy: 52.76%\nEpoch:  91. Loss: 0.8467. Acc.: 52.23%\nEpoch:  92. Loss: 0.8269. Acc.: 51.97%\nEpoch:  93. Loss: 0.8304. Acc.: 52.36%\nEpoch:  94. Loss: 0.8155. Acc.: 53.54%\nEpoch 94 best model saved with accuracy: 53.54%\nEpoch:  95. Loss: 0.8197. Acc.: 53.28%\nEpoch:  96. Loss: 0.8063. Acc.: 55.64%\nEpoch 96 best model saved with accuracy: 55.64%\nEpoch:  97. Loss: 0.8120. Acc.: 54.20%\nEpoch:  98. Loss: 0.8031. Acc.: 56.17%\nEpoch 98 best model saved with accuracy: 56.17%\nEpoch:  99. Loss: 0.7951. Acc.: 57.09%\nEpoch 99 best model saved with accuracy: 57.09%\nEpoch: 100. Loss: 0.8068. Acc.: 57.22%\nEpoch 100 best model saved with accuracy: 57.22%\nEpoch: 101. Loss: 0.7936. Acc.: 58.53%\nEpoch 101 best model saved with accuracy: 58.53%\nEpoch: 102. Loss: 0.7779. Acc.: 59.45%\nEpoch 102 best model saved with accuracy: 59.45%\nEpoch: 103. Loss: 0.7881. Acc.: 59.58%\nEpoch 103 best model saved with accuracy: 59.58%\nEpoch: 104. Loss: 0.7748. Acc.: 58.66%\nEpoch: 105. Loss: 0.7725. Acc.: 59.06%\nEpoch: 106. Loss: 0.7605. Acc.: 58.79%\nEpoch: 107. Loss: 0.7701. Acc.: 61.02%\nEpoch 107 best model saved with accuracy: 61.02%\nEpoch: 108. Loss: 0.7709. Acc.: 60.10%\nEpoch: 109. Loss: 0.7460. Acc.: 61.42%\nEpoch 109 best model saved with accuracy: 61.42%\nEpoch: 110. Loss: 0.7401. Acc.: 60.76%\nEpoch: 111. Loss: 0.7415. Acc.: 63.65%\nEpoch 111 best model saved with accuracy: 63.65%\nEpoch: 112. Loss: 0.7523. Acc.: 61.15%\nEpoch: 113. Loss: 0.7343. Acc.: 65.75%\nEpoch 113 best model saved with accuracy: 65.75%\nEpoch: 114. Loss: 0.7342. Acc.: 64.17%\nEpoch: 115. Loss: 0.7322. Acc.: 63.12%\nEpoch: 116. Loss: 0.7306. Acc.: 65.22%\nEpoch: 117. Loss: 0.7453. Acc.: 63.65%\nEpoch: 118. Loss: 0.7372. Acc.: 64.17%\nEpoch: 119. Loss: 0.7190. Acc.: 64.30%\nEpoch: 120. Loss: 0.7156. Acc.: 63.39%\nEpoch: 121. Loss: 0.7156. Acc.: 64.83%\nEpoch: 122. Loss: 0.7290. Acc.: 63.78%\nEpoch: 123. Loss: 0.7136. Acc.: 64.96%\nEpoch: 124. Loss: 0.6963. Acc.: 66.54%\nEpoch 124 best model saved with accuracy: 66.54%\nEpoch: 125. Loss: 0.7026. Acc.: 64.17%\nEpoch: 126. Loss: 0.7038. Acc.: 67.45%\nEpoch 126 best model saved with accuracy: 67.45%\nEpoch: 127. Loss: 0.6836. Acc.: 66.54%\nEpoch: 128. Loss: 0.7120. Acc.: 66.40%\nEpoch: 128. Loss: 0.7120. Acc.: 66.40%\nEpoch: 129. Loss: 0.6911. Acc.: 67.72%\nEpoch 129 best model saved with accuracy: 67.72%\nEpoch: 130. Loss: 0.6911. Acc.: 65.22%\nEpoch: 131. Loss: 0.6723. Acc.: 67.59%\nEpoch: 132. Loss: 0.6736. Acc.: 63.78%\nEpoch: 133. Loss: 0.6647. Acc.: 67.19%\nEpoch: 134. Loss: 0.6654. Acc.: 67.19%\nEpoch: 135. Loss: 0.6856. Acc.: 67.32%\nEpoch: 136. Loss: 0.6849. Acc.: 66.14%\nEpoch: 137. Loss: 0.6732. Acc.: 67.85%\nEpoch 137 best model saved with accuracy: 67.85%\nEpoch: 138. Loss: 0.6634. Acc.: 66.54%\nEpoch: 139. Loss: 0.6575. Acc.: 66.40%\nEpoch: 140. Loss: 0.6635. Acc.: 66.54%\nEpoch: 141. Loss: 0.6688. Acc.: 65.62%\nEpoch: 142. Loss: 0.6796. Acc.: 66.01%\nEpoch: 143. Loss: 0.6618. Acc.: 67.59%\nEpoch: 144. Loss: 0.6756. Acc.: 65.49%\nEpoch: 145. Loss: 0.6503. Acc.: 66.93%\nEpoch: 146. Loss: 0.6415. Acc.: 66.93%\nEpoch: 147. Loss: 0.6579. Acc.: 67.32%\nEpoch: 148. Loss: 0.6420. Acc.: 65.75%\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 149. Loss: 0.6620. Acc.: 66.67%\nEpoch: 150. Loss: 0.6459. Acc.: 66.40%\nEpoch: 151. Loss: 0.6477. Acc.: 65.88%\nEpoch: 152. Loss: 0.6545. Acc.: 67.32%\nEpoch: 153. Loss: 0.6451. Acc.: 65.62%\nEpoch: 154. Loss: 0.6469. Acc.: 66.14%\nEpoch: 155. Loss: 0.6471. Acc.: 66.93%\nEpoch: 156. Loss: 0.6397. Acc.: 67.85%\nEpoch: 157. Loss: 0.6379. Acc.: 67.32%\nEpoch: 158. Loss: 0.6316. Acc.: 66.27%\nEpoch: 159. Loss: 0.6251. Acc.: 67.19%\nEpoch: 160. Loss: 0.6376. Acc.: 67.59%\nEpoch: 161. Loss: 0.6166. Acc.: 68.24%\nEpoch 161 best model saved with accuracy: 68.24%\nEpoch: 162. Loss: 0.6269. Acc.: 66.80%\nEpoch: 163. Loss: 0.6100. Acc.: 66.80%\nEpoch: 164. Loss: 0.6143. Acc.: 68.11%\nEpoch: 165. Loss: 0.6191. Acc.: 66.40%\nEpoch: 166. Loss: 0.6173. Acc.: 65.09%\nEpoch: 167. Loss: 0.6094. Acc.: 66.80%\nEpoch: 168. Loss: 0.6376. Acc.: 65.88%\nEpoch: 169. Loss: 0.6033. Acc.: 66.54%\nEpoch: 170. Loss: 0.6097. Acc.: 68.50%\nEpoch 170 best model saved with accuracy: 68.50%\nEpoch: 171. Loss: 0.6077. Acc.: 65.75%\nEpoch: 172. Loss: 0.6391. Acc.: 67.59%\nEpoch: 173. Loss: 0.6051. Acc.: 66.54%\nEpoch: 174. Loss: 0.6030. Acc.: 68.37%\nEpoch: 175. Loss: 0.6200. Acc.: 66.80%\nEpoch: 176. Loss: 0.6162. Acc.: 67.45%\nEpoch: 177. Loss: 0.6029. Acc.: 67.72%\nEpoch: 178. Loss: 0.5980. Acc.: 67.59%\nEpoch: 179. Loss: 0.6077. Acc.: 67.32%\nEpoch: 180. Loss: 0.5974. Acc.: 67.72%\nEpoch: 181. Loss: 0.6013. Acc.: 66.93%\nEpoch: 182. Loss: 0.6122. Acc.: 66.54%\nEpoch: 183. Loss: 0.5905. Acc.: 66.67%\nEpoch: 184. Loss: 0.6023. Acc.: 67.19%\nEpoch: 185. Loss: 0.6059. Acc.: 67.32%\nEpoch: 186. Loss: 0.6087. Acc.: 66.40%\nEpoch: 187. Loss: 0.5956. Acc.: 67.72%\nEpoch: 188. Loss: 0.5926. Acc.: 67.85%\nEpoch: 189. Loss: 0.5880. Acc.: 67.59%\nEpoch: 190. Loss: 0.6081. Acc.: 66.93%\nEpoch: 191. Loss: 0.5962. Acc.: 67.85%\nEpoch: 192. Loss: 0.5948. Acc.: 68.37%\nEpoch: 193. Loss: 0.6005. Acc.: 67.59%\nEpoch: 194. Loss: 0.5789. Acc.: 69.16%\nEpoch 194 best model saved with accuracy: 69.16%\nEpoch: 195. Loss: 0.5848. Acc.: 67.72%\nEpoch: 196. Loss: 0.6003. Acc.: 69.03%\nEpoch: 197. Loss: 0.5884. Acc.: 66.80%\nEpoch: 198. Loss: 0.5849. Acc.: 69.29%\nEpoch 198 best model saved with accuracy: 69.29%\nEpoch: 199. Loss: 0.5935. Acc.: 67.45%\nEpoch: 200. Loss: 0.5804. Acc.: 66.67%\nEpoch: 201. Loss: 0.5870. Acc.: 68.24%\nEpoch: 202. Loss: 0.5750. Acc.: 67.72%\nEpoch: 203. Loss: 0.6202. Acc.: 68.24%\nEpoch: 204. Loss: 0.5926. Acc.: 67.59%\nEpoch: 205. Loss: 0.5818. Acc.: 67.85%\nEpoch: 206. Loss: 0.5903. Acc.: 67.85%\nEpoch: 207. Loss: 0.5913. Acc.: 66.67%\nEpoch: 208. Loss: 0.5820. Acc.: 68.37%\nEpoch: 209. Loss: 0.5741. Acc.: 67.98%\nEpoch: 210. Loss: 0.5974. Acc.: 67.85%\nEpoch: 211. Loss: 0.5929. Acc.: 68.24%\nEpoch: 212. Loss: 0.5704. Acc.: 68.11%\nEpoch: 213. Loss: 0.5824. Acc.: 67.32%\nEpoch: 214. Loss: 0.5866. Acc.: 69.82%\nEpoch 214 best model saved with accuracy: 69.82%\nEpoch: 215. Loss: 0.5835. Acc.: 68.11%\nEpoch: 216. Loss: 0.5910. Acc.: 69.82%\nEpoch: 217. Loss: 0.5792. Acc.: 69.03%\nEpoch: 218. Loss: 0.5754. Acc.: 68.24%\nEpoch: 219. Loss: 0.5778. Acc.: 67.32%\nEpoch: 220. Loss: 0.5746. Acc.: 68.90%\nEpoch: 221. Loss: 0.5676. Acc.: 68.64%\nEpoch: 222. Loss: 0.5512. Acc.: 68.11%\nEpoch: 223. Loss: 0.5769. Acc.: 69.16%\nEpoch: 224. Loss: 0.5703. Acc.: 68.64%\nEpoch: 225. Loss: 0.5844. Acc.: 68.24%\nEpoch: 226. Loss: 0.5798. Acc.: 69.69%\nEpoch: 227. Loss: 0.5756. Acc.: 69.42%\nEpoch: 228. Loss: 0.5803. Acc.: 67.85%\nEpoch: 229. Loss: 0.5643. Acc.: 69.03%\nEpoch: 230. Loss: 0.5523. Acc.: 69.42%\nEpoch: 231. Loss: 0.5824. Acc.: 67.85%\nEpoch: 232. Loss: 0.5824. Acc.: 68.50%\nEpoch: 233. Loss: 0.5687. Acc.: 69.16%\nEpoch: 234. Loss: 0.5549. Acc.: 68.24%\nEpoch: 235. Loss: 0.5644. Acc.: 68.90%\nEpoch: 236. Loss: 0.5566. Acc.: 69.82%\nEpoch: 237. Loss: 0.5715. Acc.: 69.03%\nEpoch: 238. Loss: 0.5493. Acc.: 69.16%\nEpoch: 239. Loss: 0.5585. Acc.: 68.37%\nEpoch: 240. Loss: 0.5459. Acc.: 69.03%\nEpoch: 241. Loss: 0.5538. Acc.: 69.82%\nEpoch: 242. Loss: 0.5735. Acc.: 69.29%\nEpoch: 243. Loss: 0.5570. Acc.: 69.82%\nEpoch: 244. Loss: 0.5696. Acc.: 67.59%\nEpoch: 245. Loss: 0.5567. Acc.: 70.60%\nEpoch 245 best model saved with accuracy: 70.60%\nEpoch: 246. Loss: 0.5417. Acc.: 68.64%\nEpoch: 247. Loss: 0.5570. Acc.: 69.55%\nEpoch: 248. Loss: 0.5590. Acc.: 69.03%\nEpoch: 249. Loss: 0.5564. Acc.: 69.42%\nEpoch: 250. Loss: 0.5604. Acc.: 69.55%\nEpoch: 251. Loss: 0.5648. Acc.: 68.37%\nEpoch: 252. Loss: 0.5688. Acc.: 69.03%\nEpoch: 253. Loss: 0.5581. Acc.: 68.37%\nEpoch: 254. Loss: 0.5407. Acc.: 68.64%\nEpoch: 255. Loss: 0.5422. Acc.: 69.95%\nEpoch: 256. Loss: 0.5555. Acc.: 68.37%\nEpoch: 256. Loss: 0.5555. Acc.: 68.37%\nEpoch: 257. Loss: 0.5566. Acc.: 70.21%\nEpoch: 258. Loss: 0.5461. Acc.: 68.77%\nEpoch: 259. Loss: 0.5446. Acc.: 69.69%\nEpoch: 260. Loss: 0.5415. Acc.: 70.47%\nEpoch: 261. Loss: 0.5632. Acc.: 69.82%\nEpoch: 262. Loss: 0.5585. Acc.: 68.90%\nEpoch: 263. Loss: 0.5533. Acc.: 69.69%\nEpoch: 264. Loss: 0.5428. Acc.: 69.16%\nEpoch: 265. Loss: 0.5507. Acc.: 69.69%\nEpoch: 266. Loss: 0.5451. Acc.: 68.90%\nEpoch: 267. Loss: 0.5479. Acc.: 68.50%\nEpoch: 268. Loss: 0.5541. Acc.: 68.90%\nEpoch: 269. Loss: 0.5484. Acc.: 68.90%\nEpoch: 270. Loss: 0.5477. Acc.: 69.95%\nEpoch: 271. Loss: 0.5490. Acc.: 70.08%\nEpoch: 272. Loss: 0.5475. Acc.: 70.47%\nEpoch: 273. Loss: 0.5465. Acc.: 72.18%\nEpoch 273 best model saved with accuracy: 72.18%\nEpoch: 274. Loss: 0.5300. Acc.: 69.42%\nEpoch: 275. Loss: 0.5300. Acc.: 70.34%\nEpoch: 276. Loss: 0.5457. Acc.: 69.29%\nEpoch: 277. Loss: 0.5490. Acc.: 69.29%\nEpoch: 278. Loss: 0.5406. Acc.: 71.13%\nEpoch: 279. Loss: 0.5388. Acc.: 70.08%\nEpoch: 280. Loss: 0.5308. Acc.: 70.47%\nEpoch: 281. Loss: 0.5342. Acc.: 71.13%\nEpoch: 282. Loss: 0.5504. Acc.: 69.29%\nEpoch: 283. Loss: 0.5344. Acc.: 71.13%\nEpoch: 284. Loss: 0.5220. Acc.: 70.34%\nEpoch: 285. Loss: 0.5447. Acc.: 70.34%\nEpoch: 286. Loss: 0.5532. Acc.: 71.52%\nEpoch: 287. Loss: 0.5451. Acc.: 69.29%\nEpoch: 288. Loss: 0.5449. Acc.: 69.82%\nEpoch: 289. Loss: 0.5490. Acc.: 69.55%\nEpoch: 290. Loss: 0.5293. Acc.: 69.82%\nEpoch: 291. Loss: 0.5304. Acc.: 69.82%\nEpoch: 292. Loss: 0.5354. Acc.: 69.29%\nEpoch: 293. Loss: 0.5632. Acc.: 70.34%\nEpoch: 294. Loss: 0.5283. Acc.: 69.29%\nEpoch: 295. Loss: 0.5408. Acc.: 69.29%\nEpoch: 296. Loss: 0.5346. Acc.: 69.82%\nEpoch: 297. Loss: 0.5279. Acc.: 69.42%\nEpoch: 298. Loss: 0.5292. Acc.: 69.29%\nEpoch: 299. Loss: 0.5317. Acc.: 70.87%\nEpoch: 300. Loss: 0.5358. Acc.: 69.55%\nEpoch: 301. Loss: 0.5500. Acc.: 71.00%\nEpoch: 302. Loss: 0.5399. Acc.: 70.21%\nEpoch: 303. Loss: 0.5481. Acc.: 70.08%\nEpoch: 304. Loss: 0.5376. Acc.: 70.08%\nEpoch: 305. Loss: 0.5420. Acc.: 69.03%\nEpoch: 306. Loss: 0.5165. Acc.: 69.95%\nEpoch: 307. Loss: 0.5345. Acc.: 67.72%\nEpoch: 308. Loss: 0.5246. Acc.: 71.00%\nEpoch: 309. Loss: 0.5170. Acc.: 70.60%\nEpoch: 310. Loss: 0.5352. Acc.: 71.78%\nEpoch: 311. Loss: 0.5385. Acc.: 71.13%\nEpoch: 312. Loss: 0.5441. Acc.: 71.78%\nEpoch: 313. Loss: 0.5501. Acc.: 69.69%\nEpoch: 314. Loss: 0.5375. Acc.: 70.34%\nEpoch: 315. Loss: 0.5273. Acc.: 70.08%\nEpoch: 316. Loss: 0.5258. Acc.: 69.82%\nEpoch: 317. Loss: 0.5141. Acc.: 70.34%\nEpoch: 318. Loss: 0.5112. Acc.: 72.05%\nEpoch: 319. Loss: 0.5231. Acc.: 70.47%\nEpoch: 320. Loss: 0.5370. Acc.: 72.70%\nEpoch 320 best model saved with accuracy: 72.70%\nEpoch: 321. Loss: 0.5255. Acc.: 69.69%\nEpoch: 322. Loss: 0.5184. Acc.: 69.55%\nEpoch: 323. Loss: 0.5185. Acc.: 72.31%\nEpoch: 324. Loss: 0.5026. Acc.: 72.44%\nEpoch: 325. Loss: 0.5278. Acc.: 71.13%\nEpoch: 326. Loss: 0.5514. Acc.: 70.60%\nEpoch: 327. Loss: 0.5247. Acc.: 72.31%\nEpoch: 328. Loss: 0.5200. Acc.: 72.70%\nEpoch: 329. Loss: 0.5275. Acc.: 72.18%\nEpoch: 330. Loss: 0.5004. Acc.: 70.60%\nEpoch: 331. Loss: 0.5128. Acc.: 72.70%\nEpoch: 332. Loss: 0.5188. Acc.: 72.57%\nEpoch: 333. Loss: 0.5140. Acc.: 71.78%\nEpoch: 334. Loss: 0.4968. Acc.: 71.39%\nEpoch: 335. Loss: 0.5027. Acc.: 71.26%\nEpoch: 336. Loss: 0.5223. Acc.: 74.02%\nEpoch 336 best model saved with accuracy: 74.02%\nEpoch: 337. Loss: 0.5364. Acc.: 71.92%\nEpoch: 338. Loss: 0.5372. Acc.: 73.49%\nEpoch: 339. Loss: 0.5297. Acc.: 70.60%\nEpoch: 340. Loss: 0.5160. Acc.: 71.78%\nEpoch: 341. Loss: 0.5216. Acc.: 72.18%\nEpoch: 342. Loss: 0.5133. Acc.: 72.31%\nEpoch: 343. Loss: 0.5170. Acc.: 72.05%\nEpoch: 344. Loss: 0.5374. Acc.: 73.75%\nEpoch: 345. Loss: 0.5265. Acc.: 72.05%\nEpoch: 346. Loss: 0.5149. Acc.: 70.87%\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 347. Loss: 0.5182. Acc.: 69.69%\nEpoch: 348. Loss: 0.5033. Acc.: 70.60%\nEpoch: 349. Loss: 0.5142. Acc.: 71.26%\nEpoch: 350. Loss: 0.5258. Acc.: 72.18%\nEpoch: 351. Loss: 0.5243. Acc.: 73.23%\nEpoch: 352. Loss: 0.5164. Acc.: 71.78%\nEpoch: 353. Loss: 0.5109. Acc.: 71.65%\nEpoch: 354. Loss: 0.5146. Acc.: 71.39%\nEpoch: 355. Loss: 0.5109. Acc.: 71.92%\nEpoch: 356. Loss: 0.5051. Acc.: 72.18%\nEpoch: 357. Loss: 0.5199. Acc.: 72.31%\nEpoch: 358. Loss: 0.5193. Acc.: 70.47%\nEpoch: 359. Loss: 0.5057. Acc.: 73.36%\nEpoch: 360. Loss: 0.5021. Acc.: 72.05%\nEpoch: 361. Loss: 0.5082. Acc.: 72.05%\nEpoch: 362. Loss: 0.5219. Acc.: 71.65%\nEpoch: 363. Loss: 0.5116. Acc.: 72.70%\nEpoch: 364. Loss: 0.5113. Acc.: 72.31%\nEpoch: 365. Loss: 0.5181. Acc.: 72.83%\nEpoch: 366. Loss: 0.5084. Acc.: 73.62%\nEpoch: 367. Loss: 0.5167. Acc.: 70.87%\nEpoch: 368. Loss: 0.5221. Acc.: 74.80%\nEpoch 368 best model saved with accuracy: 74.80%\nEpoch: 369. Loss: 0.4995. Acc.: 72.05%\nEpoch: 370. Loss: 0.5243. Acc.: 72.05%\nEpoch: 371. Loss: 0.5059. Acc.: 72.70%\nEpoch: 372. Loss: 0.5015. Acc.: 71.52%\nEpoch: 373. Loss: 0.5156. Acc.: 73.23%\nEpoch: 374. Loss: 0.5065. Acc.: 72.97%\nEpoch: 375. Loss: 0.5073. Acc.: 70.47%\nEpoch: 376. Loss: 0.5039. Acc.: 72.83%\nEpoch: 377. Loss: 0.5146. Acc.: 72.70%\nEpoch: 378. Loss: 0.5163. Acc.: 71.65%\nEpoch: 379. Loss: 0.5088. Acc.: 73.10%\nEpoch: 380. Loss: 0.5064. Acc.: 72.97%\nEpoch: 381. Loss: 0.5245. Acc.: 72.31%\nEpoch: 382. Loss: 0.5055. Acc.: 74.41%\nEpoch: 383. Loss: 0.5031. Acc.: 73.10%\nEpoch: 384. Loss: 0.5021. Acc.: 71.65%\nEpoch: 385. Loss: 0.4886. Acc.: 70.47%\nEpoch: 386. Loss: 0.5135. Acc.: 73.36%\nEpoch: 387. Loss: 0.4953. Acc.: 71.65%\nEpoch: 388. Loss: 0.5260. Acc.: 72.70%\nEpoch: 389. Loss: 0.5027. Acc.: 72.97%\nEpoch: 390. Loss: 0.5181. Acc.: 72.05%\nEpoch: 391. Loss: 0.4964. Acc.: 73.88%\nEpoch: 392. Loss: 0.5010. Acc.: 71.65%\nEpoch: 393. Loss: 0.4965. Acc.: 72.31%\nEpoch: 394. Loss: 0.4995. Acc.: 71.39%\nEpoch: 395. Loss: 0.5133. Acc.: 72.97%\nEpoch: 396. Loss: 0.5151. Acc.: 71.92%\nEpoch: 397. Loss: 0.5191. Acc.: 71.92%\nEpoch: 398. Loss: 0.5005. Acc.: 73.36%\nEpoch: 399. Loss: 0.4993. Acc.: 74.02%\nEpoch: 400. Loss: 0.5084. Acc.: 72.97%\nEpoch: 401. Loss: 0.5098. Acc.: 73.62%\nEpoch: 402. Loss: 0.4856. Acc.: 74.02%\nEpoch: 403. Loss: 0.5002. Acc.: 73.49%\nEpoch: 404. Loss: 0.4892. Acc.: 74.15%\nEpoch: 405. Loss: 0.4870. Acc.: 72.83%\nEpoch: 406. Loss: 0.4713. Acc.: 75.07%\nEpoch 406 best model saved with accuracy: 75.07%\nEpoch: 407. Loss: 0.4946. Acc.: 73.62%\nEpoch: 408. Loss: 0.5036. Acc.: 73.62%\nEpoch: 409. Loss: 0.4707. Acc.: 71.92%\nEpoch: 410. Loss: 0.4890. Acc.: 73.49%\nEpoch: 411. Loss: 0.4899. Acc.: 74.41%\nEpoch: 412. Loss: 0.4868. Acc.: 71.39%\nEpoch: 413. Loss: 0.5048. Acc.: 73.36%\nEpoch: 414. Loss: 0.4788. Acc.: 73.75%\nEpoch: 415. Loss: 0.4786. Acc.: 72.05%\nEpoch: 416. Loss: 0.5069. Acc.: 72.18%\nEpoch: 417. Loss: 0.4928. Acc.: 72.44%\nEpoch: 418. Loss: 0.4851. Acc.: 72.57%\nEpoch: 419. Loss: 0.5212. Acc.: 73.62%\nEpoch: 420. Loss: 0.4976. Acc.: 74.02%\nEpoch: 421. Loss: 0.4812. Acc.: 74.54%\nEpoch: 422. Loss: 0.4808. Acc.: 73.36%\nEpoch: 423. Loss: 0.4947. Acc.: 73.23%\nEpoch: 424. Loss: 0.4949. Acc.: 72.70%\nEpoch: 425. Loss: 0.5316. Acc.: 73.62%\nEpoch: 426. Loss: 0.4965. Acc.: 72.83%\nEpoch: 427. Loss: 0.4938. Acc.: 73.88%\nEpoch: 428. Loss: 0.4830. Acc.: 73.23%\nEpoch: 429. Loss: 0.4961. Acc.: 73.49%\nEpoch: 430. Loss: 0.5133. Acc.: 74.02%\nEpoch: 431. Loss: 0.4898. Acc.: 72.57%\nEpoch: 432. Loss: 0.4762. Acc.: 74.28%\nEpoch: 433. Loss: 0.4805. Acc.: 72.97%\nEpoch: 434. Loss: 0.4888. Acc.: 72.70%\nEpoch: 435. Loss: 0.4949. Acc.: 74.15%\nEpoch: 436. Loss: 0.4875. Acc.: 74.41%\nEpoch: 437. Loss: 0.4927. Acc.: 71.39%\nEpoch: 438. Loss: 0.4972. Acc.: 74.15%\nEpoch: 439. Loss: 0.4937. Acc.: 72.70%\nEpoch: 440. Loss: 0.5000. Acc.: 72.57%\nEpoch: 441. Loss: 0.4666. Acc.: 72.57%\nEpoch: 442. Loss: 0.4935. Acc.: 72.83%\nEpoch: 443. Loss: 0.4877. Acc.: 72.57%\nEpoch: 444. Loss: 0.4674. Acc.: 76.12%\nEpoch 444 best model saved with accuracy: 76.12%\nEpoch: 445. Loss: 0.5025. Acc.: 72.97%\nEpoch: 446. Loss: 0.4959. Acc.: 72.70%\nEpoch: 447. Loss: 0.4982. Acc.: 73.23%\nEpoch: 448. Loss: 0.4795. Acc.: 72.31%\nEpoch: 449. Loss: 0.4805. Acc.: 74.15%\nEpoch: 450. Loss: 0.4755. Acc.: 72.83%\nEpoch: 451. Loss: 0.4656. Acc.: 72.44%\nEpoch: 452. Loss: 0.4873. Acc.: 72.70%\nEpoch: 453. Loss: 0.4829. Acc.: 73.49%\nEpoch: 454. Loss: 0.4872. Acc.: 74.02%\nEpoch: 455. Loss: 0.4904. Acc.: 74.54%\nEpoch: 456. Loss: 0.4627. Acc.: 72.97%\nEpoch: 457. Loss: 0.4781. Acc.: 73.49%\nEpoch: 458. Loss: 0.4736. Acc.: 73.23%\nEpoch: 459. Loss: 0.4607. Acc.: 72.70%\nEpoch: 460. Loss: 0.4910. Acc.: 73.36%\nEpoch: 461. Loss: 0.4925. Acc.: 73.75%\nEpoch: 462. Loss: 0.5197. Acc.: 74.54%\nEpoch: 463. Loss: 0.4689. Acc.: 73.88%\nEpoch: 464. Loss: 0.4875. Acc.: 74.93%\nEpoch: 465. Loss: 0.4903. Acc.: 72.83%\nEpoch: 466. Loss: 0.4796. Acc.: 73.88%\nEpoch: 467. Loss: 0.4786. Acc.: 71.78%\nEpoch: 468. Loss: 0.5113. Acc.: 73.23%\nEpoch: 469. Loss: 0.4924. Acc.: 72.97%\nEpoch: 470. Loss: 0.4674. Acc.: 74.15%\nEpoch: 471. Loss: 0.4755. Acc.: 73.23%\nEpoch: 472. Loss: 0.4750. Acc.: 73.36%\nEpoch: 473. Loss: 0.4914. Acc.: 76.25%\nEpoch 473 best model saved with accuracy: 76.25%\nEpoch: 474. Loss: 0.4844. Acc.: 73.62%\nEpoch: 475. Loss: 0.4818. Acc.: 72.83%\nEpoch: 476. Loss: 0.4940. Acc.: 75.20%\nEpoch: 477. Loss: 0.4720. Acc.: 73.10%\nEpoch: 478. Loss: 0.4701. Acc.: 73.49%\nEpoch: 479. Loss: 0.4708. Acc.: 73.10%\nEpoch: 480. Loss: 0.5006. Acc.: 72.83%\nEpoch: 481. Loss: 0.4569. Acc.: 73.10%\nEpoch: 482. Loss: 0.4809. Acc.: 73.75%\nEpoch: 483. Loss: 0.4882. Acc.: 74.02%\nEpoch: 484. Loss: 0.4809. Acc.: 72.57%\nEpoch: 485. Loss: 0.4800. Acc.: 73.62%\nEpoch: 486. Loss: 0.4660. Acc.: 72.31%\nEpoch: 487. Loss: 0.4721. Acc.: 73.88%\nEpoch: 488. Loss: 0.4673. Acc.: 73.10%\nEpoch: 489. Loss: 0.4812. Acc.: 74.80%\nEpoch: 490. Loss: 0.4885. Acc.: 73.10%\nEpoch: 491. Loss: 0.4915. Acc.: 74.02%\nEpoch: 492. Loss: 0.4726. Acc.: 71.78%\nEpoch: 493. Loss: 0.4822. Acc.: 73.62%\nEpoch: 494. Loss: 0.4977. Acc.: 75.72%\nEpoch: 495. Loss: 0.4922. Acc.: 73.49%\nEpoch: 496. Loss: 0.4681. Acc.: 72.97%\nEpoch: 497. Loss: 0.4849. Acc.: 72.05%\nEpoch: 498. Loss: 0.4973. Acc.: 72.05%\nEpoch: 499. Loss: 0.4715. Acc.: 70.60%\nEpoch: 500. Loss: 0.4790. Acc.: 70.08%\nEpoch: 501. Loss: 0.4729. Acc.: 74.67%\nEpoch: 502. Loss: 0.4623. Acc.: 74.15%\nEpoch: 503. Loss: 0.4701. Acc.: 73.49%\nEpoch: 504. Loss: 0.4707. Acc.: 72.97%\nEpoch: 505. Loss: 0.4695. Acc.: 73.75%\nEpoch: 506. Loss: 0.4713. Acc.: 72.97%\nEpoch: 507. Loss: 0.4679. Acc.: 72.97%\nEpoch: 508. Loss: 0.4724. Acc.: 72.70%\nEpoch: 509. Loss: 0.4702. Acc.: 73.62%\nEpoch: 510. Loss: 0.4765. Acc.: 72.97%\nEpoch: 511. Loss: 0.4789. Acc.: 72.97%\nEpoch: 512. Loss: 0.4713. Acc.: 72.83%\nEpoch: 512. Loss: 0.4713. Acc.: 72.83%\nEpoch: 513. Loss: 0.4775. Acc.: 72.70%\nEpoch: 514. Loss: 0.4718. Acc.: 73.10%\nEpoch: 515. Loss: 0.4804. Acc.: 72.83%\nEpoch: 516. Loss: 0.4717. Acc.: 73.62%\nEpoch: 517. Loss: 0.4599. Acc.: 73.62%\nEpoch: 518. Loss: 0.4763. Acc.: 73.10%\nEpoch: 519. Loss: 0.4699. Acc.: 73.10%\nEpoch: 520. Loss: 0.4672. Acc.: 72.44%\nEpoch: 521. Loss: 0.4825. Acc.: 75.07%\nEpoch: 522. Loss: 0.4696. Acc.: 72.97%\nEpoch: 523. Loss: 0.4683. Acc.: 75.07%\nEpoch: 524. Loss: 0.4555. Acc.: 72.31%\nEpoch: 525. Loss: 0.4591. Acc.: 74.28%\nEpoch: 526. Loss: 0.4537. Acc.: 72.05%\nEpoch: 527. Loss: 0.4585. Acc.: 75.20%\nEpoch: 528. Loss: 0.4768. Acc.: 74.15%\nEpoch: 529. Loss: 0.4591. Acc.: 74.28%\nEpoch: 530. Loss: 0.4635. Acc.: 74.67%\nEpoch: 531. Loss: 0.4811. Acc.: 73.62%\nEpoch: 532. Loss: 0.4671. Acc.: 75.46%\nEpoch: 533. Loss: 0.4710. Acc.: 73.62%\nEpoch: 534. Loss: 0.4573. Acc.: 74.15%\nEpoch: 535. Loss: 0.4769. Acc.: 74.54%\nEpoch: 536. Loss: 0.4879. Acc.: 74.28%\nEpoch: 537. Loss: 0.4663. Acc.: 74.02%\nEpoch: 538. Loss: 0.4636. Acc.: 74.02%\nEpoch: 539. Loss: 0.4792. Acc.: 73.23%\nEpoch: 540. Loss: 0.5024. Acc.: 74.02%\nEpoch: 541. Loss: 0.4579. Acc.: 74.54%\nEpoch: 542. Loss: 0.4468. Acc.: 73.62%\nEpoch: 543. Loss: 0.4570. Acc.: 76.25%\nEpoch: 544. Loss: 0.4625. Acc.: 74.15%\nEpoch: 545. Loss: 0.4405. Acc.: 74.28%\nEpoch: 546. Loss: 0.4552. Acc.: 74.67%\nEpoch: 547. Loss: 0.4585. Acc.: 75.33%\nEpoch: 548. Loss: 0.4735. Acc.: 73.49%\nEpoch: 549. Loss: 0.4784. Acc.: 74.54%\nEpoch: 550. Loss: 0.4491. Acc.: 73.88%\nEpoch: 551. Loss: 0.4683. Acc.: 72.57%\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 552. Loss: 0.4723. Acc.: 73.62%\nEpoch: 553. Loss: 0.4748. Acc.: 72.97%\nEpoch: 554. Loss: 0.4563. Acc.: 72.57%\nEpoch: 555. Loss: 0.4544. Acc.: 73.36%\nEpoch: 556. Loss: 0.4733. Acc.: 73.75%\nEpoch: 557. Loss: 0.4826. Acc.: 73.23%\nEpoch: 558. Loss: 0.4565. Acc.: 72.31%\nEpoch: 559. Loss: 0.4668. Acc.: 73.36%\nEpoch: 560. Loss: 0.4641. Acc.: 71.78%\nEpoch: 561. Loss: 0.4825. Acc.: 72.83%\nEpoch: 562. Loss: 0.4538. Acc.: 74.28%\nEpoch: 563. Loss: 0.4489. Acc.: 76.25%\nEpoch: 564. Loss: 0.4609. Acc.: 74.28%\nEpoch: 565. Loss: 0.4577. Acc.: 74.28%\nEpoch: 566. Loss: 0.4487. Acc.: 75.98%\nEpoch: 567. Loss: 0.4552. Acc.: 73.75%\nEpoch: 568. Loss: 0.4638. Acc.: 73.23%\nEpoch: 569. Loss: 0.4473. Acc.: 73.75%\nEpoch: 570. Loss: 0.4612. Acc.: 73.75%\nEpoch: 571. Loss: 0.4753. Acc.: 76.12%\nEpoch: 572. Loss: 0.4656. Acc.: 72.97%\nEpoch: 573. Loss: 0.4391. Acc.: 73.36%\nEpoch: 574. Loss: 0.4891. Acc.: 75.20%\nEpoch: 575. Loss: 0.4449. Acc.: 73.49%\nEpoch: 576. Loss: 0.4858. Acc.: 75.20%\nEpoch: 577. Loss: 0.4775. Acc.: 73.88%\nEpoch: 578. Loss: 0.4607. Acc.: 74.28%\nEpoch: 579. Loss: 0.4474. Acc.: 73.62%\nEpoch: 580. Loss: 0.4784. Acc.: 72.18%\nEpoch: 581. Loss: 0.4561. Acc.: 74.02%\nEpoch: 582. Loss: 0.4518. Acc.: 74.28%\nEpoch: 583. Loss: 0.4645. Acc.: 72.83%\nEpoch: 584. Loss: 0.4501. Acc.: 75.20%\nEpoch: 585. Loss: 0.4647. Acc.: 75.72%\nEpoch: 586. Loss: 0.4590. Acc.: 72.18%\nEpoch: 587. Loss: 0.4730. Acc.: 75.72%\nEpoch: 588. Loss: 0.4766. Acc.: 72.44%\nEpoch: 589. Loss: 0.4648. Acc.: 73.88%\nEpoch: 590. Loss: 0.4536. Acc.: 76.25%\nEpoch: 591. Loss: 0.4515. Acc.: 73.23%\nEpoch: 592. Loss: 0.4842. Acc.: 75.98%\nEpoch: 593. Loss: 0.4665. Acc.: 73.10%\nEpoch: 594. Loss: 0.4923. Acc.: 74.02%\nEpoch: 595. Loss: 0.4620. Acc.: 75.07%\nEpoch: 596. Loss: 0.4626. Acc.: 73.75%\nEpoch: 597. Loss: 0.4563. Acc.: 74.67%\nEpoch: 598. Loss: 0.4660. Acc.: 75.72%\nEpoch: 599. Loss: 0.4753. Acc.: 74.02%\nEpoch: 600. Loss: 0.4614. Acc.: 75.72%\nEpoch: 601. Loss: 0.4491. Acc.: 74.41%\nEpoch: 602. Loss: 0.4745. Acc.: 74.54%\nEpoch: 603. Loss: 0.4630. Acc.: 75.07%\nEpoch: 604. Loss: 0.4810. Acc.: 75.72%\nEpoch: 605. Loss: 0.4707. Acc.: 74.15%\nEpoch: 606. Loss: 0.4781. Acc.: 76.38%\nEpoch 606 best model saved with accuracy: 76.38%\nEpoch: 607. Loss: 0.4609. Acc.: 73.36%\nEpoch: 608. Loss: 0.4589. Acc.: 74.67%\nEpoch: 609. Loss: 0.4472. Acc.: 75.20%\nEpoch: 610. Loss: 0.4365. Acc.: 72.44%\nEpoch: 611. Loss: 0.4599. Acc.: 74.15%\nEpoch: 612. Loss: 0.4545. Acc.: 74.80%\nEpoch: 613. Loss: 0.4506. Acc.: 75.72%\nEpoch: 614. Loss: 0.4498. Acc.: 75.33%\nEpoch: 615. Loss: 0.4839. Acc.: 73.49%\nEpoch: 616. Loss: 0.4632. Acc.: 73.88%\nEpoch: 617. Loss: 0.4540. Acc.: 76.25%\nEpoch: 618. Loss: 0.4505. Acc.: 72.97%\nEpoch: 619. Loss: 0.4511. Acc.: 75.46%\nEpoch: 620. Loss: 0.4477. Acc.: 74.80%\nEpoch: 621. Loss: 0.4669. Acc.: 73.88%\nEpoch: 622. Loss: 0.4542. Acc.: 74.41%\nEpoch: 623. Loss: 0.4586. Acc.: 74.80%\nEpoch: 624. Loss: 0.4448. Acc.: 74.67%\nEpoch: 625. Loss: 0.4498. Acc.: 74.67%\nEpoch: 626. Loss: 0.4656. Acc.: 73.88%\nEpoch: 627. Loss: 0.4555. Acc.: 74.93%\nEpoch: 628. Loss: 0.4806. Acc.: 74.02%\nEpoch: 629. Loss: 0.4610. Acc.: 73.36%\nEpoch: 630. Loss: 0.4766. Acc.: 75.46%\nEpoch: 631. Loss: 0.4475. Acc.: 72.70%\nEpoch: 632. Loss: 0.4570. Acc.: 75.72%\nEpoch: 633. Loss: 0.4554. Acc.: 75.20%\nEpoch: 634. Loss: 0.4642. Acc.: 74.41%\nEpoch: 635. Loss: 0.4716. Acc.: 74.41%\nEpoch: 636. Loss: 0.4658. Acc.: 74.67%\nEpoch: 637. Loss: 0.4762. Acc.: 74.80%\nEpoch: 638. Loss: 0.4658. Acc.: 77.69%\nEpoch 638 best model saved with accuracy: 77.69%\nEpoch: 639. Loss: 0.4594. Acc.: 72.97%\nEpoch: 640. Loss: 0.4785. Acc.: 75.59%\nEpoch: 641. Loss: 0.4647. Acc.: 74.54%\nEpoch: 642. Loss: 0.4639. Acc.: 76.90%\nEpoch: 643. Loss: 0.4505. Acc.: 75.07%\nEpoch: 644. Loss: 0.4441. Acc.: 75.33%\nEpoch: 645. Loss: 0.4611. Acc.: 74.80%\nEpoch: 646. Loss: 0.4393. Acc.: 74.93%\nEpoch: 647. Loss: 0.4649. Acc.: 74.93%\nEpoch: 648. Loss: 0.4533. Acc.: 75.85%\nEpoch: 649. Loss: 0.4607. Acc.: 75.46%\nEpoch: 650. Loss: 0.4554. Acc.: 73.75%\nEpoch: 651. Loss: 0.4563. Acc.: 77.56%\nEpoch: 652. Loss: 0.4476. Acc.: 75.33%\nEpoch: 653. Loss: 0.4552. Acc.: 73.36%\nEpoch: 654. Loss: 0.4518. Acc.: 75.98%\nEpoch: 655. Loss: 0.4582. Acc.: 75.33%\nEpoch: 656. Loss: 0.4353. Acc.: 73.23%\nEpoch: 657. Loss: 0.4359. Acc.: 75.07%\nEpoch: 658. Loss: 0.4497. Acc.: 74.80%\nEpoch: 659. Loss: 0.4611. Acc.: 74.41%\nEpoch: 660. Loss: 0.4706. Acc.: 74.80%\nEpoch: 661. Loss: 0.4542. Acc.: 75.85%\nEpoch: 662. Loss: 0.4629. Acc.: 74.15%\nEpoch: 663. Loss: 0.4572. Acc.: 73.75%\nEpoch: 664. Loss: 0.4457. Acc.: 75.85%\nEpoch: 665. Loss: 0.4551. Acc.: 75.07%\nEpoch: 666. Loss: 0.4293. Acc.: 75.59%\nEpoch: 667. Loss: 0.4503. Acc.: 76.38%\nEpoch: 668. Loss: 0.4497. Acc.: 74.80%\nEpoch: 669. Loss: 0.4557. Acc.: 75.72%\nEpoch: 670. Loss: 0.4432. Acc.: 74.02%\nEpoch: 671. Loss: 0.4688. Acc.: 75.07%\nEpoch: 672. Loss: 0.4475. Acc.: 73.88%\nEpoch: 673. Loss: 0.4555. Acc.: 71.78%\nEpoch: 674. Loss: 0.4802. Acc.: 74.80%\nEpoch: 675. Loss: 0.4550. Acc.: 73.88%\nEpoch: 676. Loss: 0.4478. Acc.: 75.33%\nEpoch: 677. Loss: 0.4472. Acc.: 74.80%\nEpoch: 678. Loss: 0.4578. Acc.: 74.80%\nEpoch: 679. Loss: 0.4337. Acc.: 75.07%\nEpoch: 680. Loss: 0.4450. Acc.: 74.41%\nEpoch: 681. Loss: 0.4594. Acc.: 75.07%\nEpoch: 682. Loss: 0.4410. Acc.: 74.41%\nEpoch: 683. Loss: 0.4598. Acc.: 73.62%\nEpoch: 684. Loss: 0.4418. Acc.: 73.75%\nEpoch: 685. Loss: 0.4483. Acc.: 74.93%\nEpoch: 686. Loss: 0.4466. Acc.: 72.18%\nEpoch: 687. Loss: 0.4517. Acc.: 74.80%\nEpoch: 688. Loss: 0.4481. Acc.: 74.67%\nEpoch: 689. Loss: 0.4557. Acc.: 75.72%\nEpoch: 690. Loss: 0.4529. Acc.: 73.49%\nEpoch: 691. Loss: 0.4386. Acc.: 72.97%\nEpoch: 692. Loss: 0.4732. Acc.: 75.98%\nEpoch: 693. Loss: 0.4718. Acc.: 74.41%\nEpoch: 694. Loss: 0.4425. Acc.: 72.97%\nEpoch: 695. Loss: 0.4343. Acc.: 74.15%\nEpoch: 696. Loss: 0.4668. Acc.: 73.62%\nEpoch: 697. Loss: 0.4519. Acc.: 73.75%\nEpoch: 698. Loss: 0.4350. Acc.: 75.98%\nEpoch: 699. Loss: 0.4530. Acc.: 74.28%\nEpoch: 700. Loss: 0.4705. Acc.: 73.62%\nEpoch: 701. Loss: 0.4586. Acc.: 75.85%\nEpoch: 702. Loss: 0.4497. Acc.: 74.54%\nEpoch: 703. Loss: 0.4420. Acc.: 75.33%\nEpoch: 704. Loss: 0.4356. Acc.: 74.93%\nEpoch: 705. Loss: 0.4475. Acc.: 76.64%\nEpoch: 706. Loss: 0.4518. Acc.: 73.49%\nEpoch: 707. Loss: 0.4361. Acc.: 73.10%\nEpoch: 708. Loss: 0.4577. Acc.: 75.46%\nEpoch: 709. Loss: 0.4377. Acc.: 73.75%\nEpoch: 710. Loss: 0.4368. Acc.: 76.12%\nEpoch: 711. Loss: 0.4435. Acc.: 76.90%\nEpoch: 712. Loss: 0.4418. Acc.: 73.36%\nEpoch: 713. Loss: 0.4704. Acc.: 74.67%\nEpoch: 714. Loss: 0.4635. Acc.: 75.07%\nEpoch: 715. Loss: 0.4563. Acc.: 73.75%\nEpoch: 716. Loss: 0.4567. Acc.: 73.23%\nEpoch: 717. Loss: 0.4651. Acc.: 74.54%\nEpoch: 718. Loss: 0.4467. Acc.: 75.85%\nEpoch: 719. Loss: 0.4428. Acc.: 74.28%\nEpoch: 720. Loss: 0.4426. Acc.: 74.67%\nEpoch: 721. Loss: 0.4468. Acc.: 75.20%\nEpoch: 722. Loss: 0.4498. Acc.: 74.41%\nEpoch: 723. Loss: 0.4396. Acc.: 74.80%\nEpoch: 724. Loss: 0.4337. Acc.: 73.49%\nEpoch: 725. Loss: 0.4427. Acc.: 74.28%\nEpoch: 726. Loss: 0.4430. Acc.: 74.93%\nEpoch: 727. Loss: 0.4366. Acc.: 74.93%\nEpoch: 728. Loss: 0.4491. Acc.: 73.75%\nEpoch: 729. Loss: 0.4334. Acc.: 74.28%\nEpoch: 730. Loss: 0.4624. Acc.: 73.75%\nEpoch: 731. Loss: 0.4356. Acc.: 75.33%\nEpoch: 732. Loss: 0.4440. Acc.: 75.07%\nEpoch: 733. Loss: 0.4597. Acc.: 75.85%\nEpoch: 734. Loss: 0.4419. Acc.: 75.07%\nEpoch: 735. Loss: 0.4379. Acc.: 75.33%\nEpoch: 736. Loss: 0.4338. Acc.: 74.41%\nEpoch: 737. Loss: 0.4587. Acc.: 75.85%\nEpoch: 738. Loss: 0.4265. Acc.: 74.02%\nEpoch: 739. Loss: 0.4671. Acc.: 75.33%\nEpoch: 740. Loss: 0.4607. Acc.: 74.41%\nEpoch: 741. Loss: 0.4419. Acc.: 74.80%\nEpoch: 742. Loss: 0.4352. Acc.: 75.20%\nEpoch: 743. Loss: 0.4452. Acc.: 72.57%\nEpoch: 744. Loss: 0.4507. Acc.: 73.75%\nEpoch: 745. Loss: 0.4480. Acc.: 75.59%\nEpoch: 746. Loss: 0.4595. Acc.: 75.59%\nEpoch: 747. Loss: 0.4272. Acc.: 74.15%\nEpoch: 748. Loss: 0.4361. Acc.: 74.54%\nEpoch: 749. Loss: 0.4321. Acc.: 74.67%\nEpoch: 750. Loss: 0.4415. Acc.: 74.02%\nEpoch: 751. Loss: 0.4090. Acc.: 75.85%\nEpoch: 752. Loss: 0.4480. Acc.: 75.46%\nEpoch: 753. Loss: 0.4400. Acc.: 74.41%\nEpoch: 754. Loss: 0.4456. Acc.: 75.20%\nEpoch: 755. Loss: 0.4262. Acc.: 73.23%\nEpoch: 756. Loss: 0.4389. Acc.: 74.80%\nEpoch: 757. Loss: 0.4387. Acc.: 75.33%\nEpoch: 758. Loss: 0.4337. Acc.: 74.80%\nEpoch: 759. Loss: 0.4552. Acc.: 76.12%\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 760. Loss: 0.4310. Acc.: 73.62%\nEpoch: 761. Loss: 0.4357. Acc.: 74.80%\nEpoch: 762. Loss: 0.4249. Acc.: 76.25%\nEpoch: 763. Loss: 0.4380. Acc.: 73.75%\nEpoch: 764. Loss: 0.4388. Acc.: 74.02%\nEpoch: 765. Loss: 0.4346. Acc.: 75.20%\nEpoch: 766. Loss: 0.4465. Acc.: 74.41%\nEpoch: 767. Loss: 0.4343. Acc.: 73.88%\nEpoch: 768. Loss: 0.4410. Acc.: 74.02%\nEpoch: 769. Loss: 0.4477. Acc.: 75.20%\nEpoch: 770. Loss: 0.4392. Acc.: 73.23%\nEpoch: 771. Loss: 0.4357. Acc.: 74.28%\nEpoch: 772. Loss: 0.4446. Acc.: 74.54%\nEpoch: 773. Loss: 0.4167. Acc.: 74.02%\nEpoch: 774. Loss: 0.4462. Acc.: 74.28%\nEpoch: 775. Loss: 0.4395. Acc.: 75.07%\nEpoch: 776. Loss: 0.4314. Acc.: 75.72%\nEpoch: 777. Loss: 0.4371. Acc.: 74.93%\nEpoch: 778. Loss: 0.4305. Acc.: 76.12%\nEpoch: 779. Loss: 0.4341. Acc.: 75.59%\nEpoch: 780. Loss: 0.4344. Acc.: 74.80%\nEpoch: 781. Loss: 0.4314. Acc.: 75.20%\nEpoch: 782. Loss: 0.4398. Acc.: 75.85%\nEpoch: 783. Loss: 0.4339. Acc.: 74.67%\nEpoch: 784. Loss: 0.4392. Acc.: 74.28%\nEpoch: 785. Loss: 0.4571. Acc.: 74.02%\nEpoch: 786. Loss: 0.4394. Acc.: 73.62%\nEpoch: 787. Loss: 0.4333. Acc.: 74.02%\nEpoch: 788. Loss: 0.4305. Acc.: 74.28%\nEpoch: 789. Loss: 0.4408. Acc.: 74.67%\nEpoch: 790. Loss: 0.4239. Acc.: 74.80%\nEpoch: 791. Loss: 0.4425. Acc.: 76.25%\nEpoch: 792. Loss: 0.4531. Acc.: 76.12%\nEpoch: 793. Loss: 0.4366. Acc.: 74.93%\nEpoch: 794. Loss: 0.4464. Acc.: 74.15%\nEpoch: 795. Loss: 0.4224. Acc.: 75.07%\nEpoch: 796. Loss: 0.4636. Acc.: 75.20%\nEpoch: 797. Loss: 0.4219. Acc.: 75.07%\nEpoch: 798. Loss: 0.4312. Acc.: 74.41%\nEpoch: 799. Loss: 0.4549. Acc.: 75.20%\nEpoch: 800. Loss: 0.4451. Acc.: 74.67%\nEpoch: 801. Loss: 0.4300. Acc.: 73.23%\nEpoch: 802. Loss: 0.4481. Acc.: 75.72%\nEpoch: 803. Loss: 0.4314. Acc.: 75.46%\nEpoch: 804. Loss: 0.4252. Acc.: 74.80%\nEpoch: 805. Loss: 0.4080. Acc.: 75.59%\nEpoch: 806. Loss: 0.4240. Acc.: 73.75%\nEpoch: 807. Loss: 0.4202. Acc.: 73.88%\nEpoch: 808. Loss: 0.4372. Acc.: 72.83%\nEpoch: 809. Loss: 0.4566. Acc.: 73.88%\nEpoch: 810. Loss: 0.4578. Acc.: 74.67%\nEpoch: 811. Loss: 0.4264. Acc.: 73.49%\nEpoch: 812. Loss: 0.4447. Acc.: 74.41%\nEpoch: 813. Loss: 0.4518. Acc.: 75.85%\nEpoch: 814. Loss: 0.4192. Acc.: 74.67%\nEpoch: 815. Loss: 0.4218. Acc.: 74.54%\nEpoch: 816. Loss: 0.4454. Acc.: 74.67%\nEpoch: 817. Loss: 0.4356. Acc.: 72.97%\nEpoch: 818. Loss: 0.4520. Acc.: 75.20%\nEpoch: 819. Loss: 0.4391. Acc.: 75.46%\nEpoch: 820. Loss: 0.4500. Acc.: 76.25%\nEpoch: 821. Loss: 0.4425. Acc.: 73.75%\nEpoch: 822. Loss: 0.4353. Acc.: 75.20%\nEpoch: 823. Loss: 0.4198. Acc.: 74.15%\nEpoch: 824. Loss: 0.4511. Acc.: 73.88%\nEpoch: 825. Loss: 0.4348. Acc.: 74.41%\nEpoch: 826. Loss: 0.4209. Acc.: 74.54%\nEpoch: 827. Loss: 0.4269. Acc.: 74.93%\nEpoch: 828. Loss: 0.4481. Acc.: 74.15%\nEpoch: 829. Loss: 0.4375. Acc.: 74.28%\nEpoch: 830. Loss: 0.4401. Acc.: 76.51%\nEpoch: 831. Loss: 0.4254. Acc.: 74.02%\nEpoch: 832. Loss: 0.4493. Acc.: 75.07%\nEpoch: 833. Loss: 0.4304. Acc.: 74.93%\nEpoch: 834. Loss: 0.4162. Acc.: 74.15%\nEpoch: 835. Loss: 0.4106. Acc.: 74.41%\nEpoch: 836. Loss: 0.4327. Acc.: 76.25%\nEpoch: 837. Loss: 0.4346. Acc.: 74.15%\nEpoch: 838. Loss: 0.4217. Acc.: 75.33%\nEpoch: 839. Loss: 0.4226. Acc.: 74.02%\nEpoch: 840. Loss: 0.4506. Acc.: 74.93%\nEpoch: 841. Loss: 0.4156. Acc.: 73.23%\nEpoch: 842. Loss: 0.4300. Acc.: 73.10%\nEpoch: 843. Loss: 0.4365. Acc.: 74.28%\nEpoch: 844. Loss: 0.4416. Acc.: 74.28%\nEpoch: 845. Loss: 0.4319. Acc.: 74.41%\nEpoch: 846. Loss: 0.4039. Acc.: 75.07%\nEpoch: 847. Loss: 0.4409. Acc.: 75.98%\nEpoch: 848. Loss: 0.4421. Acc.: 75.33%\nEpoch: 849. Loss: 0.4221. Acc.: 75.85%\nEpoch: 850. Loss: 0.4260. Acc.: 74.54%\nEpoch: 851. Loss: 0.4227. Acc.: 75.59%\nEpoch: 852. Loss: 0.4386. Acc.: 74.67%\nEpoch: 853. Loss: 0.4215. Acc.: 75.98%\nEpoch: 854. Loss: 0.4377. Acc.: 75.20%\nEpoch: 855. Loss: 0.4298. Acc.: 74.67%\nEpoch: 856. Loss: 0.4406. Acc.: 72.83%\nEpoch: 857. Loss: 0.4355. Acc.: 74.67%\nEpoch: 858. Loss: 0.4360. Acc.: 73.75%\nEpoch: 859. Loss: 0.4457. Acc.: 75.20%\nEpoch: 860. Loss: 0.4355. Acc.: 74.54%\nEpoch: 861. Loss: 0.4279. Acc.: 74.80%\nEpoch: 862. Loss: 0.4314. Acc.: 73.10%\nEpoch: 863. Loss: 0.4149. Acc.: 75.07%\nEpoch: 864. Loss: 0.4241. Acc.: 72.97%\nEpoch: 865. Loss: 0.4332. Acc.: 74.80%\nEpoch: 866. Loss: 0.4276. Acc.: 74.28%\nEpoch: 867. Loss: 0.4323. Acc.: 74.54%\nEpoch: 868. Loss: 0.4300. Acc.: 75.07%\nEpoch: 869. Loss: 0.4390. Acc.: 74.28%\nEpoch: 870. Loss: 0.4242. Acc.: 74.28%\nEpoch: 871. Loss: 0.4087. Acc.: 75.46%\nEpoch: 872. Loss: 0.4150. Acc.: 74.02%\nEpoch: 873. Loss: 0.4256. Acc.: 74.93%\nEpoch: 874. Loss: 0.4189. Acc.: 74.67%\nEpoch: 875. Loss: 0.4134. Acc.: 75.85%\nEpoch: 876. Loss: 0.4445. Acc.: 75.20%\nEpoch: 877. Loss: 0.4253. Acc.: 75.07%\nEpoch: 878. Loss: 0.4367. Acc.: 74.54%\nEpoch: 879. Loss: 0.4328. Acc.: 74.54%\nEpoch: 880. Loss: 0.4292. Acc.: 75.07%\nEpoch: 881. Loss: 0.4419. Acc.: 74.54%\nEpoch: 882. Loss: 0.4234. Acc.: 74.28%\nEpoch: 883. Loss: 0.4472. Acc.: 74.28%\nEpoch: 884. Loss: 0.4272. Acc.: 75.07%\nEpoch: 885. Loss: 0.4333. Acc.: 74.67%\nEpoch: 886. Loss: 0.4378. Acc.: 74.54%\nEpoch: 887. Loss: 0.4258. Acc.: 74.67%\nEpoch: 888. Loss: 0.4301. Acc.: 75.33%\nEpoch: 889. Loss: 0.4398. Acc.: 76.12%\nEpoch: 890. Loss: 0.4291. Acc.: 75.72%\nEpoch: 891. Loss: 0.4296. Acc.: 75.33%\nEpoch: 892. Loss: 0.4556. Acc.: 75.20%\nEpoch: 893. Loss: 0.4518. Acc.: 74.80%\nEpoch: 894. Loss: 0.4438. Acc.: 74.67%\nEpoch: 895. Loss: 0.4364. Acc.: 74.93%\nEpoch: 896. Loss: 0.4332. Acc.: 75.59%\nEpoch: 897. Loss: 0.4241. Acc.: 75.33%\nEpoch: 898. Loss: 0.4227. Acc.: 76.25%\nEpoch: 899. Loss: 0.4195. Acc.: 73.62%\nEpoch: 900. Loss: 0.4354. Acc.: 75.33%\nEpoch: 901. Loss: 0.4316. Acc.: 74.93%\nEpoch: 902. Loss: 0.4277. Acc.: 74.93%\nEpoch: 903. Loss: 0.4256. Acc.: 73.88%\nEpoch: 904. Loss: 0.4145. Acc.: 74.93%\nEpoch: 905. Loss: 0.4170. Acc.: 74.15%\nEpoch: 906. Loss: 0.4449. Acc.: 73.88%\nEpoch: 907. Loss: 0.4423. Acc.: 74.28%\nEpoch: 908. Loss: 0.4447. Acc.: 73.88%\nEpoch: 909. Loss: 0.4298. Acc.: 74.28%\nEpoch: 910. Loss: 0.4341. Acc.: 73.75%\nEpoch: 911. Loss: 0.4645. Acc.: 75.20%\nEpoch: 912. Loss: 0.4365. Acc.: 75.33%\nEpoch: 913. Loss: 0.4305. Acc.: 74.02%\nEpoch: 914. Loss: 0.4226. Acc.: 74.67%\nEpoch: 915. Loss: 0.4293. Acc.: 74.02%\nEpoch: 916. Loss: 0.4174. Acc.: 73.62%\nEpoch: 917. Loss: 0.4250. Acc.: 75.07%\nEpoch: 918. Loss: 0.4284. Acc.: 73.75%\nEpoch: 919. Loss: 0.4233. Acc.: 74.93%\nEpoch: 920. Loss: 0.4411. Acc.: 75.33%\nEpoch: 921. Loss: 0.4250. Acc.: 74.67%\nEpoch: 922. Loss: 0.4376. Acc.: 73.49%\nEpoch: 923. Loss: 0.4453. Acc.: 73.62%\nEpoch: 924. Loss: 0.4360. Acc.: 74.54%\nEpoch: 925. Loss: 0.4141. Acc.: 74.54%\nEpoch: 926. Loss: 0.4268. Acc.: 74.67%\nEpoch: 927. Loss: 0.4318. Acc.: 73.88%\nEpoch: 928. Loss: 0.4287. Acc.: 73.88%\nEpoch: 929. Loss: 0.4186. Acc.: 73.88%\nEpoch: 930. Loss: 0.4168. Acc.: 74.67%\nEpoch: 931. Loss: 0.4196. Acc.: 74.67%\nEpoch: 932. Loss: 0.4157. Acc.: 74.80%\nEpoch: 933. Loss: 0.4372. Acc.: 74.54%\nEpoch: 934. Loss: 0.4049. Acc.: 75.07%\nEpoch: 935. Loss: 0.4244. Acc.: 74.93%\nEpoch: 936. Loss: 0.4242. Acc.: 74.54%\nEpoch: 937. Loss: 0.4200. Acc.: 75.20%\nEpoch: 938. Loss: 0.4091. Acc.: 73.75%\nEpoch: 939. Loss: 0.4316. Acc.: 74.54%\nEpoch: 940. Loss: 0.4225. Acc.: 74.28%\nEpoch: 941. Loss: 0.4324. Acc.: 75.20%\nEpoch: 942. Loss: 0.4293. Acc.: 74.28%\nEpoch: 943. Loss: 0.4119. Acc.: 75.20%\nEpoch: 944. Loss: 0.4127. Acc.: 75.07%\nEpoch: 945. Loss: 0.3978. Acc.: 76.64%\nEpoch: 946. Loss: 0.4180. Acc.: 77.03%\nEpoch: 947. Loss: 0.4264. Acc.: 74.80%\nEpoch: 948. Loss: 0.4324. Acc.: 75.20%\nEpoch: 949. Loss: 0.4218. Acc.: 75.85%\nEpoch: 950. Loss: 0.4329. Acc.: 75.72%\nEpoch: 951. Loss: 0.4269. Acc.: 75.46%\nEpoch: 952. Loss: 0.4374. Acc.: 74.80%\nEpoch: 953. Loss: 0.4246. Acc.: 76.25%\nEpoch: 954. Loss: 0.4348. Acc.: 74.80%\nEpoch: 955. Loss: 0.4294. Acc.: 74.02%\nEpoch: 956. Loss: 0.4552. Acc.: 74.93%\nEpoch: 957. Loss: 0.4267. Acc.: 74.15%\nEpoch: 958. Loss: 0.4258. Acc.: 75.20%\nEpoch: 959. Loss: 0.4347. Acc.: 74.41%\nEpoch: 960. Loss: 0.4156. Acc.: 75.07%\nEpoch: 961. Loss: 0.4156. Acc.: 76.25%\nEpoch: 962. Loss: 0.4362. Acc.: 75.33%\nEpoch: 963. Loss: 0.4317. Acc.: 74.80%\nEpoch: 964. Loss: 0.4246. Acc.: 77.03%\nEpoch: 965. Loss: 0.4125. Acc.: 75.07%\nEpoch: 966. Loss: 0.4434. Acc.: 75.33%\nEpoch: 967. Loss: 0.4345. Acc.: 74.80%\nEpoch: 968. Loss: 0.4115. Acc.: 74.93%\nEpoch: 969. Loss: 0.4266. Acc.: 74.54%\nEpoch: 970. Loss: 0.4269. Acc.: 76.12%\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 971. Loss: 0.4082. Acc.: 77.03%\nEpoch: 972. Loss: 0.4324. Acc.: 77.82%\nEpoch 972 best model saved with accuracy: 77.82%\nEpoch: 973. Loss: 0.4286. Acc.: 75.20%\nEpoch: 974. Loss: 0.4211. Acc.: 76.25%\nEpoch: 975. Loss: 0.4033. Acc.: 75.07%\nEpoch: 976. Loss: 0.4102. Acc.: 74.93%\nEpoch: 977. Loss: 0.4310. Acc.: 76.25%\nEpoch: 978. Loss: 0.4232. Acc.: 75.07%\nEpoch: 979. Loss: 0.4221. Acc.: 75.46%\nEpoch: 980. Loss: 0.4421. Acc.: 74.93%\nEpoch: 981. Loss: 0.4228. Acc.: 75.59%\nEpoch: 982. Loss: 0.4239. Acc.: 75.46%\nEpoch: 983. Loss: 0.4238. Acc.: 75.07%\nEpoch: 984. Loss: 0.4327. Acc.: 76.38%\nEpoch: 985. Loss: 0.4166. Acc.: 75.85%\nEpoch: 986. Loss: 0.4239. Acc.: 77.56%\nEpoch: 987. Loss: 0.4175. Acc.: 77.17%\nEpoch: 988. Loss: 0.4185. Acc.: 75.33%\nEpoch: 989. Loss: 0.4261. Acc.: 77.43%\nEpoch: 990. Loss: 0.4286. Acc.: 75.98%\nEpoch: 991. Loss: 0.4136. Acc.: 77.17%\nEpoch: 992. Loss: 0.4212. Acc.: 75.72%\nEpoch: 993. Loss: 0.4042. Acc.: 75.33%\nEpoch: 994. Loss: 0.4173. Acc.: 74.41%\nEpoch: 995. Loss: 0.4313. Acc.: 74.67%\nEpoch: 996. Loss: 0.4298. Acc.: 74.41%\nEpoch: 997. Loss: 0.4042. Acc.: 74.80%\nEpoch: 998. Loss: 0.4371. Acc.: 75.85%\nEpoch: 999. Loss: 0.4361. Acc.: 76.51%\nEpoch: 1000. Loss: 0.4509. Acc.: 76.12%\nEpoch: 1001. Loss: 0.4201. Acc.: 76.51%\nEpoch: 1002. Loss: 0.4288. Acc.: 75.72%\nEpoch: 1003. Loss: 0.4223. Acc.: 76.51%\nEpoch: 1004. Loss: 0.4281. Acc.: 76.12%\nEpoch: 1005. Loss: 0.4203. Acc.: 76.90%\nEpoch: 1006. Loss: 0.4337. Acc.: 75.59%\nEpoch: 1007. Loss: 0.4368. Acc.: 75.72%\nEpoch: 1008. Loss: 0.4111. Acc.: 76.64%\nEpoch: 1009. Loss: 0.4271. Acc.: 75.46%\nEpoch: 1010. Loss: 0.4258. Acc.: 75.98%\nEpoch: 1011. Loss: 0.4119. Acc.: 75.59%\nEpoch: 1012. Loss: 0.4542. Acc.: 76.25%\nEpoch: 1013. Loss: 0.4200. Acc.: 75.07%\nEpoch: 1014. Loss: 0.4237. Acc.: 76.25%\nEpoch: 1015. Loss: 0.4387. Acc.: 76.38%\nEpoch: 1016. Loss: 0.4100. Acc.: 75.20%\nEpoch: 1017. Loss: 0.4192. Acc.: 74.93%\nEpoch: 1018. Loss: 0.4139. Acc.: 76.12%\nEpoch: 1019. Loss: 0.4152. Acc.: 75.07%\nEpoch: 1020. Loss: 0.4161. Acc.: 76.12%\nEpoch: 1021. Loss: 0.4424. Acc.: 75.20%\nEpoch: 1022. Loss: 0.4127. Acc.: 74.67%\nEpoch: 1023. Loss: 0.4178. Acc.: 74.93%\nEpoch: 1024. Loss: 0.4172. Acc.: 75.46%\nEpoch: 1024. Loss: 0.4172. Acc.: 75.46%\nEpoch: 1025. Loss: 0.4275. Acc.: 76.12%\nEpoch: 1026. Loss: 0.4205. Acc.: 73.88%\nEpoch: 1027. Loss: 0.4228. Acc.: 74.67%\nEpoch: 1028. Loss: 0.3909. Acc.: 75.46%\nEpoch: 1029. Loss: 0.4235. Acc.: 75.46%\nEpoch: 1030. Loss: 0.4117. Acc.: 76.25%\nEpoch: 1031. Loss: 0.4334. Acc.: 75.72%\nEpoch: 1032. Loss: 0.4469. Acc.: 74.80%\nEpoch: 1033. Loss: 0.4131. Acc.: 73.49%\nEpoch: 1034. Loss: 0.4101. Acc.: 75.33%\nEpoch: 1035. Loss: 0.4110. Acc.: 73.23%\nEpoch: 1036. Loss: 0.4090. Acc.: 74.02%\nEpoch: 1037. Loss: 0.4238. Acc.: 73.88%\nEpoch: 1038. Loss: 0.4198. Acc.: 75.33%\nEpoch: 1039. Loss: 0.4196. Acc.: 73.88%\nEpoch: 1040. Loss: 0.4104. Acc.: 74.02%\nEpoch: 1041. Loss: 0.4272. Acc.: 75.59%\nEpoch: 1042. Loss: 0.4259. Acc.: 74.93%\nEpoch: 1043. Loss: 0.4246. Acc.: 75.07%\nEpoch: 1044. Loss: 0.4289. Acc.: 75.59%\nEpoch: 1045. Loss: 0.4090. Acc.: 76.38%\nEpoch: 1046. Loss: 0.4329. Acc.: 76.77%\nEpoch: 1047. Loss: 0.4339. Acc.: 75.85%\nEpoch: 1048. Loss: 0.4126. Acc.: 75.98%\nEpoch: 1049. Loss: 0.4197. Acc.: 76.38%\nEpoch: 1050. Loss: 0.4370. Acc.: 75.85%\nEpoch: 1051. Loss: 0.4265. Acc.: 76.25%\nEpoch: 1052. Loss: 0.4274. Acc.: 76.25%\nEpoch: 1053. Loss: 0.4343. Acc.: 76.12%\nEpoch: 1054. Loss: 0.4290. Acc.: 75.46%\nEpoch: 1055. Loss: 0.4229. Acc.: 74.67%\nEpoch: 1056. Loss: 0.4359. Acc.: 75.46%\nEpoch: 1057. Loss: 0.4055. Acc.: 75.85%\nEpoch: 1058. Loss: 0.4153. Acc.: 76.77%\nEpoch: 1059. Loss: 0.4218. Acc.: 77.30%\nEpoch: 1060. Loss: 0.4415. Acc.: 74.41%\nEpoch: 1061. Loss: 0.4051. Acc.: 75.98%\nEpoch: 1062. Loss: 0.4164. Acc.: 77.69%\nEpoch: 1063. Loss: 0.4211. Acc.: 75.20%\nEpoch: 1064. Loss: 0.4274. Acc.: 76.12%\nEpoch: 1065. Loss: 0.4367. Acc.: 75.72%\nEpoch: 1066. Loss: 0.4103. Acc.: 77.03%\nEpoch: 1067. Loss: 0.4172. Acc.: 75.72%\nEpoch: 1068. Loss: 0.4462. Acc.: 74.80%\nEpoch: 1069. Loss: 0.4033. Acc.: 77.03%\nEpoch: 1070. Loss: 0.4050. Acc.: 78.22%\nEpoch 1070 best model saved with accuracy: 78.22%\nEpoch: 1071. Loss: 0.4157. Acc.: 75.85%\nEpoch: 1072. Loss: 0.4326. Acc.: 75.59%\nEpoch: 1073. Loss: 0.4148. Acc.: 75.46%\nEpoch: 1074. Loss: 0.4102. Acc.: 76.38%\nEpoch: 1075. Loss: 0.4100. Acc.: 77.43%\nEpoch: 1076. Loss: 0.4128. Acc.: 76.38%\nEpoch: 1077. Loss: 0.4312. Acc.: 75.85%\nEpoch: 1078. Loss: 0.4009. Acc.: 74.93%\nEpoch: 1079. Loss: 0.4153. Acc.: 76.77%\nEpoch: 1080. Loss: 0.4075. Acc.: 75.07%\nEpoch: 1081. Loss: 0.4035. Acc.: 77.30%\nEpoch: 1082. Loss: 0.4309. Acc.: 76.38%\nEpoch: 1083. Loss: 0.4050. Acc.: 75.33%\nEpoch: 1084. Loss: 0.3984. Acc.: 74.80%\nEpoch: 1085. Loss: 0.4249. Acc.: 76.90%\nEpoch: 1086. Loss: 0.4182. Acc.: 75.07%\nEpoch: 1087. Loss: 0.4229. Acc.: 76.12%\nEpoch: 1088. Loss: 0.4229. Acc.: 75.72%\nEpoch: 1089. Loss: 0.4103. Acc.: 76.64%\nEpoch: 1090. Loss: 0.4352. Acc.: 75.85%\nEpoch: 1091. Loss: 0.4119. Acc.: 76.25%\nEpoch: 1092. Loss: 0.4117. Acc.: 76.90%\nEpoch: 1093. Loss: 0.4205. Acc.: 76.64%\nEpoch: 1094. Loss: 0.4308. Acc.: 76.51%\nEpoch: 1095. Loss: 0.4391. Acc.: 76.12%\nEpoch: 1096. Loss: 0.4061. Acc.: 77.17%\nEpoch: 1097. Loss: 0.4135. Acc.: 76.90%\nEpoch: 1098. Loss: 0.4168. Acc.: 76.77%\nEpoch: 1099. Loss: 0.4313. Acc.: 77.17%\nEpoch: 1100. Loss: 0.4123. Acc.: 75.98%\nEpoch: 1101. Loss: 0.4287. Acc.: 76.64%\nEpoch: 1102. Loss: 0.4313. Acc.: 76.38%\nEpoch: 1103. Loss: 0.4132. Acc.: 74.80%\nEpoch: 1104. Loss: 0.4400. Acc.: 75.46%\nEpoch: 1105. Loss: 0.4184. Acc.: 74.80%\nEpoch: 1106. Loss: 0.4190. Acc.: 73.23%\nEpoch: 1107. Loss: 0.4535. Acc.: 76.25%\nEpoch: 1108. Loss: 0.4262. Acc.: 77.30%\nEpoch: 1109. Loss: 0.4090. Acc.: 76.51%\nEpoch: 1110. Loss: 0.4020. Acc.: 75.85%\nEpoch: 1111. Loss: 0.4278. Acc.: 73.49%\nEpoch: 1112. Loss: 0.4175. Acc.: 75.33%\nEpoch: 1113. Loss: 0.4160. Acc.: 77.30%\nEpoch: 1114. Loss: 0.4129. Acc.: 77.69%\nEpoch: 1115. Loss: 0.4160. Acc.: 76.12%\nEpoch: 1116. Loss: 0.4047. Acc.: 76.38%\nEpoch: 1117. Loss: 0.4040. Acc.: 76.77%\nEpoch: 1118. Loss: 0.4031. Acc.: 75.72%\nEpoch: 1119. Loss: 0.4214. Acc.: 76.25%\nEpoch: 1120. Loss: 0.4436. Acc.: 75.98%\nEpoch: 1121. Loss: 0.3886. Acc.: 75.46%\nEpoch: 1122. Loss: 0.4046. Acc.: 78.74%\nEpoch 1122 best model saved with accuracy: 78.74%\nEpoch: 1123. Loss: 0.4258. Acc.: 75.85%\nEpoch: 1124. Loss: 0.4162. Acc.: 75.33%\nEpoch: 1125. Loss: 0.4131. Acc.: 74.80%\nEpoch: 1126. Loss: 0.4271. Acc.: 76.51%\nEpoch: 1127. Loss: 0.3976. Acc.: 76.77%\nEpoch: 1128. Loss: 0.3995. Acc.: 76.77%\nEpoch: 1129. Loss: 0.4076. Acc.: 76.64%\nEpoch: 1130. Loss: 0.4110. Acc.: 75.85%\nEpoch: 1131. Loss: 0.4315. Acc.: 76.90%\nEpoch: 1132. Loss: 0.4036. Acc.: 75.59%\nEpoch: 1133. Loss: 0.4106. Acc.: 75.85%\nEpoch: 1134. Loss: 0.4057. Acc.: 77.17%\nEpoch: 1135. Loss: 0.4217. Acc.: 76.64%\nEpoch: 1136. Loss: 0.4223. Acc.: 77.30%\nEpoch: 1137. Loss: 0.3984. Acc.: 75.46%\nEpoch: 1138. Loss: 0.4117. Acc.: 77.03%\nEpoch: 1139. Loss: 0.4085. Acc.: 75.59%\nEpoch: 1140. Loss: 0.4023. Acc.: 77.17%\nEpoch: 1141. Loss: 0.4201. Acc.: 75.59%\nEpoch: 1142. Loss: 0.4068. Acc.: 75.98%\nEpoch: 1143. Loss: 0.4092. Acc.: 76.25%\nEpoch: 1144. Loss: 0.4098. Acc.: 76.90%\nEpoch: 1145. Loss: 0.4052. Acc.: 75.72%\nEpoch: 1146. Loss: 0.4053. Acc.: 75.72%\nEpoch: 1147. Loss: 0.4118. Acc.: 75.85%\nEpoch: 1148. Loss: 0.4147. Acc.: 75.98%\nEpoch: 1149. Loss: 0.3989. Acc.: 75.33%\nEpoch: 1150. Loss: 0.4287. Acc.: 76.38%\nEpoch: 1151. Loss: 0.4179. Acc.: 75.85%\nEpoch: 1152. Loss: 0.4233. Acc.: 75.59%\nEpoch: 1153. Loss: 0.4107. Acc.: 75.85%\nEpoch: 1154. Loss: 0.3934. Acc.: 74.28%\nEpoch: 1155. Loss: 0.4122. Acc.: 76.38%\nEpoch: 1156. Loss: 0.4133. Acc.: 75.46%\nEpoch: 1157. Loss: 0.4216. Acc.: 76.90%\nEpoch: 1158. Loss: 0.4085. Acc.: 75.46%\nEpoch: 1159. Loss: 0.4290. Acc.: 76.64%\nEpoch: 1160. Loss: 0.4016. Acc.: 75.98%\nEpoch: 1161. Loss: 0.3995. Acc.: 77.03%\nEpoch: 1162. Loss: 0.4120. Acc.: 75.85%\nEpoch: 1163. Loss: 0.4200. Acc.: 75.07%\nEpoch: 1164. Loss: 0.4063. Acc.: 74.41%\nEpoch: 1165. Loss: 0.4087. Acc.: 76.25%\nEpoch: 1166. Loss: 0.4023. Acc.: 75.20%\nEpoch: 1167. Loss: 0.4203. Acc.: 75.20%\nEpoch: 1168. Loss: 0.4182. Acc.: 75.20%\nEpoch: 1169. Loss: 0.4142. Acc.: 76.25%\nEpoch: 1170. Loss: 0.4290. Acc.: 76.77%\nEpoch: 1171. Loss: 0.3984. Acc.: 76.90%\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 1172. Loss: 0.4295. Acc.: 75.59%\nEpoch: 1173. Loss: 0.4111. Acc.: 77.03%\nEpoch: 1174. Loss: 0.4046. Acc.: 77.69%\nEpoch: 1175. Loss: 0.4249. Acc.: 76.38%\nEpoch: 1176. Loss: 0.4369. Acc.: 75.98%\nEpoch: 1177. Loss: 0.4026. Acc.: 76.12%\nEpoch: 1178. Loss: 0.4333. Acc.: 74.80%\nEpoch: 1179. Loss: 0.4129. Acc.: 75.59%\nEpoch: 1180. Loss: 0.4047. Acc.: 74.54%\nEpoch: 1181. Loss: 0.4193. Acc.: 75.07%\nEpoch: 1182. Loss: 0.4503. Acc.: 75.98%\nEpoch: 1183. Loss: 0.4273. Acc.: 75.07%\nEpoch: 1184. Loss: 0.4009. Acc.: 75.20%\nEpoch: 1185. Loss: 0.4028. Acc.: 77.03%\nEpoch: 1186. Loss: 0.4141. Acc.: 74.93%\nEpoch: 1187. Loss: 0.4234. Acc.: 77.03%\nEpoch: 1188. Loss: 0.4007. Acc.: 76.64%\nEpoch: 1189. Loss: 0.4059. Acc.: 76.51%\nEpoch: 1190. Loss: 0.4166. Acc.: 75.72%\nEpoch: 1191. Loss: 0.4085. Acc.: 75.33%\nEpoch: 1192. Loss: 0.4153. Acc.: 77.03%\nEpoch: 1193. Loss: 0.4257. Acc.: 76.64%\nEpoch: 1194. Loss: 0.4246. Acc.: 76.90%\nEpoch: 1195. Loss: 0.4192. Acc.: 76.12%\nEpoch: 1196. Loss: 0.4120. Acc.: 75.85%\nEpoch: 1197. Loss: 0.4056. Acc.: 76.38%\nEpoch: 1198. Loss: 0.4259. Acc.: 75.59%\nEpoch: 1199. Loss: 0.4075. Acc.: 75.46%\nEpoch: 1200. Loss: 0.4118. Acc.: 75.59%\nEpoch: 1201. Loss: 0.4076. Acc.: 76.12%\nEpoch: 1202. Loss: 0.4085. Acc.: 77.17%\nEpoch: 1203. Loss: 0.4231. Acc.: 76.51%\nEpoch: 1204. Loss: 0.4120. Acc.: 76.64%\nEpoch: 1205. Loss: 0.4036. Acc.: 75.85%\nEpoch: 1206. Loss: 0.4104. Acc.: 75.33%\nEpoch: 1207. Loss: 0.3959. Acc.: 75.72%\nEpoch: 1208. Loss: 0.4148. Acc.: 74.80%\nEpoch: 1209. Loss: 0.4119. Acc.: 77.17%\nEpoch: 1210. Loss: 0.4110. Acc.: 75.85%\nEpoch: 1211. Loss: 0.4109. Acc.: 76.38%\nEpoch: 1212. Loss: 0.4283. Acc.: 75.20%\nEpoch: 1213. Loss: 0.4237. Acc.: 75.85%\nEpoch: 1214. Loss: 0.4181. Acc.: 76.51%\nEpoch: 1215. Loss: 0.4198. Acc.: 78.61%\nEpoch: 1216. Loss: 0.4309. Acc.: 78.35%\nEpoch: 1217. Loss: 0.4061. Acc.: 75.72%\nEpoch: 1218. Loss: 0.4154. Acc.: 76.77%\nEpoch: 1219. Loss: 0.4169. Acc.: 76.51%\nEpoch: 1220. Loss: 0.4189. Acc.: 77.30%\nEpoch: 1221. Loss: 0.4139. Acc.: 76.38%\nEpoch: 1222. Loss: 0.4118. Acc.: 77.03%\nEpoch: 1223. Loss: 0.4366. Acc.: 76.51%\nEpoch: 1224. Loss: 0.4223. Acc.: 75.85%\nEpoch: 1225. Loss: 0.4331. Acc.: 76.64%\nEpoch: 1226. Loss: 0.4093. Acc.: 77.69%\nEpoch: 1227. Loss: 0.4050. Acc.: 76.64%\nEpoch: 1228. Loss: 0.4175. Acc.: 75.72%\nEpoch: 1229. Loss: 0.4008. Acc.: 76.51%\nEpoch: 1230. Loss: 0.4076. Acc.: 76.38%\nEpoch: 1231. Loss: 0.4322. Acc.: 76.77%\nEpoch: 1232. Loss: 0.4217. Acc.: 78.35%\nEpoch: 1233. Loss: 0.4253. Acc.: 76.51%\nEpoch: 1234. Loss: 0.4025. Acc.: 76.77%\nEpoch: 1235. Loss: 0.4336. Acc.: 77.03%\nEpoch: 1236. Loss: 0.4082. Acc.: 77.30%\nEpoch: 1237. Loss: 0.4326. Acc.: 76.12%\nEpoch: 1238. Loss: 0.4114. Acc.: 76.51%\nEpoch: 1239. Loss: 0.4378. Acc.: 76.25%\nEpoch: 1240. Loss: 0.4477. Acc.: 75.46%\nEpoch: 1241. Loss: 0.4281. Acc.: 74.80%\nEpoch: 1242. Loss: 0.4308. Acc.: 74.93%\nEpoch: 1243. Loss: 0.4204. Acc.: 75.46%\nEpoch: 1244. Loss: 0.4084. Acc.: 75.98%\nEpoch: 1245. Loss: 0.4087. Acc.: 75.59%\nEpoch: 1246. Loss: 0.4085. Acc.: 76.90%\nEpoch: 1247. Loss: 0.4324. Acc.: 76.90%\nEpoch: 1248. Loss: 0.4212. Acc.: 76.51%\nEpoch: 1249. Loss: 0.3974. Acc.: 76.38%\nEpoch: 1250. Loss: 0.4095. Acc.: 76.12%\nEpoch: 1251. Loss: 0.4186. Acc.: 76.25%\nEpoch: 1252. Loss: 0.4219. Acc.: 76.25%\nEpoch: 1253. Loss: 0.4194. Acc.: 76.25%\nEpoch: 1254. Loss: 0.3962. Acc.: 75.59%\nEpoch: 1255. Loss: 0.4061. Acc.: 75.46%\nEpoch: 1256. Loss: 0.4298. Acc.: 75.85%\nEpoch: 1257. Loss: 0.3951. Acc.: 75.85%\nEpoch: 1258. Loss: 0.4178. Acc.: 76.51%\nEpoch: 1259. Loss: 0.4202. Acc.: 76.51%\nEpoch: 1260. Loss: 0.4153. Acc.: 75.07%\nEpoch: 1261. Loss: 0.4186. Acc.: 76.90%\nEpoch: 1262. Loss: 0.4073. Acc.: 74.41%\nEpoch: 1263. Loss: 0.4216. Acc.: 74.67%\nEpoch: 1264. Loss: 0.4005. Acc.: 76.77%\nEpoch: 1265. Loss: 0.4054. Acc.: 76.12%\nEpoch: 1266. Loss: 0.4185. Acc.: 77.82%\nEpoch: 1267. Loss: 0.3933. Acc.: 75.07%\nEpoch: 1268. Loss: 0.4206. Acc.: 75.07%\nEpoch: 1269. Loss: 0.4352. Acc.: 76.77%\nEpoch: 1270. Loss: 0.4020. Acc.: 74.93%\nEpoch: 1271. Loss: 0.4073. Acc.: 77.17%\nEpoch: 1272. Loss: 0.4211. Acc.: 77.56%\nEpoch: 1273. Loss: 0.4178. Acc.: 77.03%\nEpoch: 1274. Loss: 0.3984. Acc.: 75.98%\nEpoch: 1275. Loss: 0.4050. Acc.: 76.64%\nEpoch: 1276. Loss: 0.4100. Acc.: 76.77%\nEpoch: 1277. Loss: 0.3974. Acc.: 77.03%\nEpoch: 1278. Loss: 0.4131. Acc.: 76.12%\nEpoch: 1279. Loss: 0.4180. Acc.: 76.51%\nEpoch: 1280. Loss: 0.3998. Acc.: 76.51%\nEpoch: 1281. Loss: 0.4007. Acc.: 75.85%\nEpoch: 1282. Loss: 0.3998. Acc.: 76.25%\nEpoch: 1283. Loss: 0.4154. Acc.: 76.38%\nEpoch: 1284. Loss: 0.4022. Acc.: 75.72%\nEpoch: 1285. Loss: 0.3900. Acc.: 75.59%\nEpoch: 1286. Loss: 0.3884. Acc.: 75.98%\nEpoch: 1287. Loss: 0.4173. Acc.: 76.77%\nEpoch: 1288. Loss: 0.4104. Acc.: 77.56%\nEpoch: 1289. Loss: 0.4139. Acc.: 76.51%\nEpoch: 1290. Loss: 0.4063. Acc.: 77.56%\nEpoch: 1291. Loss: 0.4007. Acc.: 76.51%\nEpoch: 1292. Loss: 0.4093. Acc.: 75.98%\nEpoch: 1293. Loss: 0.3975. Acc.: 75.72%\nEpoch: 1294. Loss: 0.3955. Acc.: 75.59%\nEpoch: 1295. Loss: 0.3801. Acc.: 75.98%\nEpoch: 1296. Loss: 0.4077. Acc.: 75.46%\nEpoch: 1297. Loss: 0.4197. Acc.: 77.03%\nEpoch: 1298. Loss: 0.4079. Acc.: 75.33%\nEpoch: 1299. Loss: 0.4161. Acc.: 75.33%\nEpoch: 1300. Loss: 0.4036. Acc.: 75.98%\nEpoch: 1301. Loss: 0.3942. Acc.: 75.20%\nEpoch: 1302. Loss: 0.4012. Acc.: 75.33%\nEpoch: 1303. Loss: 0.4094. Acc.: 75.98%\nEpoch: 1304. Loss: 0.3816. Acc.: 75.46%\nEpoch: 1305. Loss: 0.3990. Acc.: 75.46%\nEpoch: 1306. Loss: 0.4240. Acc.: 75.07%\nEpoch: 1307. Loss: 0.4079. Acc.: 76.51%\nEpoch: 1308. Loss: 0.3974. Acc.: 75.59%\nEpoch: 1309. Loss: 0.4123. Acc.: 75.85%\nEpoch: 1310. Loss: 0.4038. Acc.: 75.46%\nEpoch: 1311. Loss: 0.4033. Acc.: 75.20%\nEpoch: 1312. Loss: 0.4186. Acc.: 75.46%\nEpoch: 1313. Loss: 0.4139. Acc.: 76.51%\nEpoch: 1314. Loss: 0.4080. Acc.: 76.77%\nEpoch: 1315. Loss: 0.4116. Acc.: 76.25%\nEpoch: 1316. Loss: 0.4022. Acc.: 77.43%\nEpoch: 1317. Loss: 0.4215. Acc.: 77.17%\nEpoch: 1318. Loss: 0.4019. Acc.: 75.46%\nEpoch: 1319. Loss: 0.4003. Acc.: 75.46%\nEpoch: 1320. Loss: 0.4050. Acc.: 75.98%\nEpoch: 1321. Loss: 0.4107. Acc.: 76.64%\nEpoch: 1322. Loss: 0.4018. Acc.: 75.33%\nEpoch: 1323. Loss: 0.4206. Acc.: 74.67%\nEpoch: 1324. Loss: 0.3899. Acc.: 76.64%\nEpoch: 1325. Loss: 0.4148. Acc.: 77.03%\nEpoch: 1326. Loss: 0.4060. Acc.: 74.93%\nEpoch: 1327. Loss: 0.4071. Acc.: 75.46%\nEpoch: 1328. Loss: 0.3853. Acc.: 75.98%\nEpoch: 1329. Loss: 0.3989. Acc.: 75.46%\nEpoch: 1330. Loss: 0.3953. Acc.: 75.20%\nEpoch: 1331. Loss: 0.4030. Acc.: 75.59%\nEpoch: 1332. Loss: 0.4224. Acc.: 76.77%\nEpoch: 1333. Loss: 0.3993. Acc.: 76.25%\nEpoch: 1334. Loss: 0.4125. Acc.: 76.25%\nEpoch: 1335. Loss: 0.4035. Acc.: 75.33%\nEpoch: 1336. Loss: 0.4122. Acc.: 75.85%\nEpoch: 1337. Loss: 0.4110. Acc.: 76.12%\nEpoch: 1338. Loss: 0.4115. Acc.: 75.85%\nEpoch: 1339. Loss: 0.4486. Acc.: 75.20%\nEpoch: 1340. Loss: 0.4074. Acc.: 75.20%\nEpoch: 1341. Loss: 0.3943. Acc.: 75.59%\nEpoch: 1342. Loss: 0.4061. Acc.: 76.25%\nEpoch: 1343. Loss: 0.3983. Acc.: 75.72%\nEpoch: 1344. Loss: 0.4162. Acc.: 76.38%\nEpoch: 1345. Loss: 0.3965. Acc.: 75.85%\nEpoch: 1346. Loss: 0.4141. Acc.: 76.77%\nEpoch: 1347. Loss: 0.4184. Acc.: 76.90%\nEpoch: 1348. Loss: 0.4019. Acc.: 75.98%\nEpoch: 1349. Loss: 0.4152. Acc.: 77.95%\nEpoch: 1350. Loss: 0.3976. Acc.: 76.64%\nEpoch: 1351. Loss: 0.4024. Acc.: 75.85%\nEpoch: 1352. Loss: 0.4061. Acc.: 76.90%\nEpoch: 1353. Loss: 0.4105. Acc.: 75.98%\nEpoch: 1354. Loss: 0.3993. Acc.: 75.07%\nEpoch: 1355. Loss: 0.4021. Acc.: 74.80%\nEpoch: 1356. Loss: 0.4056. Acc.: 75.85%\nEpoch: 1357. Loss: 0.4033. Acc.: 75.59%\nEpoch: 1358. Loss: 0.4042. Acc.: 76.90%\nEpoch: 1359. Loss: 0.4055. Acc.: 76.12%\nEpoch: 1360. Loss: 0.4200. Acc.: 76.51%\nEpoch: 1361. Loss: 0.4058. Acc.: 77.03%\nEpoch: 1362. Loss: 0.4158. Acc.: 76.90%\nEpoch: 1363. Loss: 0.3944. Acc.: 75.98%\nEpoch: 1364. Loss: 0.4105. Acc.: 76.64%\nEpoch: 1365. Loss: 0.3978. Acc.: 76.77%\nEpoch: 1366. Loss: 0.4123. Acc.: 76.25%\nEpoch: 1367. Loss: 0.4150. Acc.: 76.51%\nEpoch: 1368. Loss: 0.3798. Acc.: 75.07%\nEpoch: 1369. Loss: 0.4065. Acc.: 75.07%\nEpoch: 1370. Loss: 0.4131. Acc.: 74.28%\nEpoch: 1371. Loss: 0.4082. Acc.: 75.59%\nEpoch: 1372. Loss: 0.4156. Acc.: 74.67%\nEpoch: 1373. Loss: 0.3907. Acc.: 75.59%\nEpoch: 1374. Loss: 0.4033. Acc.: 75.72%\nEpoch: 1375. Loss: 0.4033. Acc.: 75.20%\nEpoch: 1376. Loss: 0.4018. Acc.: 75.59%\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 1377. Loss: 0.4117. Acc.: 74.93%\nEpoch: 1378. Loss: 0.3876. Acc.: 76.12%\nEpoch: 1379. Loss: 0.3943. Acc.: 75.46%\nEpoch: 1380. Loss: 0.3977. Acc.: 75.46%\nEpoch: 1381. Loss: 0.3860. Acc.: 75.20%\nEpoch: 1382. Loss: 0.4065. Acc.: 75.72%\nEpoch: 1383. Loss: 0.3999. Acc.: 76.38%\nEpoch: 1384. Loss: 0.3936. Acc.: 77.30%\nEpoch: 1385. Loss: 0.3997. Acc.: 76.51%\nEpoch: 1386. Loss: 0.3917. Acc.: 75.98%\nEpoch: 1387. Loss: 0.4018. Acc.: 76.25%\nEpoch: 1388. Loss: 0.4168. Acc.: 76.90%\nEpoch: 1389. Loss: 0.4134. Acc.: 77.30%\nEpoch: 1390. Loss: 0.4109. Acc.: 76.90%\nEpoch: 1391. Loss: 0.4204. Acc.: 76.64%\nEpoch: 1392. Loss: 0.3991. Acc.: 77.03%\nEpoch: 1393. Loss: 0.3741. Acc.: 77.03%\nEpoch: 1394. Loss: 0.3927. Acc.: 75.98%\nEpoch: 1395. Loss: 0.4205. Acc.: 77.43%\nEpoch: 1396. Loss: 0.4038. Acc.: 76.12%\nEpoch: 1397. Loss: 0.4023. Acc.: 76.25%\nEpoch: 1398. Loss: 0.4114. Acc.: 76.25%\nEpoch: 1399. Loss: 0.3932. Acc.: 76.12%\nEpoch: 1400. Loss: 0.4085. Acc.: 75.85%\nEpoch: 1401. Loss: 0.4151. Acc.: 76.51%\nEpoch: 1402. Loss: 0.4100. Acc.: 76.25%\nEpoch: 1403. Loss: 0.4054. Acc.: 76.12%\nEpoch: 1404. Loss: 0.3888. Acc.: 76.12%\nEpoch: 1405. Loss: 0.4115. Acc.: 76.25%\nEpoch: 1406. Loss: 0.4346. Acc.: 74.54%\nEpoch: 1407. Loss: 0.4063. Acc.: 76.25%\nEpoch: 1408. Loss: 0.4133. Acc.: 75.59%\nEpoch: 1409. Loss: 0.4059. Acc.: 76.64%\nEpoch: 1410. Loss: 0.3978. Acc.: 75.33%\nEpoch: 1411. Loss: 0.3983. Acc.: 75.33%\nEpoch: 1412. Loss: 0.4184. Acc.: 75.33%\nEpoch: 1413. Loss: 0.4072. Acc.: 75.98%\nEpoch: 1414. Loss: 0.4165. Acc.: 75.07%\nEpoch: 1415. Loss: 0.4082. Acc.: 74.28%\nEpoch: 1416. Loss: 0.4160. Acc.: 74.93%\nEpoch: 1417. Loss: 0.3980. Acc.: 76.77%\nEpoch: 1418. Loss: 0.3967. Acc.: 77.03%\nEpoch: 1419. Loss: 0.4062. Acc.: 76.25%\nEpoch: 1420. Loss: 0.4289. Acc.: 75.85%\nEpoch: 1421. Loss: 0.4020. Acc.: 75.85%\nEpoch: 1422. Loss: 0.3984. Acc.: 74.93%\nEpoch: 1423. Loss: 0.4060. Acc.: 76.51%\nEpoch: 1424. Loss: 0.3852. Acc.: 75.98%\nEpoch: 1425. Loss: 0.3912. Acc.: 76.77%\nEpoch: 1426. Loss: 0.3987. Acc.: 75.98%\nEpoch: 1427. Loss: 0.4167. Acc.: 76.12%\nEpoch: 1428. Loss: 0.4008. Acc.: 77.17%\nEpoch: 1429. Loss: 0.3973. Acc.: 75.72%\nEpoch: 1430. Loss: 0.3915. Acc.: 75.72%\nEpoch: 1431. Loss: 0.3918. Acc.: 75.46%\nEpoch: 1432. Loss: 0.4026. Acc.: 76.90%\nEpoch: 1433. Loss: 0.3877. Acc.: 77.30%\nEpoch: 1434. Loss: 0.4062. Acc.: 75.33%\nEpoch: 1435. Loss: 0.4153. Acc.: 76.90%\nEpoch: 1436. Loss: 0.4012. Acc.: 74.80%\nEpoch: 1437. Loss: 0.3945. Acc.: 76.77%\nEpoch: 1438. Loss: 0.4163. Acc.: 76.38%\nEpoch: 1439. Loss: 0.4130. Acc.: 74.93%\nEpoch: 1440. Loss: 0.3991. Acc.: 75.72%\nEpoch: 1441. Loss: 0.4031. Acc.: 76.90%\nEpoch: 1442. Loss: 0.4218. Acc.: 76.25%\nEpoch: 1443. Loss: 0.3954. Acc.: 74.67%\nEpoch: 1444. Loss: 0.4212. Acc.: 75.85%\nEpoch: 1445. Loss: 0.3968. Acc.: 76.12%\nEpoch: 1446. Loss: 0.4034. Acc.: 75.98%\nEpoch: 1447. Loss: 0.4035. Acc.: 77.56%\nEpoch: 1448. Loss: 0.3951. Acc.: 76.12%\nEpoch: 1449. Loss: 0.4139. Acc.: 76.12%\nEpoch: 1450. Loss: 0.3986. Acc.: 75.85%\nEpoch: 1451. Loss: 0.3889. Acc.: 76.25%\nEpoch: 1452. Loss: 0.4078. Acc.: 75.59%\nEpoch: 1453. Loss: 0.4326. Acc.: 76.64%\nEpoch: 1454. Loss: 0.3959. Acc.: 75.72%\nEpoch: 1455. Loss: 0.4261. Acc.: 75.59%\nEpoch: 1456. Loss: 0.4060. Acc.: 76.51%\nEpoch: 1457. Loss: 0.3654. Acc.: 77.43%\nEpoch: 1458. Loss: 0.4139. Acc.: 76.38%\nEpoch: 1459. Loss: 0.3936. Acc.: 75.72%\nEpoch: 1460. Loss: 0.4064. Acc.: 75.85%\nEpoch: 1461. Loss: 0.4105. Acc.: 75.72%\nEpoch: 1462. Loss: 0.4054. Acc.: 75.98%\nEpoch: 1463. Loss: 0.4093. Acc.: 76.12%\nEpoch: 1464. Loss: 0.3884. Acc.: 74.93%\nEpoch: 1465. Loss: 0.4155. Acc.: 74.54%\nEpoch: 1466. Loss: 0.3987. Acc.: 76.77%\nEpoch: 1467. Loss: 0.4063. Acc.: 74.67%\nEpoch: 1468. Loss: 0.4192. Acc.: 75.33%\nEpoch: 1469. Loss: 0.4195. Acc.: 75.46%\nEpoch: 1470. Loss: 0.4128. Acc.: 75.59%\nEpoch: 1471. Loss: 0.4044. Acc.: 74.80%\nEpoch: 1472. Loss: 0.3979. Acc.: 75.59%\nEpoch: 1473. Loss: 0.4042. Acc.: 76.12%\nEpoch: 1474. Loss: 0.3817. Acc.: 76.51%\nEpoch: 1475. Loss: 0.4263. Acc.: 76.64%\nEpoch: 1476. Loss: 0.4105. Acc.: 77.17%\nEpoch: 1477. Loss: 0.4017. Acc.: 76.64%\nEpoch: 1478. Loss: 0.3918. Acc.: 77.17%\nEpoch: 1479. Loss: 0.4153. Acc.: 74.93%\nEpoch: 1480. Loss: 0.4309. Acc.: 76.90%\nEpoch: 1481. Loss: 0.4097. Acc.: 76.51%\nEpoch: 1482. Loss: 0.4024. Acc.: 75.85%\nEpoch: 1483. Loss: 0.4102. Acc.: 74.80%\nEpoch: 1484. Loss: 0.4000. Acc.: 77.03%\nEpoch: 1485. Loss: 0.4133. Acc.: 75.59%\nEpoch: 1486. Loss: 0.3905. Acc.: 76.25%\nEpoch: 1487. Loss: 0.4075. Acc.: 75.20%\nEpoch: 1488. Loss: 0.4084. Acc.: 74.54%\nEpoch: 1489. Loss: 0.4130. Acc.: 75.33%\nEpoch: 1490. Loss: 0.3911. Acc.: 76.12%\nEpoch: 1491. Loss: 0.4088. Acc.: 74.93%\nEpoch: 1492. Loss: 0.4207. Acc.: 76.64%\nEpoch: 1493. Loss: 0.4053. Acc.: 76.12%\nEpoch: 1494. Loss: 0.4104. Acc.: 74.54%\nEpoch: 1495. Loss: 0.3922. Acc.: 75.46%\nEpoch: 1496. Loss: 0.3853. Acc.: 74.41%\nEpoch: 1497. Loss: 0.4208. Acc.: 75.72%\nEpoch: 1498. Loss: 0.4192. Acc.: 76.64%\nEpoch: 1499. Loss: 0.3957. Acc.: 76.90%\nEpoch: 1500. Loss: 0.4133. Acc.: 75.98%\nEpoch: 1501. Loss: 0.3844. Acc.: 75.59%\nEpoch: 1502. Loss: 0.3998. Acc.: 74.67%\nEpoch: 1503. Loss: 0.4055. Acc.: 76.64%\nEpoch: 1504. Loss: 0.4237. Acc.: 74.67%\nEpoch: 1505. Loss: 0.4093. Acc.: 76.38%\nEpoch: 1506. Loss: 0.3963. Acc.: 76.77%\nEpoch: 1507. Loss: 0.4046. Acc.: 76.90%\nEpoch: 1508. Loss: 0.3961. Acc.: 75.98%\nEpoch: 1509. Loss: 0.4103. Acc.: 75.33%\nEpoch: 1510. Loss: 0.4016. Acc.: 75.85%\nEpoch: 1511. Loss: 0.4027. Acc.: 75.20%\nEpoch: 1512. Loss: 0.4000. Acc.: 75.72%\nEpoch: 1513. Loss: 0.3929. Acc.: 76.90%\nEpoch: 1514. Loss: 0.4211. Acc.: 75.59%\nEpoch: 1515. Loss: 0.3954. Acc.: 75.20%\nEpoch: 1516. Loss: 0.3919. Acc.: 76.51%\nEpoch: 1517. Loss: 0.3905. Acc.: 76.77%\nEpoch: 1518. Loss: 0.3856. Acc.: 77.17%\nEpoch: 1519. Loss: 0.4231. Acc.: 77.69%\nEpoch: 1520. Loss: 0.3966. Acc.: 75.98%\nEpoch: 1521. Loss: 0.3895. Acc.: 76.90%\nEpoch: 1522. Loss: 0.3921. Acc.: 77.30%\nEpoch: 1523. Loss: 0.4100. Acc.: 77.17%\nEpoch: 1524. Loss: 0.4059. Acc.: 77.17%\nEpoch: 1525. Loss: 0.3924. Acc.: 76.77%\nEpoch: 1526. Loss: 0.4090. Acc.: 78.08%\nEpoch: 1527. Loss: 0.3950. Acc.: 76.51%\nEpoch: 1528. Loss: 0.3845. Acc.: 76.64%\nEpoch: 1529. Loss: 0.3925. Acc.: 77.69%\nEpoch: 1530. Loss: 0.4076. Acc.: 76.90%\nEpoch: 1531. Loss: 0.4027. Acc.: 77.03%\nEpoch: 1532. Loss: 0.3918. Acc.: 77.03%\nEpoch: 1533. Loss: 0.4070. Acc.: 75.98%\nEpoch: 1534. Loss: 0.3794. Acc.: 76.90%\nEpoch: 1535. Loss: 0.3949. Acc.: 77.17%\nEpoch: 1536. Loss: 0.4080. Acc.: 77.30%\nEpoch: 1537. Loss: 0.3939. Acc.: 75.85%\nEpoch: 1538. Loss: 0.3875. Acc.: 76.77%\nEpoch: 1539. Loss: 0.3851. Acc.: 75.72%\nEpoch: 1540. Loss: 0.3960. Acc.: 76.77%\nEpoch: 1541. Loss: 0.4058. Acc.: 76.77%\nEpoch: 1542. Loss: 0.4003. Acc.: 75.33%\nEpoch: 1543. Loss: 0.4178. Acc.: 74.67%\nEpoch: 1544. Loss: 0.3944. Acc.: 75.85%\nEpoch: 1545. Loss: 0.3883. Acc.: 76.12%\nEpoch: 1546. Loss: 0.3983. Acc.: 76.90%\nEpoch: 1547. Loss: 0.3914. Acc.: 75.59%\nEpoch: 1548. Loss: 0.4072. Acc.: 75.20%\nEpoch: 1549. Loss: 0.3975. Acc.: 75.07%\nEpoch: 1550. Loss: 0.3937. Acc.: 76.77%\nEpoch: 1551. Loss: 0.4045. Acc.: 76.90%\nEpoch: 1552. Loss: 0.3918. Acc.: 75.72%\nEpoch: 1553. Loss: 0.3922. Acc.: 74.54%\nEpoch: 1554. Loss: 0.3842. Acc.: 75.46%\nEpoch: 1555. Loss: 0.4043. Acc.: 76.90%\nEpoch: 1556. Loss: 0.3912. Acc.: 75.46%\nEpoch: 1557. Loss: 0.4083. Acc.: 76.64%\nEpoch: 1558. Loss: 0.3817. Acc.: 76.12%\nEpoch: 1559. Loss: 0.3883. Acc.: 76.12%\nEpoch: 1560. Loss: 0.4065. Acc.: 74.41%\nEpoch: 1561. Loss: 0.4065. Acc.: 74.80%\nEpoch: 1562. Loss: 0.3867. Acc.: 74.67%\nEpoch: 1563. Loss: 0.3770. Acc.: 76.12%\nEpoch: 1564. Loss: 0.3800. Acc.: 75.46%\nEpoch: 1565. Loss: 0.3994. Acc.: 74.54%\nEpoch: 1566. Loss: 0.4077. Acc.: 77.69%\nEpoch: 1567. Loss: 0.3759. Acc.: 76.64%\nEpoch: 1568. Loss: 0.4113. Acc.: 75.59%\nEpoch: 1569. Loss: 0.3841. Acc.: 74.93%\nEpoch: 1570. Loss: 0.3797. Acc.: 76.25%\nEpoch: 1571. Loss: 0.4173. Acc.: 77.82%\nEpoch: 1572. Loss: 0.3950. Acc.: 75.33%\nEpoch: 1573. Loss: 0.4095. Acc.: 75.85%\nEpoch: 1574. Loss: 0.4050. Acc.: 76.90%\nEpoch: 1575. Loss: 0.3919. Acc.: 75.85%\nEpoch: 1576. Loss: 0.4049. Acc.: 76.12%\nEpoch: 1577. Loss: 0.3965. Acc.: 75.85%\nEpoch: 1578. Loss: 0.3835. Acc.: 75.72%\nEpoch: 1579. Loss: 0.4088. Acc.: 76.38%\nEpoch: 1580. Loss: 0.4209. Acc.: 77.30%\nEpoch: 1581. Loss: 0.3961. Acc.: 76.90%\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 1582. Loss: 0.3881. Acc.: 76.64%\nEpoch: 1583. Loss: 0.3962. Acc.: 76.12%\nEpoch: 1584. Loss: 0.4044. Acc.: 76.51%\nEpoch: 1585. Loss: 0.4230. Acc.: 76.38%\nEpoch: 1586. Loss: 0.4050. Acc.: 76.12%\nEpoch: 1587. Loss: 0.3910. Acc.: 75.33%\nEpoch: 1588. Loss: 0.4108. Acc.: 76.64%\nEpoch: 1589. Loss: 0.4216. Acc.: 75.07%\nEpoch: 1590. Loss: 0.3823. Acc.: 74.54%\nEpoch: 1591. Loss: 0.3909. Acc.: 76.51%\nEpoch: 1592. Loss: 0.4053. Acc.: 77.03%\nEpoch: 1593. Loss: 0.3954. Acc.: 75.98%\nEpoch: 1594. Loss: 0.3964. Acc.: 75.46%\nEpoch: 1595. Loss: 0.3912. Acc.: 75.33%\nEpoch: 1596. Loss: 0.3881. Acc.: 76.25%\nEpoch: 1597. Loss: 0.4032. Acc.: 75.59%\nEpoch: 1598. Loss: 0.4066. Acc.: 75.98%\nEpoch: 1599. Loss: 0.3925. Acc.: 75.07%\nEpoch: 1600. Loss: 0.3849. Acc.: 76.51%\nEpoch: 1601. Loss: 0.3919. Acc.: 75.85%\nEpoch: 1602. Loss: 0.3924. Acc.: 75.72%\nEpoch: 1603. Loss: 0.3952. Acc.: 76.12%\nEpoch: 1604. Loss: 0.3880. Acc.: 76.25%\nEpoch: 1605. Loss: 0.4139. Acc.: 77.30%\nEpoch: 1606. Loss: 0.3922. Acc.: 76.64%\nEpoch: 1607. Loss: 0.4253. Acc.: 77.82%\nEpoch: 1608. Loss: 0.3859. Acc.: 75.85%\nEpoch: 1609. Loss: 0.3830. Acc.: 77.17%\nEpoch: 1610. Loss: 0.3814. Acc.: 77.30%\nEpoch: 1611. Loss: 0.3760. Acc.: 77.69%\nEpoch: 1612. Loss: 0.3925. Acc.: 76.51%\nEpoch: 1613. Loss: 0.3974. Acc.: 76.51%\nEpoch: 1614. Loss: 0.4255. Acc.: 77.82%\nEpoch: 1615. Loss: 0.3934. Acc.: 75.98%\nEpoch: 1616. Loss: 0.4172. Acc.: 77.82%\nEpoch: 1617. Loss: 0.4110. Acc.: 75.85%\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"5\">**Compute result on testing data**</font><br>"},{"metadata":{"_uuid":"d0f483d5-5542-4d65-a70f-1aa7b3e13c3c","_cell_guid":"05b2b8d9-2b63-436e-a906-a839fb008c0e","trusted":true},"cell_type":"code","source":"test_results = []\nmodel.load_state_dict(torch.load('best.pth'))\nmodel.eval()\n\n# for batch in val_dl:\n#     x_raw, y_batch = [t.to(device) for t in batch]\n#     out = model(x_raw)\n#     preds = F.log_softmax(out, dim=1).argmax(dim=1)\n#     total += y_batch.size(0)\n#     correct += (preds == y_batch).sum().item()\n#     acc = correct / total\n#     acc_history.append(acc)\n\n# for batch in tst_dl:\n#     x, y_batche = [t.to(device) for t in (batch)]\n#     out = model(x)\n#     y_hat = F.log_softmax(out, dim=1).argmax(dim=1)\n#     test_results.extend(y_hat.tolist())\n    \nfor batch in tst_dl:\n        x_raw, y_batch = [t.to(device) for t in batch]\n        out = model(x_channel_0, x_channel_1, x_channel_2, x_channel_3, x_channel_4, x_channel_5, x_channel_6, x_channel_7, x_channel_8)\n        preds = F.log_softmax(out, dim=1).argmax(dim=1)\n        total += y_batch.size(0)\n        correct += (preds == y_batch).sum().item()\n    \nacc = correct / total\nprint(\"accurancy on test data: \"+str(acc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"5\">**Store final result**</font><br>"},{"metadata":{"_uuid":"7a49a38e-e203-4884-ba78-33fe8aa02426","_cell_guid":"f4bd2628-c801-42d5-9af6-11bbd89f2352","trusted":true},"cell_type":"code","source":"'''\nsubmit = pd.read_csv(sample)\nenc = joblib.load(ROOT/'encoder.model')\nsubmit['surface'] = enc.inverse_transform(test_results)\nsubmit.to_csv('submit_base.csv', index=None)\nprint(\"store result successfully!\")\n'''","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}