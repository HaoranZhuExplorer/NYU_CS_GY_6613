{"cells":[{"metadata":{},"cell_type":"markdown","source":"<font size=\"5\">**Project Summary**</font><br>\nIn this competition, the main task is to do surface time series classification. 1d convolution is a good technique when doing the time series classification job. So I apply 1d convolutional neural network to solve this problem. The whole code is written in Pytorch."},{"metadata":{},"cell_type":"markdown","source":"<font size=\"5\">**External libs we are using**</font><br>"},{"metadata":{"_uuid":"a6ed998e-ee9a-4333-8332-8fa753b7f0d4","_cell_guid":"e6e0bc2b-9cba-4688-9e0b-15f11d219c2d","trusted":true},"cell_type":"code","source":"# libs we are using\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport torch\nfrom torch import nn\nfrom torch import optim\nfrom torch.nn import functional as F\nfrom torch.utils.data import TensorDataset, DataLoader","execution_count":85,"outputs":[]},{"metadata":{"_uuid":"b8906e48-2384-4bb3-bba7-722160428e39","_cell_guid":"295d38d1-6663-4575-93ba-c6cfac9f6129","trusted":true},"cell_type":"markdown","source":"<font size=\"5\">**Data preparations**</font><br>\n\nIn this project, I use the raw data as input of the network. I concatenated all datasets into one single numpy array. The first 3810 rows are training data with given labels, the rest 3816 rows are testing data where labels are not given. In total raw data has 7816 rows. In addition to that, we split the raw training data into training data and validation data by the ratio of 80/20.\n\n\nI create three datasets and data loaders for them to make the data ready for model's training. The process is straightforward. I split the labelled data into two subsets, and keep testing data as is. Also, I convert Numpy arrays into `torch.tensor` objects of proper type (float for samples, and long - for targets).\n"},{"metadata":{"_uuid":"94c41c8d-0c91-4aa1-83a4-ff3f518f35f4","_cell_guid":"88b9ec00-fc64-4040-896b-895d06cd58f2","trusted":true},"cell_type":"code","source":"def create_datasets(data, target, train_size, valid_pct=0.2, seed=None):\n    \"\"\"Converts NumPy arrays into PyTorch datsets.\n    \n    Three datasets are created in total:\n        * training dataset\n        * validation dataset\n        * testing (un-labelled) dataset\n\n    \"\"\"\n    raw =  data\n    sz = train_size\n    idx = np.arange(sz)\n    trn_idx, val_idx = train_test_split(\n        idx, test_size=valid_pct, random_state=seed)\n     \n    trn_ds = TensorDataset(\n        torch.tensor(raw[:sz][trn_idx]).float(), \n        torch.tensor(target[:sz][trn_idx]).long())\n    val_ds = TensorDataset(\n        torch.tensor(raw[:sz][val_idx]).float(), \n        torch.tensor(target[:sz][val_idx]).long())\n    tst_ds = TensorDataset(\n        torch.tensor(raw[sz:]).float(), \n        torch.tensor(target[sz:]).long())\n    return trn_ds, val_ds, tst_ds\n\n# We use this function to divide the data\ndef create_datasets2(data_arr, train_size, valid_pct=0.2, seed=None):\n    \"\"\"Converts NumPy arrays into PyTorch datsets.\n    \n    Three datasets are created in total:\n        * training dataset\n        * validation dataset\n        * testing (un-labelled) dataset\n\n    \"\"\"\n    raw =  data_arr\n    sz = train_size\n\n    idx = np.arange(sz)\n    print(\"total data length\")\n    print(len(idx))\n    trn_idx, tst_idx = train_test_split(idx, test_size=0.2, random_state=1)\n    trn_idx, val_idx = train_test_split(trn_idx, test_size=0.25, random_state=1)\n    \n    print(\"input one dimension shape\")\n    print(raw[0].shape)\n    print(\"training data length\")\n    print(len(trn_idx))\n    print(\"validation data le\")\n    print(len(val_idx))\n    print(\"testing d\")\n    print(len(tst_idx))\n    \n    trn_ds = TensorDataset(\n        torch.tensor(raw[0][:sz][trn_idx]).float(),\n        torch.tensor(raw[1][:sz][trn_idx]).float(),\n        torch.tensor(raw[2][:sz][trn_idx]).float(),\n        torch.tensor(raw[3][:sz][trn_idx]).float(),\n        torch.tensor(raw[4][:sz][trn_idx]).float(),\n        torch.tensor(raw[5][:sz][trn_idx]).float(),\n        torch.tensor(raw[6][:sz][trn_idx]).float(),\n        torch.tensor(raw[7][:sz][trn_idx]).float(),\n        torch.tensor(raw[8][:sz][trn_idx]).float(),\n        torch.tensor(target[:sz][trn_idx]).long())\n    \n    val_ds = TensorDataset(\n        torch.tensor(raw[0][:sz][val_idx]).float(),\n        torch.tensor(raw[1][:sz][val_idx]).float(), \n        torch.tensor(raw[2][:sz][val_idx]).float(), \n        torch.tensor(raw[3][:sz][val_idx]).float(), \n        torch.tensor(raw[4][:sz][val_idx]).float(), \n        torch.tensor(raw[5][:sz][val_idx]).float(), \n        torch.tensor(raw[6][:sz][val_idx]).float(), \n        torch.tensor(raw[7][:sz][val_idx]).float(), \n        torch.tensor(raw[8][:sz][val_idx]).float(), \n        torch.tensor(target[:sz][val_idx]).long())\n    \n    tst_ds = TensorDataset(\n        torch.tensor(raw[0][:sz][tst_idx]).float(),\n        torch.tensor(raw[1][:sz][tst_idx]).float(),\n        torch.tensor(raw[2][:sz][tst_idx]).float(),\n        torch.tensor(raw[3][:sz][tst_idx]).float(),\n        torch.tensor(raw[4][:sz][tst_idx]).float(),\n        torch.tensor(raw[5][:sz][tst_idx]).float(),\n        torch.tensor(raw[6][:sz][tst_idx]).float(),\n        torch.tensor(raw[7][:sz][tst_idx]).float(),\n        torch.tensor(raw[8][:sz][tst_idx]).float(),\n        torch.tensor(target[:sz][tst_idx]).long())\n    \n    return trn_ds, val_ds, tst_ds\n\ndef create_loaders(data, bs=128, jobs=0):\n    \"\"\"Wraps the datasets returned by create_datasets function with data loaders.\"\"\"\n    \n    trn_ds, val_ds, tst_ds = data\n    trn_dl = DataLoader(trn_ds, batch_size=bs, shuffle=True, num_workers=jobs)\n    val_dl = DataLoader(val_ds, batch_size=bs, shuffle=False, num_workers=jobs)\n    tst_dl = DataLoader(tst_ds, batch_size=bs, shuffle=False, num_workers=jobs)\n    return trn_dl, val_dl, tst_dl","execution_count":86,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"5\">**Neural Network Architecture**</font><br>"},{"metadata":{"_uuid":"f144e459-53bb-4b5a-860d-abec117fc2cc","_cell_guid":"07870588-c88f-42ff-b782-5ae3c5e6e568","trusted":true},"cell_type":"code","source":"class Surface_Classifier(nn.Module):\n    def __init__(self, raw_ni, no, drop=.5):\n        super().__init__()\n        \n        self.conv1d_channel_0 = nn.Sequential(\n            nn.Conv1d(1, 16, 7, 2, 3),\n            nn.BatchNorm1d(16),\n            nn.PReLU(),\n            nn.Dropout(),\n            nn.Conv1d(    16,  64, 8, 4, 2),\n            nn.BatchNorm1d(64),\n            nn.PReLU(),\n            nn.Dropout(),\n            nn.Conv1d(    64, 128, 8, 4, 2),\n            nn.BatchNorm1d(128),\n            nn.PReLU(),\n            nn.Dropout(),\n            nn.Conv1d(   128, 256, 8, 4, 2),\n            nn.BatchNorm1d(256),\n            nn.PReLU(),\n            nn.Dropout()\n        )\n        \n        self.conv1d_channel_1 = nn.Sequential(\n            nn.Conv1d(1, 16, 7, 2, 3),\n            nn.BatchNorm1d(16),\n            nn.PReLU(),\n            nn.Dropout(),\n            nn.Conv1d(    16,  64, 8, 4, 2),\n            nn.BatchNorm1d(64),\n            nn.PReLU(),\n            nn.Dropout(),\n            nn.Conv1d(    64, 128, 8, 4, 2),\n            nn.BatchNorm1d(128),\n            nn.PReLU(),\n            nn.Dropout(),\n            nn.Conv1d(   128, 256, 8, 4, 2),\n            nn.BatchNorm1d(256),\n            nn.PReLU(),\n            nn.Dropout()\n        )\n        \n        self.conv1d_channel_2 = nn.Sequential(\n            nn.Conv1d(1, 16, 7, 2, 3),\n            nn.BatchNorm1d(16),\n            nn.PReLU(),\n            nn.Dropout(),\n            nn.Conv1d(    16,  64, 8, 4, 2),\n            nn.BatchNorm1d(64),\n            nn.PReLU(),\n            nn.Dropout(),\n            nn.Conv1d(    64, 128, 8, 4, 2),\n            nn.BatchNorm1d(128),\n            nn.PReLU(),\n            nn.Dropout(),\n            nn.Conv1d(   128, 256, 8, 4, 2),\n            nn.BatchNorm1d(256),\n            nn.PReLU(),\n            nn.Dropout()\n        )\n        \n        self.conv1d_channel_3 = nn.Sequential(\n            nn.Conv1d(1, 16, 7, 2, 3),\n            nn.BatchNorm1d(16),\n            nn.PReLU(),\n            nn.Dropout(),\n            nn.Conv1d(    16,  64, 8, 4, 2),\n            nn.BatchNorm1d(64),\n            nn.PReLU(),\n            nn.Dropout(),\n            nn.Conv1d(    64, 128, 8, 4, 2),\n            nn.BatchNorm1d(128),\n            nn.PReLU(),\n            nn.Dropout(),\n            nn.Conv1d(   128, 256, 8, 4, 2),\n            nn.BatchNorm1d(256),\n            nn.PReLU(),\n            nn.Dropout()\n        )\n        \n        self.conv1d_channel_4 = nn.Sequential(\n            nn.Conv1d(1, 16, 7, 2, 3),\n            nn.BatchNorm1d(16),\n            nn.PReLU(),\n            nn.Dropout(),\n            nn.Conv1d(    16,  64, 8, 4, 2),\n            nn.BatchNorm1d(64),\n            nn.PReLU(),\n            nn.Dropout(),\n            nn.Conv1d(    64, 128, 8, 4, 2),\n            nn.BatchNorm1d(128),\n            nn.PReLU(),\n            nn.Dropout(),\n            nn.Conv1d(   128, 256, 8, 4, 2),\n            nn.BatchNorm1d(256),\n            nn.PReLU(),\n            nn.Dropout()\n        )\n        \n        self.conv1d_channel_5 = nn.Sequential(\n            nn.Conv1d(1, 16, 7, 2, 3),\n            nn.BatchNorm1d(16),\n            nn.PReLU(),\n            nn.Dropout(),\n            nn.Conv1d(    16,  64, 8, 4, 2),\n            nn.BatchNorm1d(64),\n            nn.PReLU(),\n            nn.Dropout(),\n            nn.Conv1d(    64, 128, 8, 4, 2),\n            nn.BatchNorm1d(128),\n            nn.PReLU(),\n            nn.Dropout(),\n            nn.Conv1d(   128, 256, 8, 4, 2),\n            nn.BatchNorm1d(256),\n            nn.PReLU(),\n            nn.Dropout()\n        )\n        \n        self.conv1d_channel_6 = nn.Sequential(\n            nn.Conv1d(1, 16, 7, 2, 3),\n            nn.BatchNorm1d(16),\n            nn.PReLU(),\n            nn.Dropout(),\n            nn.Conv1d(    16,  64, 8, 4, 2),\n            nn.BatchNorm1d(64),\n            nn.PReLU(),\n            nn.Dropout(),\n            nn.Conv1d(    64, 128, 8, 4, 2),\n            nn.BatchNorm1d(128),\n            nn.PReLU(),\n            nn.Dropout(),\n            nn.Conv1d(   128, 256, 8, 4, 2),\n            nn.BatchNorm1d(256),\n            nn.PReLU(),\n            nn.Dropout()\n        )\n        \n        self.conv1d_channel_7 = nn.Sequential(\n            nn.Conv1d(1, 16, 7, 2, 3),\n            nn.BatchNorm1d(16),\n            nn.PReLU(),\n            nn.Dropout(),\n            nn.Conv1d(    16,  64, 8, 4, 2),\n            nn.BatchNorm1d(64),\n            nn.PReLU(),\n            nn.Dropout(),\n            nn.Conv1d(    64, 128, 8, 4, 2),\n            nn.BatchNorm1d(128),\n            nn.PReLU(),\n            nn.Dropout(),\n            nn.Conv1d(   128, 256, 8, 4, 2),\n            nn.BatchNorm1d(256),\n            nn.PReLU(),\n            nn.Dropout()\n        )\n        \n        self.conv1d_channel_8 = nn.Sequential(\n            nn.Conv1d(1, 16, 7, 2, 3),\n            nn.BatchNorm1d(16),\n            nn.PReLU(),\n            nn.Dropout(),\n            nn.Conv1d(    16,  64, 8, 4, 2),\n            nn.BatchNorm1d(64),\n            nn.PReLU(),\n            nn.Dropout(),\n            nn.Conv1d(    64, 128, 8, 4, 2),\n            nn.BatchNorm1d(128),\n            nn.PReLU(),\n            nn.Dropout(),\n            nn.Conv1d(   128, 256, 8, 4, 2),\n            nn.BatchNorm1d(256),\n            nn.PReLU(),\n            nn.Dropout()\n        )\n        \n        self.dense = nn.Sequential(\n            nn.Linear(2304, 128), nn.ReLU(), nn.Dropout(),\n            nn.Linear(128, 64), nn.Softmax(dim=1), \n            nn.Linear(64, no)\n        )\n            \n\n    def forward(self, t_channel_0, t_channel_1, t_channel_2, t_channel_3, t_channel_4, t_channel_5, t_channel_6, t_channel_7, t_channel_8):\n        conv1d_out_channel_0 = self.conv1d_channel_0(t_channel_0)\n        conv1d_out_channel_1 = self.conv1d_channel_1(t_channel_1)\n        conv1d_out_channel_2 = self.conv1d_channel_2(t_channel_2)\n        conv1d_out_channel_3 = self.conv1d_channel_3(t_channel_3)\n        conv1d_out_channel_4 = self.conv1d_channel_4(t_channel_4)\n        conv1d_out_channel_5 = self.conv1d_channel_5(t_channel_5)\n        conv1d_out_channel_6 = self.conv1d_channel_6(t_channel_6)\n        conv1d_out_channel_7 = self.conv1d_channel_7(t_channel_7)\n        conv1d_out_channel_8 = self.conv1d_channel_8(t_channel_8)\n        \n        t_in = torch.cat([conv1d_out_channel_0,conv1d_out_channel_1, conv1d_out_channel_2, conv1d_out_channel_3, conv1d_out_channel_4, conv1d_out_channel_5, conv1d_out_channel_6, conv1d_out_channel_7, conv1d_out_channel_8], dim=1)\n        res = t_in.view(t_in.size(0), -1)\n       \n        out = self.dense(res)\n        return out\n        ","execution_count":87,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"5\">**Initial setup for random seed and project path**</font><br>"},{"metadata":{"_uuid":"b4b74c47-df3d-4201-93a7-f8cb79d6a652","_cell_guid":"aa2ea0fc-dfc2-436e-9a73-aab6bfb63d20","trusted":true},"cell_type":"code","source":"\n# set up the seed\nseed = 1\nnp.random.seed(seed)\n\n# path to sample submission\nsample = Path.cwd().parent/'input'/'career-con-2019'/'sample_submission.csv'\n\nROOT = Path.cwd().parent/'input'/'career-con-2019-preprocessed-data'\nraw_arr = np.load(ROOT/'feat.npy').transpose(0, 2, 1)\ntarget = np.load(ROOT/'target.npy')\n\nraw_dim_data = [None]*9\n#print(raw_dim_data)\nfor i in range(0, 9):\n    raw_dim_data[i] = raw_arr[:,i,:]\n#    print(\"raw data shape\")\n    \n    raw_dim_data[i] = raw_dim_data[i].reshape([7626,1,128])\n#    print(raw_dim_data[i].shape)\n    \n# print(\"raw array shape\")\n# print(raw_arr.shape)\n# print(\"label array shape\")\n# print(target.shape)\n\ntrn_sz = 3810  # only the first `trn_sz` rows in each array include labelled data\n#datasets = create_datasets((raw_arr), target, trn_sz, seed=seed)\ndatasets = create_datasets2((raw_dim_data), trn_sz, seed=seed)\n\n\n# make sure that we run on a proper device (not relevant for Kaggle kernels but helpful in Jupyter sessions)\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\n\ntrn_dl, val_dl, tst_dl = create_loaders(datasets, bs=256)","execution_count":88,"outputs":[{"output_type":"stream","text":"total data length\n3810\ninput one dimension shape\n(7626, 1, 128)\ntraining data length\n2286\nvalidation data le\n762\ntesting d\n762\n","name":"stdout"}]},{"metadata":{"_uuid":"b9026a32-1355-4689-8bf9-581a5228af69","_cell_guid":"5db66d39-80aa-48f4-9aab-4f44eace0192","trusted":true},"cell_type":"markdown","source":"<font size=\"5\">**Training model**</font><br>\nNow everything is ready to create a training loop and see if our model works. For each training epoch, the loop performs the following actions:\n1. train model on the `trn_ds` dataset;\n2. verify quality on the `val_ds` dataset;\n3. check if the quality improved since previous epoch, and if so, save the model's weights onto disk;\n4. in case if the model's quality isn't impoving for `patience` epochs, the training is stopped.\nAlso, the code tracks loss and accuracy history, and prints current scores with exponentially increasing logging frequency, i.e., only at 1, 2, 4, 8... epochs."},{"metadata":{"_uuid":"dee51d07-1cff-4db3-a87d-bebc4bf5bcc0","_cell_guid":"72095ada-fe94-4757-9a8a-918481b04336","trusted":true},"cell_type":"code","source":"raw_feat = raw_arr.shape[1]\n\n\nlr = 0.001\nn_epochs = 10000\niterations_per_epoch = len(trn_dl)\nnum_classes = 9\nbest_acc = 0\npatience, trials = 500, 0\nbase = 1\nstep = 2\nloss_history = []\nacc_history = []\n\nmodel = Surface_Classifier(raw_feat, num_classes).to(device)\nmodel.cuda()\ncriterion = nn.CrossEntropyLoss(reduction='sum')\nopt = optim.Adam(model.parameters(), lr=lr)\n\nprint('Start model training')\n\nfor epoch in range(1, n_epochs + 1):\n    \n    model.train()\n    epoch_loss = 0\n    for i, batch in enumerate(trn_dl):\n        #x_raw, y_batch = [t.to(device) for t in batch]\n        x_channel_0, x_channel_1,  x_channel_2,  x_channel_3,  x_channel_4,  x_channel_5,  x_channel_6,  x_channel_7,  x_channel_8, y_batch = [t.to(device) for t in batch]\n        opt.zero_grad()\n        \n#         print(\"channel 0 type\")\n#         print(type(x_channel_0))\n#         print(\"channel 0 shape\")\n#         print(x_channel_0.shape)\n#         print(\"batch type\")\n#         print(type(batch))\n#         print(len(batch))\n#         print(batch[0].shape)\n#         print(batch[9].shape)\n\n        out = model(x_channel_0, x_channel_1, x_channel_2, x_channel_3, x_channel_4, x_channel_5, x_channel_6, x_channel_7, x_channel_8)\n        \n        \n#         out = []\n#         with torch.no_grad():\n#             for x_channel_0, x_channel_1,  x_channel_2,  x_channel_3,  x_channel_4,  x_channel_5,  x_channel_6,  x_channel_7,  x_channel_8, y_batch in batch:\n#                 output = model(x_channel_0, x_channel_1, x_channel_2, x_channel_3, x_channel_4, x_channel_5, x_channel_6, x_channel_7, x_channel_8)\n#                 out.append(output.detach())\n#         out = torch.cat(out)\n        \n\n    \n        loss = criterion(out, y_batch)\n        epoch_loss += loss.item()\n        loss.backward()\n        opt.step()\n        \n    epoch_loss /= trn_sz\n    loss_history.append(epoch_loss)\n    \n    model.eval()\n    correct, total = 0, 0\n    \n    for batch in val_dl:\n        x_channel_0, x_channel_1,  x_channel_2,  x_channel_3,  x_channel_4,  x_channel_5,  x_channel_6,  x_channel_7,  x_channel_8, y_batch = [t.to(device) for t in batch]\n        \n        out = model(x_channel_0, x_channel_1, x_channel_2, x_channel_3, x_channel_4, x_channel_5, x_channel_6, x_channel_7, x_channel_8)\n        preds = F.log_softmax(out, dim=1).argmax(dim=1)\n        total += y_batch.size(0)\n        correct += (preds == y_batch).sum().item()\n    \n    acc = correct / total\n    acc_history.append(acc)\n\n    if epoch % base == 0:\n        print(f'Epoch: {epoch:3d}. Loss: {epoch_loss:.4f}. Acc.: {acc:2.2%}')\n        base *= step\n    print(f'Epoch: {epoch:3d}. Loss: {epoch_loss:.4f}. Acc.: {acc:2.2%}')\n\n    \n    if acc > best_acc:\n        trials = 0\n        best_acc = acc\n        torch.save(model.state_dict(), 'best.pth')\n        print(f'Epoch {epoch} best model saved with accuracy: {best_acc:2.2%}')\n    else:\n        trials += 1\n        if trials >= patience:\n            print(f'Early stopping on epoch {epoch}')\n            break\n            \nprint('Done!')","execution_count":89,"outputs":[{"output_type":"stream","text":"Start model training\nEpoch:   1. Loss: 1.3149. Acc.: 15.09%\nEpoch:   1. Loss: 1.3149. Acc.: 15.09%\nEpoch 1 best model saved with accuracy: 15.09%\nEpoch:   2. Loss: 1.2988. Acc.: 15.09%\nEpoch:   2. Loss: 1.2988. Acc.: 15.09%\nEpoch:   3. Loss: 1.2833. Acc.: 16.27%\nEpoch 3 best model saved with accuracy: 16.27%\nEpoch:   4. Loss: 1.2714. Acc.: 17.72%\nEpoch:   4. Loss: 1.2714. Acc.: 17.72%\nEpoch 4 best model saved with accuracy: 17.72%\nEpoch:   5. Loss: 1.2627. Acc.: 20.60%\nEpoch 5 best model saved with accuracy: 20.60%\nEpoch:   6. Loss: 1.2548. Acc.: 21.65%\nEpoch 6 best model saved with accuracy: 21.65%\nEpoch:   7. Loss: 1.2480. Acc.: 22.05%\nEpoch 7 best model saved with accuracy: 22.05%\nEpoch:   8. Loss: 1.2408. Acc.: 22.18%\nEpoch:   8. Loss: 1.2408. Acc.: 22.18%\nEpoch 8 best model saved with accuracy: 22.18%\nEpoch:   9. Loss: 1.2335. Acc.: 22.31%\nEpoch 9 best model saved with accuracy: 22.31%\nEpoch:  10. Loss: 1.2270. Acc.: 22.18%\nEpoch:  11. Loss: 1.2214. Acc.: 22.97%\nEpoch 11 best model saved with accuracy: 22.97%\nEpoch:  12. Loss: 1.2153. Acc.: 23.36%\nEpoch 12 best model saved with accuracy: 23.36%\nEpoch:  13. Loss: 1.2094. Acc.: 23.62%\nEpoch 13 best model saved with accuracy: 23.62%\nEpoch:  14. Loss: 1.2047. Acc.: 21.39%\nEpoch:  15. Loss: 1.1981. Acc.: 26.90%\nEpoch 15 best model saved with accuracy: 26.90%\nEpoch:  16. Loss: 1.1918. Acc.: 26.90%\nEpoch:  16. Loss: 1.1918. Acc.: 26.90%\nEpoch:  17. Loss: 1.1862. Acc.: 25.98%\nEpoch:  18. Loss: 1.1806. Acc.: 26.38%\nEpoch:  19. Loss: 1.1753. Acc.: 27.17%\nEpoch 19 best model saved with accuracy: 27.17%\nEpoch:  20. Loss: 1.1709. Acc.: 29.00%\nEpoch 20 best model saved with accuracy: 29.00%\nEpoch:  21. Loss: 1.1652. Acc.: 27.17%\nEpoch:  22. Loss: 1.1609. Acc.: 28.08%\nEpoch:  23. Loss: 1.1575. Acc.: 29.13%\nEpoch 23 best model saved with accuracy: 29.13%\nEpoch:  24. Loss: 1.1527. Acc.: 29.79%\nEpoch 24 best model saved with accuracy: 29.79%\nEpoch:  25. Loss: 1.1473. Acc.: 31.36%\nEpoch 25 best model saved with accuracy: 31.36%\nEpoch:  26. Loss: 1.1427. Acc.: 32.81%\nEpoch 26 best model saved with accuracy: 32.81%\nEpoch:  27. Loss: 1.1394. Acc.: 30.18%\nEpoch:  28. Loss: 1.1341. Acc.: 30.58%\nEpoch:  29. Loss: 1.1304. Acc.: 29.92%\nEpoch:  30. Loss: 1.1248. Acc.: 31.10%\nEpoch:  31. Loss: 1.1207. Acc.: 31.50%\nEpoch:  32. Loss: 1.1181. Acc.: 32.28%\nEpoch:  32. Loss: 1.1181. Acc.: 32.28%\nEpoch:  33. Loss: 1.1129. Acc.: 31.63%\nEpoch:  34. Loss: 1.1100. Acc.: 32.41%\nEpoch:  35. Loss: 1.1076. Acc.: 32.02%\nEpoch:  36. Loss: 1.1031. Acc.: 32.68%\nEpoch:  37. Loss: 1.1007. Acc.: 33.20%\nEpoch 37 best model saved with accuracy: 33.20%\nEpoch:  38. Loss: 1.0965. Acc.: 33.99%\nEpoch 38 best model saved with accuracy: 33.99%\nEpoch:  39. Loss: 1.0923. Acc.: 33.99%\nEpoch:  40. Loss: 1.0875. Acc.: 33.20%\nEpoch:  41. Loss: 1.0837. Acc.: 34.12%\nEpoch 41 best model saved with accuracy: 34.12%\nEpoch:  42. Loss: 1.0808. Acc.: 33.60%\nEpoch:  43. Loss: 1.0783. Acc.: 33.60%\nEpoch:  44. Loss: 1.0780. Acc.: 36.61%\nEpoch 44 best model saved with accuracy: 36.61%\nEpoch:  45. Loss: 1.0696. Acc.: 35.30%\nEpoch:  46. Loss: 1.0667. Acc.: 34.65%\nEpoch:  47. Loss: 1.0714. Acc.: 34.91%\nEpoch:  48. Loss: 1.0622. Acc.: 36.48%\nEpoch:  49. Loss: 1.0611. Acc.: 35.56%\nEpoch:  50. Loss: 1.0557. Acc.: 35.43%\nEpoch:  51. Loss: 1.0549. Acc.: 36.61%\nEpoch:  52. Loss: 1.0480. Acc.: 36.09%\nEpoch:  53. Loss: 1.0486. Acc.: 36.35%\nEpoch:  54. Loss: 1.0446. Acc.: 36.61%\nEpoch:  55. Loss: 1.0437. Acc.: 36.61%\nEpoch:  56. Loss: 1.0425. Acc.: 36.09%\nEpoch:  57. Loss: 1.0345. Acc.: 36.61%\nEpoch:  58. Loss: 1.0344. Acc.: 36.22%\nEpoch:  59. Loss: 1.0337. Acc.: 36.35%\nEpoch:  60. Loss: 1.0248. Acc.: 35.04%\nEpoch:  61. Loss: 1.0260. Acc.: 35.96%\nEpoch:  62. Loss: 1.0213. Acc.: 35.17%\nEpoch:  63. Loss: 1.0217. Acc.: 35.83%\nEpoch:  64. Loss: 1.0199. Acc.: 36.61%\nEpoch:  64. Loss: 1.0199. Acc.: 36.61%\nEpoch:  65. Loss: 1.0174. Acc.: 36.35%\nEpoch:  66. Loss: 1.0114. Acc.: 37.66%\nEpoch 66 best model saved with accuracy: 37.66%\nEpoch:  67. Loss: 1.0096. Acc.: 38.19%\nEpoch 67 best model saved with accuracy: 38.19%\nEpoch:  68. Loss: 1.0080. Acc.: 38.45%\nEpoch 68 best model saved with accuracy: 38.45%\nEpoch:  69. Loss: 1.0075. Acc.: 38.58%\nEpoch 69 best model saved with accuracy: 38.58%\nEpoch:  70. Loss: 1.0062. Acc.: 37.66%\nEpoch:  71. Loss: 1.0049. Acc.: 39.11%\nEpoch 71 best model saved with accuracy: 39.11%\nEpoch:  72. Loss: 0.9985. Acc.: 38.58%\nEpoch:  73. Loss: 1.0005. Acc.: 36.61%\nEpoch:  74. Loss: 1.0003. Acc.: 37.40%\nEpoch:  75. Loss: 0.9935. Acc.: 36.35%\nEpoch:  76. Loss: 0.9914. Acc.: 38.85%\nEpoch:  77. Loss: 0.9879. Acc.: 39.63%\nEpoch 77 best model saved with accuracy: 39.63%\nEpoch:  78. Loss: 0.9866. Acc.: 38.06%\nEpoch:  79. Loss: 0.9859. Acc.: 38.06%\nEpoch:  80. Loss: 0.9865. Acc.: 37.66%\nEpoch:  81. Loss: 0.9836. Acc.: 38.45%\nEpoch:  82. Loss: 0.9870. Acc.: 37.93%\nEpoch:  83. Loss: 0.9775. Acc.: 37.93%\nEpoch:  84. Loss: 0.9743. Acc.: 38.32%\nEpoch:  85. Loss: 0.9789. Acc.: 37.66%\nEpoch:  86. Loss: 0.9729. Acc.: 37.40%\nEpoch:  87. Loss: 0.9687. Acc.: 39.37%\nEpoch:  88. Loss: 0.9707. Acc.: 37.53%\nEpoch:  89. Loss: 0.9706. Acc.: 39.24%\nEpoch:  90. Loss: 0.9697. Acc.: 36.61%\nEpoch:  91. Loss: 0.9634. Acc.: 39.76%\nEpoch 91 best model saved with accuracy: 39.76%\nEpoch:  92. Loss: 0.9644. Acc.: 38.85%\nEpoch:  93. Loss: 0.9639. Acc.: 38.19%\nEpoch:  94. Loss: 0.9593. Acc.: 38.45%\nEpoch:  95. Loss: 0.9501. Acc.: 39.76%\nEpoch:  96. Loss: 0.9539. Acc.: 40.29%\nEpoch 96 best model saved with accuracy: 40.29%\nEpoch:  97. Loss: 0.9504. Acc.: 38.98%\nEpoch:  98. Loss: 0.9470. Acc.: 39.90%\nEpoch:  99. Loss: 0.9525. Acc.: 39.90%\nEpoch: 100. Loss: 0.9516. Acc.: 39.11%\nEpoch: 101. Loss: 0.9438. Acc.: 39.63%\nEpoch: 102. Loss: 0.9452. Acc.: 37.93%\nEpoch: 103. Loss: 0.9443. Acc.: 38.85%\nEpoch: 104. Loss: 0.9413. Acc.: 40.03%\nEpoch: 105. Loss: 0.9404. Acc.: 39.90%\nEpoch: 106. Loss: 0.9400. Acc.: 40.03%\nEpoch: 107. Loss: 0.9397. Acc.: 39.37%\nEpoch: 108. Loss: 0.9336. Acc.: 39.63%\nEpoch: 109. Loss: 0.9294. Acc.: 40.55%\nEpoch 109 best model saved with accuracy: 40.55%\nEpoch: 110. Loss: 0.9261. Acc.: 41.21%\nEpoch 110 best model saved with accuracy: 41.21%\nEpoch: 111. Loss: 0.9277. Acc.: 40.55%\nEpoch: 112. Loss: 0.9290. Acc.: 40.68%\nEpoch: 113. Loss: 0.9216. Acc.: 40.68%\nEpoch: 114. Loss: 0.9184. Acc.: 40.29%\nEpoch: 115. Loss: 0.9201. Acc.: 40.03%\nEpoch: 116. Loss: 0.9183. Acc.: 39.76%\nEpoch: 117. Loss: 0.9134. Acc.: 40.68%\nEpoch: 118. Loss: 0.9182. Acc.: 40.55%\nEpoch: 119. Loss: 0.9066. Acc.: 40.94%\nEpoch: 120. Loss: 0.9104. Acc.: 40.94%\nEpoch: 121. Loss: 0.9142. Acc.: 40.55%\nEpoch: 122. Loss: 0.9047. Acc.: 40.42%\nEpoch: 123. Loss: 0.9062. Acc.: 40.68%\nEpoch: 124. Loss: 0.9038. Acc.: 40.29%\nEpoch: 125. Loss: 0.9002. Acc.: 40.94%\nEpoch: 126. Loss: 0.9030. Acc.: 41.08%\nEpoch: 127. Loss: 0.9036. Acc.: 40.42%\nEpoch: 128. Loss: 0.8974. Acc.: 40.55%\nEpoch: 128. Loss: 0.8974. Acc.: 40.55%\nEpoch: 129. Loss: 0.9007. Acc.: 40.68%\nEpoch: 130. Loss: 0.8951. Acc.: 41.08%\nEpoch: 131. Loss: 0.8934. Acc.: 41.08%\nEpoch: 132. Loss: 0.8915. Acc.: 40.94%\nEpoch: 133. Loss: 0.8924. Acc.: 41.34%\nEpoch 133 best model saved with accuracy: 41.34%\nEpoch: 134. Loss: 0.8906. Acc.: 40.94%\nEpoch: 135. Loss: 0.8868. Acc.: 40.81%\nEpoch: 136. Loss: 0.8872. Acc.: 40.55%\nEpoch: 137. Loss: 0.8897. Acc.: 40.42%\nEpoch: 138. Loss: 0.8857. Acc.: 40.29%\nEpoch: 139. Loss: 0.8793. Acc.: 40.81%\nEpoch: 140. Loss: 0.8831. Acc.: 39.90%\nEpoch: 141. Loss: 0.8785. Acc.: 40.94%\nEpoch: 142. Loss: 0.8750. Acc.: 41.08%\nEpoch: 143. Loss: 0.8747. Acc.: 41.34%\nEpoch: 144. Loss: 0.8681. Acc.: 41.60%\nEpoch 144 best model saved with accuracy: 41.60%\nEpoch: 145. Loss: 0.8728. Acc.: 40.55%\nEpoch: 146. Loss: 0.8755. Acc.: 40.42%\nEpoch: 147. Loss: 0.8701. Acc.: 40.42%\nEpoch: 148. Loss: 0.8652. Acc.: 40.68%\nEpoch: 149. Loss: 0.8663. Acc.: 40.94%\nEpoch: 150. Loss: 0.8647. Acc.: 40.94%\nEpoch: 151. Loss: 0.8670. Acc.: 40.94%\nEpoch: 152. Loss: 0.8655. Acc.: 40.29%\nEpoch: 153. Loss: 0.8678. Acc.: 40.68%\nEpoch: 154. Loss: 0.8640. Acc.: 40.81%\nEpoch: 155. Loss: 0.8698. Acc.: 40.81%\nEpoch: 156. Loss: 0.8602. Acc.: 41.21%\nEpoch: 157. Loss: 0.8619. Acc.: 41.34%\nEpoch: 158. Loss: 0.8579. Acc.: 41.60%\nEpoch: 159. Loss: 0.8590. Acc.: 41.34%\nEpoch: 160. Loss: 0.8548. Acc.: 41.21%\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 161. Loss: 0.8600. Acc.: 41.08%\nEpoch: 162. Loss: 0.8572. Acc.: 41.60%\nEpoch: 163. Loss: 0.8549. Acc.: 41.21%\nEpoch: 164. Loss: 0.8496. Acc.: 40.94%\nEpoch: 165. Loss: 0.8526. Acc.: 40.42%\nEpoch: 166. Loss: 0.8541. Acc.: 41.73%\nEpoch 166 best model saved with accuracy: 41.73%\nEpoch: 167. Loss: 0.8543. Acc.: 42.26%\nEpoch 167 best model saved with accuracy: 42.26%\nEpoch: 168. Loss: 0.8476. Acc.: 41.60%\nEpoch: 169. Loss: 0.8455. Acc.: 41.60%\nEpoch: 170. Loss: 0.8457. Acc.: 41.21%\nEpoch: 171. Loss: 0.8427. Acc.: 41.47%\nEpoch: 172. Loss: 0.8466. Acc.: 41.34%\nEpoch: 173. Loss: 0.8451. Acc.: 41.60%\nEpoch: 174. Loss: 0.8435. Acc.: 41.86%\nEpoch: 175. Loss: 0.8443. Acc.: 42.39%\nEpoch 175 best model saved with accuracy: 42.39%\nEpoch: 176. Loss: 0.8412. Acc.: 42.26%\nEpoch: 177. Loss: 0.8350. Acc.: 41.60%\nEpoch: 178. Loss: 0.8312. Acc.: 41.47%\nEpoch: 179. Loss: 0.8327. Acc.: 41.99%\nEpoch: 180. Loss: 0.8360. Acc.: 42.26%\nEpoch: 181. Loss: 0.8271. Acc.: 41.86%\nEpoch: 182. Loss: 0.8326. Acc.: 42.39%\nEpoch: 183. Loss: 0.8265. Acc.: 41.73%\nEpoch: 184. Loss: 0.8299. Acc.: 41.86%\nEpoch: 185. Loss: 0.8266. Acc.: 42.13%\nEpoch: 186. Loss: 0.8296. Acc.: 42.39%\nEpoch: 187. Loss: 0.8341. Acc.: 42.39%\nEpoch: 188. Loss: 0.8271. Acc.: 42.13%\nEpoch: 189. Loss: 0.8259. Acc.: 42.26%\nEpoch: 190. Loss: 0.8361. Acc.: 41.99%\nEpoch: 191. Loss: 0.8238. Acc.: 42.91%\nEpoch 191 best model saved with accuracy: 42.91%\nEpoch: 192. Loss: 0.8311. Acc.: 42.39%\nEpoch: 193. Loss: 0.8264. Acc.: 40.94%\nEpoch: 194. Loss: 0.8257. Acc.: 41.34%\nEpoch: 195. Loss: 0.8249. Acc.: 41.73%\nEpoch: 196. Loss: 0.8244. Acc.: 41.86%\nEpoch: 197. Loss: 0.8210. Acc.: 41.60%\nEpoch: 198. Loss: 0.8186. Acc.: 42.39%\nEpoch: 199. Loss: 0.8263. Acc.: 41.60%\nEpoch: 200. Loss: 0.8126. Acc.: 41.86%\nEpoch: 201. Loss: 0.8257. Acc.: 42.52%\nEpoch: 202. Loss: 0.8158. Acc.: 42.26%\nEpoch: 203. Loss: 0.8172. Acc.: 42.65%\nEpoch: 204. Loss: 0.8120. Acc.: 42.26%\nEpoch: 205. Loss: 0.8115. Acc.: 42.52%\nEpoch: 206. Loss: 0.8103. Acc.: 42.13%\nEpoch: 207. Loss: 0.8119. Acc.: 42.78%\nEpoch: 208. Loss: 0.8110. Acc.: 42.91%\nEpoch: 209. Loss: 0.8068. Acc.: 42.65%\nEpoch: 210. Loss: 0.8076. Acc.: 43.04%\nEpoch 210 best model saved with accuracy: 43.04%\nEpoch: 211. Loss: 0.8128. Acc.: 42.52%\nEpoch: 212. Loss: 0.8076. Acc.: 41.73%\nEpoch: 213. Loss: 0.8114. Acc.: 41.47%\nEpoch: 214. Loss: 0.8089. Acc.: 43.04%\nEpoch: 215. Loss: 0.8103. Acc.: 43.18%\nEpoch 215 best model saved with accuracy: 43.18%\nEpoch: 216. Loss: 0.7982. Acc.: 42.91%\nEpoch: 217. Loss: 0.8054. Acc.: 42.91%\nEpoch: 218. Loss: 0.8104. Acc.: 42.39%\nEpoch: 219. Loss: 0.8094. Acc.: 43.44%\nEpoch 219 best model saved with accuracy: 43.44%\nEpoch: 220. Loss: 0.8019. Acc.: 42.65%\nEpoch: 221. Loss: 0.7998. Acc.: 43.83%\nEpoch 221 best model saved with accuracy: 43.83%\nEpoch: 222. Loss: 0.7977. Acc.: 43.04%\nEpoch: 223. Loss: 0.7970. Acc.: 43.70%\nEpoch: 224. Loss: 0.7987. Acc.: 42.91%\nEpoch: 225. Loss: 0.7984. Acc.: 43.04%\nEpoch: 226. Loss: 0.7904. Acc.: 43.57%\nEpoch: 227. Loss: 0.7950. Acc.: 43.70%\nEpoch: 228. Loss: 0.7985. Acc.: 43.31%\nEpoch: 229. Loss: 0.7951. Acc.: 43.31%\nEpoch: 230. Loss: 0.7993. Acc.: 42.91%\nEpoch: 231. Loss: 0.7985. Acc.: 43.44%\nEpoch: 232. Loss: 0.7916. Acc.: 43.31%\nEpoch: 233. Loss: 0.7921. Acc.: 43.70%\nEpoch: 234. Loss: 0.7930. Acc.: 43.57%\nEpoch: 235. Loss: 0.7853. Acc.: 43.83%\nEpoch: 236. Loss: 0.7918. Acc.: 44.23%\nEpoch 236 best model saved with accuracy: 44.23%\nEpoch: 237. Loss: 0.7857. Acc.: 43.31%\nEpoch: 238. Loss: 0.7905. Acc.: 43.18%\nEpoch: 239. Loss: 0.7858. Acc.: 42.26%\nEpoch: 240. Loss: 0.7902. Acc.: 42.52%\nEpoch: 241. Loss: 0.7925. Acc.: 42.39%\nEpoch: 242. Loss: 0.7862. Acc.: 43.96%\nEpoch: 243. Loss: 0.7833. Acc.: 43.31%\nEpoch: 244. Loss: 0.7857. Acc.: 43.18%\nEpoch: 245. Loss: 0.7789. Acc.: 43.44%\nEpoch: 246. Loss: 0.7803. Acc.: 42.91%\nEpoch: 247. Loss: 0.7770. Acc.: 42.78%\nEpoch: 248. Loss: 0.7707. Acc.: 43.04%\nEpoch: 249. Loss: 0.7821. Acc.: 43.70%\nEpoch: 250. Loss: 0.7698. Acc.: 43.31%\nEpoch: 251. Loss: 0.7764. Acc.: 42.65%\nEpoch: 252. Loss: 0.7678. Acc.: 43.83%\nEpoch: 253. Loss: 0.7728. Acc.: 43.70%\nEpoch: 254. Loss: 0.7688. Acc.: 43.44%\nEpoch: 255. Loss: 0.7639. Acc.: 44.49%\nEpoch 255 best model saved with accuracy: 44.49%\nEpoch: 256. Loss: 0.7703. Acc.: 44.49%\nEpoch: 256. Loss: 0.7703. Acc.: 44.49%\nEpoch: 257. Loss: 0.7671. Acc.: 44.09%\nEpoch: 258. Loss: 0.7640. Acc.: 43.70%\nEpoch: 259. Loss: 0.7599. Acc.: 43.31%\nEpoch: 260. Loss: 0.7670. Acc.: 43.18%\nEpoch: 261. Loss: 0.7578. Acc.: 41.60%\nEpoch: 262. Loss: 0.7722. Acc.: 42.52%\nEpoch: 263. Loss: 0.7697. Acc.: 43.57%\nEpoch: 264. Loss: 0.7668. Acc.: 43.31%\nEpoch: 265. Loss: 0.7606. Acc.: 42.39%\nEpoch: 266. Loss: 0.7612. Acc.: 41.99%\nEpoch: 267. Loss: 0.7547. Acc.: 42.39%\nEpoch: 268. Loss: 0.7517. Acc.: 43.31%\nEpoch: 269. Loss: 0.7568. Acc.: 42.78%\nEpoch: 270. Loss: 0.7488. Acc.: 42.26%\nEpoch: 271. Loss: 0.7545. Acc.: 42.65%\nEpoch: 272. Loss: 0.7476. Acc.: 42.91%\nEpoch: 273. Loss: 0.7472. Acc.: 42.78%\nEpoch: 274. Loss: 0.7387. Acc.: 42.52%\nEpoch: 275. Loss: 0.7444. Acc.: 43.31%\nEpoch: 276. Loss: 0.7337. Acc.: 42.65%\nEpoch: 277. Loss: 0.7468. Acc.: 41.99%\nEpoch: 278. Loss: 0.7410. Acc.: 41.86%\nEpoch: 279. Loss: 0.7410. Acc.: 41.86%\nEpoch: 280. Loss: 0.7317. Acc.: 41.73%\nEpoch: 281. Loss: 0.7274. Acc.: 42.52%\nEpoch: 282. Loss: 0.7322. Acc.: 40.94%\nEpoch: 283. Loss: 0.7421. Acc.: 40.94%\nEpoch: 284. Loss: 0.7306. Acc.: 41.86%\nEpoch: 285. Loss: 0.7280. Acc.: 41.86%\nEpoch: 286. Loss: 0.7318. Acc.: 41.47%\nEpoch: 287. Loss: 0.7252. Acc.: 41.73%\nEpoch: 288. Loss: 0.7186. Acc.: 41.86%\nEpoch: 289. Loss: 0.7266. Acc.: 45.01%\nEpoch 289 best model saved with accuracy: 45.01%\nEpoch: 290. Loss: 0.7094. Acc.: 44.23%\nEpoch: 291. Loss: 0.7175. Acc.: 43.96%\nEpoch: 292. Loss: 0.7188. Acc.: 43.70%\nEpoch: 293. Loss: 0.7133. Acc.: 44.62%\nEpoch: 294. Loss: 0.7108. Acc.: 44.23%\nEpoch: 295. Loss: 0.7183. Acc.: 44.09%\nEpoch: 296. Loss: 0.7080. Acc.: 44.09%\nEpoch: 297. Loss: 0.7089. Acc.: 43.96%\nEpoch: 298. Loss: 0.7127. Acc.: 44.23%\nEpoch: 299. Loss: 0.7133. Acc.: 44.09%\nEpoch: 300. Loss: 0.7035. Acc.: 44.23%\nEpoch: 301. Loss: 0.7044. Acc.: 44.09%\nEpoch: 302. Loss: 0.7117. Acc.: 44.36%\nEpoch: 303. Loss: 0.7003. Acc.: 44.62%\nEpoch: 304. Loss: 0.6987. Acc.: 44.23%\nEpoch: 305. Loss: 0.6975. Acc.: 43.96%\nEpoch: 306. Loss: 0.6939. Acc.: 44.36%\nEpoch: 307. Loss: 0.6889. Acc.: 44.36%\nEpoch: 308. Loss: 0.7057. Acc.: 43.83%\nEpoch: 309. Loss: 0.7037. Acc.: 43.83%\nEpoch: 310. Loss: 0.6948. Acc.: 44.49%\nEpoch: 311. Loss: 0.6927. Acc.: 45.01%\nEpoch: 312. Loss: 0.6960. Acc.: 44.49%\nEpoch: 313. Loss: 0.6896. Acc.: 44.62%\nEpoch: 314. Loss: 0.6856. Acc.: 44.49%\nEpoch: 315. Loss: 0.6896. Acc.: 44.23%\nEpoch: 316. Loss: 0.6857. Acc.: 44.88%\nEpoch: 317. Loss: 0.6913. Acc.: 44.36%\nEpoch: 318. Loss: 0.6877. Acc.: 44.36%\nEpoch: 319. Loss: 0.6855. Acc.: 44.62%\nEpoch: 320. Loss: 0.6934. Acc.: 44.75%\nEpoch: 321. Loss: 0.6844. Acc.: 45.01%\nEpoch: 322. Loss: 0.6880. Acc.: 44.75%\nEpoch: 323. Loss: 0.6937. Acc.: 45.67%\nEpoch 323 best model saved with accuracy: 45.67%\nEpoch: 324. Loss: 0.6838. Acc.: 45.28%\nEpoch: 325. Loss: 0.6877. Acc.: 45.41%\nEpoch: 326. Loss: 0.6834. Acc.: 45.01%\nEpoch: 327. Loss: 0.6773. Acc.: 44.88%\nEpoch: 328. Loss: 0.6778. Acc.: 44.62%\nEpoch: 329. Loss: 0.6770. Acc.: 44.62%\nEpoch: 330. Loss: 0.6802. Acc.: 45.01%\nEpoch: 331. Loss: 0.6762. Acc.: 44.88%\nEpoch: 332. Loss: 0.6792. Acc.: 44.75%\nEpoch: 333. Loss: 0.6779. Acc.: 45.14%\nEpoch: 334. Loss: 0.6874. Acc.: 44.88%\nEpoch: 335. Loss: 0.6732. Acc.: 45.01%\nEpoch: 336. Loss: 0.6902. Acc.: 45.14%\nEpoch: 337. Loss: 0.6766. Acc.: 44.36%\nEpoch: 338. Loss: 0.6814. Acc.: 45.54%\nEpoch: 339. Loss: 0.6740. Acc.: 45.01%\nEpoch: 340. Loss: 0.6724. Acc.: 45.01%\nEpoch: 341. Loss: 0.6783. Acc.: 45.41%\nEpoch: 342. Loss: 0.6709. Acc.: 45.28%\nEpoch: 343. Loss: 0.6607. Acc.: 45.41%\nEpoch: 344. Loss: 0.6672. Acc.: 46.19%\nEpoch 344 best model saved with accuracy: 46.19%\nEpoch: 345. Loss: 0.6655. Acc.: 45.80%\nEpoch: 346. Loss: 0.6729. Acc.: 45.01%\nEpoch: 347. Loss: 0.6683. Acc.: 44.75%\nEpoch: 348. Loss: 0.6674. Acc.: 45.28%\nEpoch: 349. Loss: 0.6700. Acc.: 44.75%\nEpoch: 350. Loss: 0.6680. Acc.: 44.09%\nEpoch: 351. Loss: 0.6620. Acc.: 44.36%\nEpoch: 352. Loss: 0.6638. Acc.: 43.96%\nEpoch: 353. Loss: 0.6607. Acc.: 44.88%\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 354. Loss: 0.6665. Acc.: 44.75%\nEpoch: 355. Loss: 0.6611. Acc.: 45.01%\nEpoch: 356. Loss: 0.6684. Acc.: 44.49%\nEpoch: 357. Loss: 0.6652. Acc.: 44.75%\nEpoch: 358. Loss: 0.6529. Acc.: 44.62%\nEpoch: 359. Loss: 0.6615. Acc.: 44.75%\nEpoch: 360. Loss: 0.6581. Acc.: 45.80%\nEpoch: 361. Loss: 0.6620. Acc.: 43.83%\nEpoch: 362. Loss: 0.6564. Acc.: 44.36%\nEpoch: 363. Loss: 0.6528. Acc.: 44.88%\nEpoch: 364. Loss: 0.6448. Acc.: 46.85%\nEpoch 364 best model saved with accuracy: 46.85%\nEpoch: 365. Loss: 0.6567. Acc.: 45.54%\nEpoch: 366. Loss: 0.6540. Acc.: 45.28%\nEpoch: 367. Loss: 0.6518. Acc.: 47.38%\nEpoch 367 best model saved with accuracy: 47.38%\nEpoch: 368. Loss: 0.6475. Acc.: 47.11%\nEpoch: 369. Loss: 0.6444. Acc.: 47.64%\nEpoch 369 best model saved with accuracy: 47.64%\nEpoch: 370. Loss: 0.6513. Acc.: 48.16%\nEpoch 370 best model saved with accuracy: 48.16%\nEpoch: 371. Loss: 0.6479. Acc.: 49.34%\nEpoch 371 best model saved with accuracy: 49.34%\nEpoch: 372. Loss: 0.6432. Acc.: 48.43%\nEpoch: 373. Loss: 0.6390. Acc.: 48.16%\nEpoch: 374. Loss: 0.6344. Acc.: 47.77%\nEpoch: 375. Loss: 0.6319. Acc.: 48.69%\nEpoch: 376. Loss: 0.6315. Acc.: 49.21%\nEpoch: 377. Loss: 0.6363. Acc.: 49.08%\nEpoch: 378. Loss: 0.6189. Acc.: 50.26%\nEpoch 378 best model saved with accuracy: 50.26%\nEpoch: 379. Loss: 0.6299. Acc.: 50.39%\nEpoch 379 best model saved with accuracy: 50.39%\nEpoch: 380. Loss: 0.6276. Acc.: 50.39%\nEpoch: 381. Loss: 0.6208. Acc.: 49.34%\nEpoch: 382. Loss: 0.6157. Acc.: 50.00%\nEpoch: 383. Loss: 0.6197. Acc.: 51.84%\nEpoch 383 best model saved with accuracy: 51.84%\nEpoch: 384. Loss: 0.6180. Acc.: 51.44%\nEpoch: 385. Loss: 0.6140. Acc.: 51.57%\nEpoch: 386. Loss: 0.6092. Acc.: 51.44%\nEpoch: 387. Loss: 0.6110. Acc.: 50.52%\nEpoch: 388. Loss: 0.6032. Acc.: 50.66%\nEpoch: 389. Loss: 0.6220. Acc.: 52.89%\nEpoch 389 best model saved with accuracy: 52.89%\nEpoch: 390. Loss: 0.6071. Acc.: 53.02%\nEpoch 390 best model saved with accuracy: 53.02%\nEpoch: 391. Loss: 0.5991. Acc.: 51.71%\nEpoch: 392. Loss: 0.6080. Acc.: 51.57%\nEpoch: 393. Loss: 0.5986. Acc.: 51.84%\nEpoch: 394. Loss: 0.6058. Acc.: 50.92%\nEpoch: 395. Loss: 0.6110. Acc.: 49.87%\nEpoch: 396. Loss: 0.6069. Acc.: 50.79%\nEpoch: 397. Loss: 0.5901. Acc.: 52.49%\nEpoch: 398. Loss: 0.5966. Acc.: 53.15%\nEpoch 398 best model saved with accuracy: 53.15%\nEpoch: 399. Loss: 0.5981. Acc.: 51.44%\nEpoch: 400. Loss: 0.5932. Acc.: 51.97%\nEpoch: 401. Loss: 0.5892. Acc.: 52.23%\nEpoch: 402. Loss: 0.5862. Acc.: 51.44%\nEpoch: 403. Loss: 0.5948. Acc.: 51.71%\nEpoch: 404. Loss: 0.5927. Acc.: 52.62%\nEpoch: 405. Loss: 0.5755. Acc.: 51.44%\nEpoch: 406. Loss: 0.5880. Acc.: 52.49%\nEpoch: 407. Loss: 0.5890. Acc.: 52.10%\nEpoch: 408. Loss: 0.5833. Acc.: 51.57%\nEpoch: 409. Loss: 0.5808. Acc.: 52.62%\nEpoch: 410. Loss: 0.5801. Acc.: 51.57%\nEpoch: 411. Loss: 0.5824. Acc.: 53.02%\nEpoch: 412. Loss: 0.5778. Acc.: 52.62%\nEpoch: 413. Loss: 0.5766. Acc.: 52.23%\nEpoch: 414. Loss: 0.5771. Acc.: 52.23%\nEpoch: 415. Loss: 0.5694. Acc.: 51.97%\nEpoch: 416. Loss: 0.5762. Acc.: 52.89%\nEpoch: 417. Loss: 0.5628. Acc.: 52.89%\nEpoch: 418. Loss: 0.5607. Acc.: 53.41%\nEpoch 418 best model saved with accuracy: 53.41%\nEpoch: 419. Loss: 0.5645. Acc.: 51.71%\nEpoch: 420. Loss: 0.5690. Acc.: 53.02%\nEpoch: 421. Loss: 0.5644. Acc.: 53.54%\nEpoch 421 best model saved with accuracy: 53.54%\nEpoch: 422. Loss: 0.5593. Acc.: 53.15%\nEpoch: 423. Loss: 0.5531. Acc.: 52.49%\nEpoch: 424. Loss: 0.5616. Acc.: 53.81%\nEpoch 424 best model saved with accuracy: 53.81%\nEpoch: 425. Loss: 0.5564. Acc.: 54.59%\nEpoch 425 best model saved with accuracy: 54.59%\nEpoch: 426. Loss: 0.5547. Acc.: 53.67%\nEpoch: 427. Loss: 0.5575. Acc.: 53.67%\nEpoch: 428. Loss: 0.5526. Acc.: 53.94%\nEpoch: 429. Loss: 0.5562. Acc.: 53.15%\nEpoch: 430. Loss: 0.5442. Acc.: 52.76%\nEpoch: 431. Loss: 0.5417. Acc.: 53.81%\nEpoch: 432. Loss: 0.5470. Acc.: 53.41%\nEpoch: 433. Loss: 0.5484. Acc.: 53.15%\nEpoch: 434. Loss: 0.5547. Acc.: 52.62%\nEpoch: 435. Loss: 0.5527. Acc.: 51.84%\nEpoch: 436. Loss: 0.5392. Acc.: 52.49%\nEpoch: 437. Loss: 0.5407. Acc.: 52.76%\nEpoch: 438. Loss: 0.5432. Acc.: 53.41%\nEpoch: 439. Loss: 0.5441. Acc.: 53.02%\nEpoch: 440. Loss: 0.5382. Acc.: 52.76%\nEpoch: 441. Loss: 0.5373. Acc.: 54.07%\nEpoch: 442. Loss: 0.5362. Acc.: 53.15%\nEpoch: 443. Loss: 0.5351. Acc.: 52.76%\nEpoch: 444. Loss: 0.5329. Acc.: 53.41%\nEpoch: 445. Loss: 0.5377. Acc.: 53.41%\nEpoch: 446. Loss: 0.5396. Acc.: 53.02%\nEpoch: 447. Loss: 0.5341. Acc.: 52.89%\nEpoch: 448. Loss: 0.5283. Acc.: 53.81%\nEpoch: 449. Loss: 0.5313. Acc.: 53.54%\nEpoch: 450. Loss: 0.5370. Acc.: 54.46%\nEpoch: 451. Loss: 0.5238. Acc.: 53.94%\nEpoch: 452. Loss: 0.5235. Acc.: 54.59%\nEpoch: 453. Loss: 0.5324. Acc.: 54.46%\nEpoch: 454. Loss: 0.5236. Acc.: 55.25%\nEpoch 454 best model saved with accuracy: 55.25%\nEpoch: 455. Loss: 0.5268. Acc.: 55.12%\nEpoch: 456. Loss: 0.5129. Acc.: 54.86%\nEpoch: 457. Loss: 0.5218. Acc.: 55.38%\nEpoch 457 best model saved with accuracy: 55.38%\nEpoch: 458. Loss: 0.5211. Acc.: 55.77%\nEpoch 458 best model saved with accuracy: 55.77%\nEpoch: 459. Loss: 0.5146. Acc.: 55.64%\nEpoch: 460. Loss: 0.5154. Acc.: 54.99%\nEpoch: 461. Loss: 0.5118. Acc.: 55.91%\nEpoch 461 best model saved with accuracy: 55.91%\nEpoch: 462. Loss: 0.5133. Acc.: 55.91%\nEpoch: 463. Loss: 0.5266. Acc.: 55.38%\nEpoch: 464. Loss: 0.5134. Acc.: 54.59%\nEpoch: 465. Loss: 0.5159. Acc.: 54.20%\nEpoch: 466. Loss: 0.5068. Acc.: 55.38%\nEpoch: 467. Loss: 0.5110. Acc.: 54.72%\nEpoch: 468. Loss: 0.5097. Acc.: 54.99%\nEpoch: 469. Loss: 0.5086. Acc.: 55.12%\nEpoch: 470. Loss: 0.5077. Acc.: 55.38%\nEpoch: 471. Loss: 0.5018. Acc.: 54.99%\nEpoch: 472. Loss: 0.4937. Acc.: 54.72%\nEpoch: 473. Loss: 0.5017. Acc.: 55.51%\nEpoch: 474. Loss: 0.4974. Acc.: 55.77%\nEpoch: 475. Loss: 0.4963. Acc.: 55.77%\nEpoch: 476. Loss: 0.4948. Acc.: 55.12%\nEpoch: 477. Loss: 0.4899. Acc.: 56.43%\nEpoch 477 best model saved with accuracy: 56.43%\nEpoch: 478. Loss: 0.4988. Acc.: 56.69%\nEpoch 478 best model saved with accuracy: 56.69%\nEpoch: 479. Loss: 0.4951. Acc.: 55.64%\nEpoch: 480. Loss: 0.4933. Acc.: 55.51%\nEpoch: 481. Loss: 0.4883. Acc.: 55.12%\nEpoch: 482. Loss: 0.4880. Acc.: 55.51%\nEpoch: 483. Loss: 0.4780. Acc.: 56.04%\nEpoch: 484. Loss: 0.4889. Acc.: 54.86%\nEpoch: 485. Loss: 0.4856. Acc.: 56.04%\nEpoch: 486. Loss: 0.4902. Acc.: 56.82%\nEpoch 486 best model saved with accuracy: 56.82%\nEpoch: 487. Loss: 0.4886. Acc.: 55.77%\nEpoch: 488. Loss: 0.4890. Acc.: 55.38%\nEpoch: 489. Loss: 0.4877. Acc.: 55.38%\nEpoch: 490. Loss: 0.4748. Acc.: 55.64%\nEpoch: 491. Loss: 0.4747. Acc.: 55.91%\nEpoch: 492. Loss: 0.4807. Acc.: 55.77%\nEpoch: 493. Loss: 0.4690. Acc.: 55.51%\nEpoch: 494. Loss: 0.4772. Acc.: 56.17%\nEpoch: 495. Loss: 0.4683. Acc.: 56.17%\nEpoch: 496. Loss: 0.4768. Acc.: 56.17%\nEpoch: 497. Loss: 0.4645. Acc.: 56.04%\nEpoch: 498. Loss: 0.4758. Acc.: 56.17%\nEpoch: 499. Loss: 0.4713. Acc.: 57.09%\nEpoch 499 best model saved with accuracy: 57.09%\nEpoch: 500. Loss: 0.4795. Acc.: 55.64%\nEpoch: 501. Loss: 0.4668. Acc.: 56.96%\nEpoch: 502. Loss: 0.4684. Acc.: 57.48%\nEpoch 502 best model saved with accuracy: 57.48%\nEpoch: 503. Loss: 0.4672. Acc.: 57.09%\nEpoch: 504. Loss: 0.4581. Acc.: 56.56%\nEpoch: 505. Loss: 0.4612. Acc.: 55.38%\nEpoch: 506. Loss: 0.4702. Acc.: 55.51%\nEpoch: 507. Loss: 0.4554. Acc.: 56.04%\nEpoch: 508. Loss: 0.4498. Acc.: 56.04%\nEpoch: 509. Loss: 0.4464. Acc.: 56.17%\nEpoch: 510. Loss: 0.4603. Acc.: 56.43%\nEpoch: 511. Loss: 0.4551. Acc.: 56.43%\nEpoch: 512. Loss: 0.4501. Acc.: 55.91%\nEpoch: 512. Loss: 0.4501. Acc.: 55.91%\nEpoch: 513. Loss: 0.4575. Acc.: 56.82%\nEpoch: 514. Loss: 0.4477. Acc.: 56.69%\nEpoch: 515. Loss: 0.4491. Acc.: 56.56%\nEpoch: 516. Loss: 0.4565. Acc.: 57.35%\nEpoch: 517. Loss: 0.4466. Acc.: 57.61%\nEpoch 517 best model saved with accuracy: 57.61%\nEpoch: 518. Loss: 0.4434. Acc.: 57.09%\nEpoch: 519. Loss: 0.4453. Acc.: 57.22%\nEpoch: 520. Loss: 0.4438. Acc.: 58.53%\nEpoch 520 best model saved with accuracy: 58.53%\nEpoch: 521. Loss: 0.4328. Acc.: 57.09%\nEpoch: 522. Loss: 0.4288. Acc.: 57.48%\nEpoch: 523. Loss: 0.4345. Acc.: 58.01%\nEpoch: 524. Loss: 0.4355. Acc.: 57.74%\nEpoch: 525. Loss: 0.4319. Acc.: 58.53%\nEpoch: 526. Loss: 0.4344. Acc.: 63.12%\nEpoch 526 best model saved with accuracy: 63.12%\nEpoch: 527. Loss: 0.4463. Acc.: 63.52%\nEpoch 527 best model saved with accuracy: 63.52%\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 528. Loss: 0.4256. Acc.: 64.30%\nEpoch 528 best model saved with accuracy: 64.30%\nEpoch: 529. Loss: 0.4265. Acc.: 64.57%\nEpoch 529 best model saved with accuracy: 64.57%\nEpoch: 530. Loss: 0.4235. Acc.: 63.91%\nEpoch: 531. Loss: 0.4131. Acc.: 64.04%\nEpoch: 532. Loss: 0.4145. Acc.: 64.83%\nEpoch 532 best model saved with accuracy: 64.83%\nEpoch: 533. Loss: 0.4359. Acc.: 64.44%\nEpoch: 534. Loss: 0.4326. Acc.: 64.57%\nEpoch: 535. Loss: 0.4080. Acc.: 64.96%\nEpoch 535 best model saved with accuracy: 64.96%\nEpoch: 536. Loss: 0.4157. Acc.: 63.78%\nEpoch: 537. Loss: 0.4141. Acc.: 64.44%\nEpoch: 538. Loss: 0.4128. Acc.: 63.65%\nEpoch: 539. Loss: 0.4223. Acc.: 63.91%\nEpoch: 540. Loss: 0.4120. Acc.: 65.09%\nEpoch 540 best model saved with accuracy: 65.09%\nEpoch: 541. Loss: 0.4223. Acc.: 64.83%\nEpoch: 542. Loss: 0.4059. Acc.: 64.57%\nEpoch: 543. Loss: 0.4011. Acc.: 63.78%\nEpoch: 544. Loss: 0.4064. Acc.: 64.04%\nEpoch: 545. Loss: 0.3932. Acc.: 64.17%\nEpoch: 546. Loss: 0.4054. Acc.: 64.83%\nEpoch: 547. Loss: 0.4032. Acc.: 64.17%\nEpoch: 548. Loss: 0.3989. Acc.: 64.70%\nEpoch: 549. Loss: 0.3997. Acc.: 63.25%\nEpoch: 550. Loss: 0.4084. Acc.: 64.17%\nEpoch: 551. Loss: 0.3927. Acc.: 64.44%\nEpoch: 552. Loss: 0.3982. Acc.: 63.78%\nEpoch: 553. Loss: 0.4041. Acc.: 64.83%\nEpoch: 554. Loss: 0.3882. Acc.: 65.35%\nEpoch 554 best model saved with accuracy: 65.35%\nEpoch: 555. Loss: 0.4020. Acc.: 64.57%\nEpoch: 556. Loss: 0.3786. Acc.: 64.83%\nEpoch: 557. Loss: 0.3885. Acc.: 64.70%\nEpoch: 558. Loss: 0.3850. Acc.: 64.17%\nEpoch: 559. Loss: 0.3941. Acc.: 65.09%\nEpoch: 560. Loss: 0.3829. Acc.: 66.27%\nEpoch 560 best model saved with accuracy: 66.27%\nEpoch: 561. Loss: 0.3788. Acc.: 65.09%\nEpoch: 562. Loss: 0.3810. Acc.: 65.35%\nEpoch: 563. Loss: 0.3954. Acc.: 64.70%\nEpoch: 564. Loss: 0.3773. Acc.: 65.22%\nEpoch: 565. Loss: 0.3674. Acc.: 64.44%\nEpoch: 566. Loss: 0.3769. Acc.: 64.70%\nEpoch: 567. Loss: 0.3787. Acc.: 65.22%\nEpoch: 568. Loss: 0.3703. Acc.: 65.35%\nEpoch: 569. Loss: 0.3695. Acc.: 64.70%\nEpoch: 570. Loss: 0.3654. Acc.: 65.49%\nEpoch: 571. Loss: 0.3779. Acc.: 66.14%\nEpoch: 572. Loss: 0.3644. Acc.: 66.01%\nEpoch: 573. Loss: 0.3717. Acc.: 66.54%\nEpoch 573 best model saved with accuracy: 66.54%\nEpoch: 574. Loss: 0.3661. Acc.: 65.88%\nEpoch: 575. Loss: 0.3764. Acc.: 65.35%\nEpoch: 576. Loss: 0.3701. Acc.: 64.17%\nEpoch: 577. Loss: 0.3546. Acc.: 64.83%\nEpoch: 578. Loss: 0.3683. Acc.: 64.04%\nEpoch: 579. Loss: 0.3590. Acc.: 64.04%\nEpoch: 580. Loss: 0.3493. Acc.: 64.70%\nEpoch: 581. Loss: 0.3571. Acc.: 65.75%\nEpoch: 582. Loss: 0.3504. Acc.: 65.49%\nEpoch: 583. Loss: 0.3584. Acc.: 65.09%\nEpoch: 584. Loss: 0.3593. Acc.: 64.70%\nEpoch: 585. Loss: 0.3655. Acc.: 64.96%\nEpoch: 586. Loss: 0.3604. Acc.: 65.22%\nEpoch: 587. Loss: 0.3534. Acc.: 66.27%\nEpoch: 588. Loss: 0.3532. Acc.: 65.62%\nEpoch: 589. Loss: 0.3511. Acc.: 65.49%\nEpoch: 590. Loss: 0.3542. Acc.: 65.22%\nEpoch: 591. Loss: 0.3494. Acc.: 65.35%\nEpoch: 592. Loss: 0.3465. Acc.: 65.62%\nEpoch: 593. Loss: 0.3542. Acc.: 66.01%\nEpoch: 594. Loss: 0.3587. Acc.: 65.22%\nEpoch: 595. Loss: 0.3605. Acc.: 63.91%\nEpoch: 596. Loss: 0.3471. Acc.: 64.44%\nEpoch: 597. Loss: 0.3446. Acc.: 65.35%\nEpoch: 598. Loss: 0.3363. Acc.: 64.44%\nEpoch: 599. Loss: 0.3351. Acc.: 65.49%\nEpoch: 600. Loss: 0.3387. Acc.: 64.57%\nEpoch: 601. Loss: 0.3330. Acc.: 64.83%\nEpoch: 602. Loss: 0.3416. Acc.: 64.44%\nEpoch: 603. Loss: 0.3482. Acc.: 65.75%\nEpoch: 604. Loss: 0.3481. Acc.: 65.35%\nEpoch: 605. Loss: 0.3418. Acc.: 64.57%\nEpoch: 606. Loss: 0.3415. Acc.: 64.57%\nEpoch: 607. Loss: 0.3335. Acc.: 64.44%\nEpoch: 608. Loss: 0.3328. Acc.: 64.96%\nEpoch: 609. Loss: 0.3454. Acc.: 65.88%\nEpoch: 610. Loss: 0.3335. Acc.: 66.54%\nEpoch: 611. Loss: 0.3268. Acc.: 65.49%\nEpoch: 612. Loss: 0.3245. Acc.: 65.22%\nEpoch: 613. Loss: 0.3327. Acc.: 64.70%\nEpoch: 614. Loss: 0.3333. Acc.: 65.49%\nEpoch: 615. Loss: 0.3297. Acc.: 65.49%\nEpoch: 616. Loss: 0.3245. Acc.: 64.96%\nEpoch: 617. Loss: 0.3290. Acc.: 64.17%\nEpoch: 618. Loss: 0.3314. Acc.: 66.67%\nEpoch 618 best model saved with accuracy: 66.67%\nEpoch: 619. Loss: 0.3206. Acc.: 65.62%\nEpoch: 620. Loss: 0.3223. Acc.: 65.88%\nEpoch: 621. Loss: 0.3236. Acc.: 66.93%\nEpoch 621 best model saved with accuracy: 66.93%\nEpoch: 622. Loss: 0.3173. Acc.: 65.62%\nEpoch: 623. Loss: 0.3316. Acc.: 65.35%\nEpoch: 624. Loss: 0.3165. Acc.: 66.14%\nEpoch: 625. Loss: 0.3210. Acc.: 65.35%\nEpoch: 626. Loss: 0.3275. Acc.: 66.01%\nEpoch: 627. Loss: 0.3202. Acc.: 66.01%\nEpoch: 628. Loss: 0.3098. Acc.: 65.35%\nEpoch: 629. Loss: 0.3207. Acc.: 65.88%\nEpoch: 630. Loss: 0.3179. Acc.: 65.62%\nEpoch: 631. Loss: 0.3114. Acc.: 64.83%\nEpoch: 632. Loss: 0.3133. Acc.: 64.44%\nEpoch: 633. Loss: 0.3200. Acc.: 65.35%\nEpoch: 634. Loss: 0.3184. Acc.: 66.14%\nEpoch: 635. Loss: 0.3073. Acc.: 65.88%\nEpoch: 636. Loss: 0.3148. Acc.: 66.01%\nEpoch: 637. Loss: 0.3141. Acc.: 66.54%\nEpoch: 638. Loss: 0.3080. Acc.: 66.54%\nEpoch: 639. Loss: 0.3112. Acc.: 66.40%\nEpoch: 640. Loss: 0.3025. Acc.: 64.17%\nEpoch: 641. Loss: 0.3037. Acc.: 65.09%\nEpoch: 642. Loss: 0.3115. Acc.: 66.01%\nEpoch: 643. Loss: 0.2993. Acc.: 65.35%\nEpoch: 644. Loss: 0.3112. Acc.: 65.35%\nEpoch: 645. Loss: 0.3062. Acc.: 64.70%\nEpoch: 646. Loss: 0.3071. Acc.: 65.09%\nEpoch: 647. Loss: 0.3073. Acc.: 65.62%\nEpoch: 648. Loss: 0.3020. Acc.: 65.49%\nEpoch: 649. Loss: 0.3043. Acc.: 65.35%\nEpoch: 650. Loss: 0.2921. Acc.: 64.83%\nEpoch: 651. Loss: 0.3125. Acc.: 65.88%\nEpoch: 652. Loss: 0.3053. Acc.: 66.01%\nEpoch: 653. Loss: 0.3088. Acc.: 66.67%\nEpoch: 654. Loss: 0.3054. Acc.: 65.88%\nEpoch: 655. Loss: 0.2950. Acc.: 66.01%\nEpoch: 656. Loss: 0.2942. Acc.: 66.80%\nEpoch: 657. Loss: 0.2999. Acc.: 66.01%\nEpoch: 658. Loss: 0.2858. Acc.: 66.14%\nEpoch: 659. Loss: 0.2939. Acc.: 65.35%\nEpoch: 660. Loss: 0.2897. Acc.: 66.40%\nEpoch: 661. Loss: 0.2921. Acc.: 66.01%\nEpoch: 662. Loss: 0.2883. Acc.: 65.35%\nEpoch: 663. Loss: 0.2878. Acc.: 65.35%\nEpoch: 664. Loss: 0.2795. Acc.: 65.75%\nEpoch: 665. Loss: 0.2824. Acc.: 66.54%\nEpoch: 666. Loss: 0.2959. Acc.: 66.14%\nEpoch: 667. Loss: 0.2702. Acc.: 66.67%\nEpoch: 668. Loss: 0.2847. Acc.: 65.75%\nEpoch: 669. Loss: 0.2822. Acc.: 67.06%\nEpoch 669 best model saved with accuracy: 67.06%\nEpoch: 670. Loss: 0.2788. Acc.: 66.54%\nEpoch: 671. Loss: 0.2790. Acc.: 65.62%\nEpoch: 672. Loss: 0.2931. Acc.: 66.01%\nEpoch: 673. Loss: 0.2799. Acc.: 65.49%\nEpoch: 674. Loss: 0.2723. Acc.: 66.67%\nEpoch: 675. Loss: 0.2854. Acc.: 66.27%\nEpoch: 676. Loss: 0.2858. Acc.: 66.80%\nEpoch: 677. Loss: 0.2889. Acc.: 66.27%\nEpoch: 678. Loss: 0.2867. Acc.: 66.01%\nEpoch: 679. Loss: 0.2797. Acc.: 65.75%\nEpoch: 680. Loss: 0.2757. Acc.: 66.54%\nEpoch: 681. Loss: 0.2894. Acc.: 65.09%\nEpoch: 682. Loss: 0.2721. Acc.: 65.88%\nEpoch: 683. Loss: 0.2866. Acc.: 64.70%\nEpoch: 684. Loss: 0.2850. Acc.: 65.75%\nEpoch: 685. Loss: 0.2704. Acc.: 66.40%\nEpoch: 686. Loss: 0.2703. Acc.: 65.75%\nEpoch: 687. Loss: 0.2772. Acc.: 66.54%\nEpoch: 688. Loss: 0.2729. Acc.: 66.01%\nEpoch: 689. Loss: 0.2768. Acc.: 65.62%\nEpoch: 690. Loss: 0.2772. Acc.: 65.09%\nEpoch: 691. Loss: 0.2811. Acc.: 64.70%\nEpoch: 692. Loss: 0.2794. Acc.: 65.09%\nEpoch: 693. Loss: 0.2679. Acc.: 65.75%\nEpoch: 694. Loss: 0.2821. Acc.: 66.80%\nEpoch: 695. Loss: 0.2833. Acc.: 66.40%\nEpoch: 696. Loss: 0.2772. Acc.: 66.40%\nEpoch: 697. Loss: 0.2766. Acc.: 67.32%\nEpoch 697 best model saved with accuracy: 67.32%\nEpoch: 698. Loss: 0.2724. Acc.: 66.54%\nEpoch: 699. Loss: 0.2620. Acc.: 65.09%\nEpoch: 700. Loss: 0.2709. Acc.: 65.62%\nEpoch: 701. Loss: 0.2800. Acc.: 66.01%\nEpoch: 702. Loss: 0.2776. Acc.: 66.80%\nEpoch: 703. Loss: 0.2626. Acc.: 66.54%\nEpoch: 704. Loss: 0.2609. Acc.: 65.75%\nEpoch: 705. Loss: 0.2681. Acc.: 65.75%\nEpoch: 706. Loss: 0.2570. Acc.: 66.14%\nEpoch: 707. Loss: 0.2663. Acc.: 66.80%\nEpoch: 708. Loss: 0.2671. Acc.: 65.22%\nEpoch: 709. Loss: 0.2651. Acc.: 65.75%\nEpoch: 710. Loss: 0.2762. Acc.: 66.01%\nEpoch: 711. Loss: 0.2721. Acc.: 65.49%\nEpoch: 712. Loss: 0.2641. Acc.: 66.01%\nEpoch: 713. Loss: 0.2732. Acc.: 65.75%\nEpoch: 714. Loss: 0.2760. Acc.: 66.40%\nEpoch: 715. Loss: 0.2587. Acc.: 66.14%\nEpoch: 716. Loss: 0.2642. Acc.: 65.49%\nEpoch: 717. Loss: 0.2657. Acc.: 65.75%\nEpoch: 718. Loss: 0.2701. Acc.: 66.01%\nEpoch: 719. Loss: 0.2564. Acc.: 65.75%\nEpoch: 720. Loss: 0.2566. Acc.: 66.27%\nEpoch: 721. Loss: 0.2655. Acc.: 66.67%\nEpoch: 722. Loss: 0.2554. Acc.: 66.01%\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 723. Loss: 0.2543. Acc.: 66.93%\nEpoch: 724. Loss: 0.2604. Acc.: 66.67%\nEpoch: 725. Loss: 0.2479. Acc.: 65.35%\nEpoch: 726. Loss: 0.2632. Acc.: 65.49%\nEpoch: 727. Loss: 0.2467. Acc.: 66.01%\nEpoch: 728. Loss: 0.2453. Acc.: 66.27%\nEpoch: 729. Loss: 0.2652. Acc.: 65.09%\nEpoch: 730. Loss: 0.2476. Acc.: 65.09%\nEpoch: 731. Loss: 0.2478. Acc.: 64.96%\nEpoch: 732. Loss: 0.2511. Acc.: 66.40%\nEpoch: 733. Loss: 0.2569. Acc.: 67.72%\nEpoch 733 best model saved with accuracy: 67.72%\nEpoch: 734. Loss: 0.2543. Acc.: 67.59%\nEpoch: 735. Loss: 0.2484. Acc.: 67.72%\nEpoch: 736. Loss: 0.2632. Acc.: 67.32%\nEpoch: 737. Loss: 0.2554. Acc.: 66.01%\nEpoch: 738. Loss: 0.2478. Acc.: 66.14%\nEpoch: 739. Loss: 0.2540. Acc.: 66.67%\nEpoch: 740. Loss: 0.2408. Acc.: 65.88%\nEpoch: 741. Loss: 0.2534. Acc.: 66.14%\nEpoch: 742. Loss: 0.2476. Acc.: 66.27%\nEpoch: 743. Loss: 0.2525. Acc.: 65.09%\nEpoch: 744. Loss: 0.2573. Acc.: 66.14%\nEpoch: 745. Loss: 0.2578. Acc.: 66.40%\nEpoch: 746. Loss: 0.2527. Acc.: 65.88%\nEpoch: 747. Loss: 0.2641. Acc.: 66.01%\nEpoch: 748. Loss: 0.2452. Acc.: 66.27%\nEpoch: 749. Loss: 0.2442. Acc.: 67.32%\nEpoch: 750. Loss: 0.2459. Acc.: 67.85%\nEpoch 750 best model saved with accuracy: 67.85%\nEpoch: 751. Loss: 0.2505. Acc.: 66.80%\nEpoch: 752. Loss: 0.2360. Acc.: 66.67%\nEpoch: 753. Loss: 0.2420. Acc.: 65.88%\nEpoch: 754. Loss: 0.2425. Acc.: 66.67%\nEpoch: 755. Loss: 0.2430. Acc.: 66.54%\nEpoch: 756. Loss: 0.2543. Acc.: 66.01%\nEpoch: 757. Loss: 0.2292. Acc.: 66.40%\nEpoch: 758. Loss: 0.2610. Acc.: 66.67%\nEpoch: 759. Loss: 0.2511. Acc.: 67.19%\nEpoch: 760. Loss: 0.2520. Acc.: 66.40%\nEpoch: 761. Loss: 0.2515. Acc.: 65.88%\nEpoch: 762. Loss: 0.2481. Acc.: 66.40%\nEpoch: 763. Loss: 0.2408. Acc.: 68.24%\nEpoch 763 best model saved with accuracy: 68.24%\nEpoch: 764. Loss: 0.2300. Acc.: 65.75%\nEpoch: 765. Loss: 0.2395. Acc.: 66.01%\nEpoch: 766. Loss: 0.2320. Acc.: 65.88%\nEpoch: 767. Loss: 0.2423. Acc.: 64.44%\nEpoch: 768. Loss: 0.2365. Acc.: 65.09%\nEpoch: 769. Loss: 0.2372. Acc.: 65.09%\nEpoch: 770. Loss: 0.2223. Acc.: 65.49%\nEpoch: 771. Loss: 0.2278. Acc.: 65.35%\nEpoch: 772. Loss: 0.2218. Acc.: 66.01%\nEpoch: 773. Loss: 0.2198. Acc.: 66.27%\nEpoch: 774. Loss: 0.2274. Acc.: 66.67%\nEpoch: 775. Loss: 0.2266. Acc.: 65.35%\nEpoch: 776. Loss: 0.2161. Acc.: 66.67%\nEpoch: 777. Loss: 0.2166. Acc.: 66.54%\nEpoch: 778. Loss: 0.2014. Acc.: 69.69%\nEpoch 778 best model saved with accuracy: 69.69%\nEpoch: 779. Loss: 0.2118. Acc.: 69.03%\nEpoch: 780. Loss: 0.2035. Acc.: 69.16%\nEpoch: 781. Loss: 0.2213. Acc.: 68.64%\nEpoch: 782. Loss: 0.2161. Acc.: 70.21%\nEpoch 782 best model saved with accuracy: 70.21%\nEpoch: 783. Loss: 0.2163. Acc.: 70.34%\nEpoch 783 best model saved with accuracy: 70.34%\nEpoch: 784. Loss: 0.2120. Acc.: 70.87%\nEpoch 784 best model saved with accuracy: 70.87%\nEpoch: 785. Loss: 0.2034. Acc.: 70.60%\nEpoch: 786. Loss: 0.2061. Acc.: 69.82%\nEpoch: 787. Loss: 0.2165. Acc.: 69.03%\nEpoch: 788. Loss: 0.2055. Acc.: 68.90%\nEpoch: 789. Loss: 0.2144. Acc.: 69.03%\nEpoch: 790. Loss: 0.2030. Acc.: 70.47%\nEpoch: 791. Loss: 0.2085. Acc.: 69.29%\nEpoch: 792. Loss: 0.1985. Acc.: 70.47%\nEpoch: 793. Loss: 0.1935. Acc.: 70.87%\nEpoch: 794. Loss: 0.2030. Acc.: 70.47%\nEpoch: 795. Loss: 0.2104. Acc.: 68.24%\nEpoch: 796. Loss: 0.2146. Acc.: 68.77%\nEpoch: 797. Loss: 0.2039. Acc.: 69.55%\nEpoch: 798. Loss: 0.1831. Acc.: 69.95%\nEpoch: 799. Loss: 0.1889. Acc.: 70.60%\nEpoch: 800. Loss: 0.1860. Acc.: 70.60%\nEpoch: 801. Loss: 0.1952. Acc.: 70.34%\nEpoch: 802. Loss: 0.2049. Acc.: 70.73%\nEpoch: 803. Loss: 0.1837. Acc.: 69.82%\nEpoch: 804. Loss: 0.1874. Acc.: 69.55%\nEpoch: 805. Loss: 0.1929. Acc.: 70.21%\nEpoch: 806. Loss: 0.1908. Acc.: 69.82%\nEpoch: 807. Loss: 0.1889. Acc.: 68.77%\nEpoch: 808. Loss: 0.1874. Acc.: 69.82%\nEpoch: 809. Loss: 0.1881. Acc.: 70.47%\nEpoch: 810. Loss: 0.1853. Acc.: 69.69%\nEpoch: 811. Loss: 0.1780. Acc.: 69.55%\nEpoch: 812. Loss: 0.1908. Acc.: 71.39%\nEpoch 812 best model saved with accuracy: 71.39%\nEpoch: 813. Loss: 0.2019. Acc.: 72.70%\nEpoch 813 best model saved with accuracy: 72.70%\nEpoch: 814. Loss: 0.1953. Acc.: 71.13%\nEpoch: 815. Loss: 0.1924. Acc.: 71.52%\nEpoch: 816. Loss: 0.1893. Acc.: 70.34%\nEpoch: 817. Loss: 0.1819. Acc.: 70.34%\nEpoch: 818. Loss: 0.1843. Acc.: 69.82%\nEpoch: 819. Loss: 0.1755. Acc.: 69.95%\nEpoch: 820. Loss: 0.1850. Acc.: 70.21%\nEpoch: 821. Loss: 0.1786. Acc.: 71.26%\nEpoch: 822. Loss: 0.1827. Acc.: 71.52%\nEpoch: 823. Loss: 0.1827. Acc.: 70.60%\nEpoch: 824. Loss: 0.1732. Acc.: 71.13%\nEpoch: 825. Loss: 0.1895. Acc.: 70.60%\nEpoch: 826. Loss: 0.1859. Acc.: 70.21%\nEpoch: 827. Loss: 0.1811. Acc.: 70.87%\nEpoch: 828. Loss: 0.1759. Acc.: 70.47%\nEpoch: 829. Loss: 0.1838. Acc.: 69.55%\nEpoch: 830. Loss: 0.1899. Acc.: 70.73%\nEpoch: 831. Loss: 0.1967. Acc.: 70.08%\nEpoch: 832. Loss: 0.1759. Acc.: 71.39%\nEpoch: 833. Loss: 0.1827. Acc.: 70.34%\nEpoch: 834. Loss: 0.1763. Acc.: 70.08%\nEpoch: 835. Loss: 0.1669. Acc.: 70.73%\nEpoch: 836. Loss: 0.1714. Acc.: 69.82%\nEpoch: 837. Loss: 0.1690. Acc.: 69.69%\nEpoch: 838. Loss: 0.1683. Acc.: 69.03%\nEpoch: 839. Loss: 0.1695. Acc.: 69.16%\nEpoch: 840. Loss: 0.1686. Acc.: 68.90%\nEpoch: 841. Loss: 0.1685. Acc.: 69.42%\nEpoch: 842. Loss: 0.1658. Acc.: 69.95%\nEpoch: 843. Loss: 0.1681. Acc.: 70.21%\nEpoch: 844. Loss: 0.1795. Acc.: 71.52%\nEpoch: 845. Loss: 0.1726. Acc.: 71.26%\nEpoch: 846. Loss: 0.1725. Acc.: 70.21%\nEpoch: 847. Loss: 0.1646. Acc.: 70.60%\nEpoch: 848. Loss: 0.1664. Acc.: 71.92%\nEpoch: 849. Loss: 0.1720. Acc.: 72.70%\nEpoch: 850. Loss: 0.1637. Acc.: 71.39%\nEpoch: 851. Loss: 0.1661. Acc.: 71.92%\nEpoch: 852. Loss: 0.1595. Acc.: 71.52%\nEpoch: 853. Loss: 0.1660. Acc.: 71.13%\nEpoch: 854. Loss: 0.1932. Acc.: 70.60%\nEpoch: 855. Loss: 0.1725. Acc.: 69.82%\nEpoch: 856. Loss: 0.1667. Acc.: 69.55%\nEpoch: 857. Loss: 0.1679. Acc.: 69.95%\nEpoch: 858. Loss: 0.1631. Acc.: 70.21%\nEpoch: 859. Loss: 0.1863. Acc.: 71.52%\nEpoch: 860. Loss: 0.1691. Acc.: 70.34%\nEpoch: 861. Loss: 0.1615. Acc.: 69.55%\nEpoch: 862. Loss: 0.1624. Acc.: 69.82%\nEpoch: 863. Loss: 0.1667. Acc.: 69.82%\nEpoch: 864. Loss: 0.1716. Acc.: 71.39%\nEpoch: 865. Loss: 0.1710. Acc.: 70.87%\nEpoch: 866. Loss: 0.1632. Acc.: 71.00%\nEpoch: 867. Loss: 0.1503. Acc.: 70.73%\nEpoch: 868. Loss: 0.1584. Acc.: 70.87%\nEpoch: 869. Loss: 0.1604. Acc.: 71.00%\nEpoch: 870. Loss: 0.1567. Acc.: 71.39%\nEpoch: 871. Loss: 0.1717. Acc.: 71.52%\nEpoch: 872. Loss: 0.1751. Acc.: 72.44%\nEpoch: 873. Loss: 0.1646. Acc.: 71.92%\nEpoch: 874. Loss: 0.1577. Acc.: 71.52%\nEpoch: 875. Loss: 0.1662. Acc.: 70.47%\nEpoch: 876. Loss: 0.1591. Acc.: 70.60%\nEpoch: 877. Loss: 0.1604. Acc.: 71.00%\nEpoch: 878. Loss: 0.1546. Acc.: 71.00%\nEpoch: 879. Loss: 0.1496. Acc.: 69.55%\nEpoch: 880. Loss: 0.1714. Acc.: 69.95%\nEpoch: 881. Loss: 0.1618. Acc.: 71.00%\nEpoch: 882. Loss: 0.1546. Acc.: 70.08%\nEpoch: 883. Loss: 0.1633. Acc.: 69.82%\nEpoch: 884. Loss: 0.1520. Acc.: 69.16%\nEpoch: 885. Loss: 0.1513. Acc.: 69.82%\nEpoch: 886. Loss: 0.1585. Acc.: 70.08%\nEpoch: 887. Loss: 0.1654. Acc.: 69.55%\nEpoch: 888. Loss: 0.1648. Acc.: 71.00%\nEpoch: 889. Loss: 0.1643. Acc.: 69.82%\nEpoch: 890. Loss: 0.1526. Acc.: 69.55%\nEpoch: 891. Loss: 0.1555. Acc.: 70.34%\nEpoch: 892. Loss: 0.1504. Acc.: 70.21%\nEpoch: 893. Loss: 0.1546. Acc.: 70.73%\nEpoch: 894. Loss: 0.1529. Acc.: 71.65%\nEpoch: 895. Loss: 0.1504. Acc.: 70.47%\nEpoch: 896. Loss: 0.1374. Acc.: 71.00%\nEpoch: 897. Loss: 0.1540. Acc.: 69.95%\nEpoch: 898. Loss: 0.1552. Acc.: 70.47%\nEpoch: 899. Loss: 0.1579. Acc.: 71.65%\nEpoch: 900. Loss: 0.1430. Acc.: 71.92%\nEpoch: 901. Loss: 0.1438. Acc.: 71.00%\nEpoch: 902. Loss: 0.1506. Acc.: 71.39%\nEpoch: 903. Loss: 0.1559. Acc.: 71.39%\nEpoch: 904. Loss: 0.1446. Acc.: 72.18%\nEpoch: 905. Loss: 0.1437. Acc.: 72.18%\nEpoch: 906. Loss: 0.1421. Acc.: 72.05%\nEpoch: 907. Loss: 0.1500. Acc.: 71.13%\nEpoch: 908. Loss: 0.1568. Acc.: 71.39%\nEpoch: 909. Loss: 0.1653. Acc.: 71.78%\nEpoch: 910. Loss: 0.1695. Acc.: 71.00%\nEpoch: 911. Loss: 0.1532. Acc.: 71.39%\nEpoch: 912. Loss: 0.1407. Acc.: 71.39%\nEpoch: 913. Loss: 0.1540. Acc.: 70.47%\nEpoch: 914. Loss: 0.1627. Acc.: 70.47%\nEpoch: 915. Loss: 0.1410. Acc.: 69.95%\nEpoch: 916. Loss: 0.1428. Acc.: 70.21%\nEpoch: 917. Loss: 0.1443. Acc.: 71.13%\nEpoch: 918. Loss: 0.1693. Acc.: 70.60%\nEpoch: 919. Loss: 0.1471. Acc.: 70.87%\nEpoch: 920. Loss: 0.1510. Acc.: 72.31%\nEpoch: 921. Loss: 0.1511. Acc.: 72.57%\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 922. Loss: 0.1584. Acc.: 72.18%\nEpoch: 923. Loss: 0.1550. Acc.: 71.39%\nEpoch: 924. Loss: 0.1573. Acc.: 72.05%\nEpoch: 925. Loss: 0.1562. Acc.: 71.78%\nEpoch: 926. Loss: 0.1536. Acc.: 71.52%\nEpoch: 927. Loss: 0.1576. Acc.: 72.31%\nEpoch: 928. Loss: 0.1476. Acc.: 71.52%\nEpoch: 929. Loss: 0.1456. Acc.: 70.60%\nEpoch: 930. Loss: 0.1515. Acc.: 70.60%\nEpoch: 931. Loss: 0.1501. Acc.: 71.00%\nEpoch: 932. Loss: 0.1631. Acc.: 70.47%\nEpoch: 933. Loss: 0.1412. Acc.: 70.73%\nEpoch: 934. Loss: 0.1562. Acc.: 69.42%\nEpoch: 935. Loss: 0.1452. Acc.: 69.55%\nEpoch: 936. Loss: 0.1432. Acc.: 70.34%\nEpoch: 937. Loss: 0.1478. Acc.: 71.13%\nEpoch: 938. Loss: 0.1378. Acc.: 71.13%\nEpoch: 939. Loss: 0.1472. Acc.: 71.26%\nEpoch: 940. Loss: 0.1530. Acc.: 70.21%\nEpoch: 941. Loss: 0.1431. Acc.: 69.82%\nEpoch: 942. Loss: 0.1511. Acc.: 70.47%\nEpoch: 943. Loss: 0.1398. Acc.: 69.69%\nEpoch: 944. Loss: 0.1377. Acc.: 69.82%\nEpoch: 945. Loss: 0.1397. Acc.: 69.55%\nEpoch: 946. Loss: 0.1382. Acc.: 71.00%\nEpoch: 947. Loss: 0.1350. Acc.: 70.87%\nEpoch: 948. Loss: 0.1343. Acc.: 70.87%\nEpoch: 949. Loss: 0.1397. Acc.: 71.78%\nEpoch: 950. Loss: 0.1412. Acc.: 71.39%\nEpoch: 951. Loss: 0.1493. Acc.: 71.52%\nEpoch: 952. Loss: 0.1451. Acc.: 71.00%\nEpoch: 953. Loss: 0.1474. Acc.: 70.60%\nEpoch: 954. Loss: 0.1423. Acc.: 70.87%\nEpoch: 955. Loss: 0.1476. Acc.: 70.47%\nEpoch: 956. Loss: 0.1448. Acc.: 70.47%\nEpoch: 957. Loss: 0.1428. Acc.: 70.47%\nEpoch: 958. Loss: 0.1466. Acc.: 70.47%\nEpoch: 959. Loss: 0.1399. Acc.: 69.42%\nEpoch: 960. Loss: 0.1379. Acc.: 69.69%\nEpoch: 961. Loss: 0.1439. Acc.: 70.34%\nEpoch: 962. Loss: 0.1393. Acc.: 70.73%\nEpoch: 963. Loss: 0.1403. Acc.: 71.13%\nEpoch: 964. Loss: 0.1375. Acc.: 71.26%\nEpoch: 965. Loss: 0.1421. Acc.: 70.34%\nEpoch: 966. Loss: 0.1422. Acc.: 69.95%\nEpoch: 967. Loss: 0.1471. Acc.: 70.21%\nEpoch: 968. Loss: 0.1465. Acc.: 70.60%\nEpoch: 969. Loss: 0.1484. Acc.: 70.60%\nEpoch: 970. Loss: 0.1421. Acc.: 69.29%\nEpoch: 971. Loss: 0.1442. Acc.: 70.34%\nEpoch: 972. Loss: 0.1390. Acc.: 69.55%\nEpoch: 973. Loss: 0.1303. Acc.: 70.73%\nEpoch: 974. Loss: 0.1450. Acc.: 71.26%\nEpoch: 975. Loss: 0.1577. Acc.: 71.92%\nEpoch: 976. Loss: 0.1365. Acc.: 71.39%\nEpoch: 977. Loss: 0.1406. Acc.: 70.08%\nEpoch: 978. Loss: 0.1335. Acc.: 69.69%\nEpoch: 979. Loss: 0.1421. Acc.: 69.82%\nEpoch: 980. Loss: 0.1454. Acc.: 69.55%\nEpoch: 981. Loss: 0.1461. Acc.: 70.47%\nEpoch: 982. Loss: 0.1309. Acc.: 71.52%\nEpoch: 983. Loss: 0.1239. Acc.: 71.78%\nEpoch: 984. Loss: 0.1478. Acc.: 71.13%\nEpoch: 985. Loss: 0.1365. Acc.: 70.73%\nEpoch: 986. Loss: 0.1439. Acc.: 70.60%\nEpoch: 987. Loss: 0.1431. Acc.: 71.13%\nEpoch: 988. Loss: 0.1320. Acc.: 69.95%\nEpoch: 989. Loss: 0.1290. Acc.: 69.69%\nEpoch: 990. Loss: 0.1432. Acc.: 69.69%\nEpoch: 991. Loss: 0.1273. Acc.: 70.73%\nEpoch: 992. Loss: 0.1348. Acc.: 70.21%\nEpoch: 993. Loss: 0.1366. Acc.: 71.13%\nEpoch: 994. Loss: 0.1303. Acc.: 72.31%\nEpoch: 995. Loss: 0.1345. Acc.: 69.95%\nEpoch: 996. Loss: 0.1522. Acc.: 68.77%\nEpoch: 997. Loss: 0.1501. Acc.: 68.77%\nEpoch: 998. Loss: 0.1401. Acc.: 69.55%\nEpoch: 999. Loss: 0.1343. Acc.: 70.87%\nEpoch: 1000. Loss: 0.1355. Acc.: 71.13%\nEpoch: 1001. Loss: 0.1390. Acc.: 70.47%\nEpoch: 1002. Loss: 0.1371. Acc.: 71.00%\nEpoch: 1003. Loss: 0.1317. Acc.: 71.92%\nEpoch: 1004. Loss: 0.1465. Acc.: 71.52%\nEpoch: 1005. Loss: 0.1343. Acc.: 70.60%\nEpoch: 1006. Loss: 0.1305. Acc.: 70.34%\nEpoch: 1007. Loss: 0.1330. Acc.: 71.13%\nEpoch: 1008. Loss: 0.1409. Acc.: 71.65%\nEpoch: 1009. Loss: 0.1392. Acc.: 70.73%\nEpoch: 1010. Loss: 0.1203. Acc.: 70.73%\nEpoch: 1011. Loss: 0.1273. Acc.: 70.60%\nEpoch: 1012. Loss: 0.1236. Acc.: 71.65%\nEpoch: 1013. Loss: 0.1367. Acc.: 71.92%\nEpoch: 1014. Loss: 0.1342. Acc.: 71.92%\nEpoch: 1015. Loss: 0.1343. Acc.: 71.65%\nEpoch: 1016. Loss: 0.1323. Acc.: 72.05%\nEpoch: 1017. Loss: 0.1245. Acc.: 71.26%\nEpoch: 1018. Loss: 0.1238. Acc.: 71.39%\nEpoch: 1019. Loss: 0.1326. Acc.: 71.13%\nEpoch: 1020. Loss: 0.1347. Acc.: 70.60%\nEpoch: 1021. Loss: 0.1294. Acc.: 71.52%\nEpoch: 1022. Loss: 0.1366. Acc.: 72.31%\nEpoch: 1023. Loss: 0.1411. Acc.: 70.87%\nEpoch: 1024. Loss: 0.1317. Acc.: 71.78%\nEpoch: 1024. Loss: 0.1317. Acc.: 71.78%\nEpoch: 1025. Loss: 0.1402. Acc.: 71.39%\nEpoch: 1026. Loss: 0.1273. Acc.: 71.78%\nEpoch: 1027. Loss: 0.1345. Acc.: 71.26%\nEpoch: 1028. Loss: 0.1319. Acc.: 72.05%\nEpoch: 1029. Loss: 0.1371. Acc.: 71.52%\nEpoch: 1030. Loss: 0.1388. Acc.: 70.73%\nEpoch: 1031. Loss: 0.1374. Acc.: 70.47%\nEpoch: 1032. Loss: 0.1302. Acc.: 70.47%\nEpoch: 1033. Loss: 0.1313. Acc.: 71.92%\nEpoch: 1034. Loss: 0.1307. Acc.: 71.78%\nEpoch: 1035. Loss: 0.1291. Acc.: 71.26%\nEpoch: 1036. Loss: 0.1299. Acc.: 70.21%\nEpoch: 1037. Loss: 0.1251. Acc.: 71.92%\nEpoch: 1038. Loss: 0.1383. Acc.: 71.13%\nEpoch: 1039. Loss: 0.1293. Acc.: 70.21%\nEpoch: 1040. Loss: 0.1308. Acc.: 71.52%\nEpoch: 1041. Loss: 0.1304. Acc.: 71.52%\nEpoch: 1042. Loss: 0.1200. Acc.: 71.78%\nEpoch: 1043. Loss: 0.1393. Acc.: 72.83%\nEpoch 1043 best model saved with accuracy: 72.83%\nEpoch: 1044. Loss: 0.1390. Acc.: 71.52%\nEpoch: 1045. Loss: 0.1266. Acc.: 70.21%\nEpoch: 1046. Loss: 0.1329. Acc.: 69.55%\nEpoch: 1047. Loss: 0.1305. Acc.: 69.82%\nEpoch: 1048. Loss: 0.1185. Acc.: 70.73%\nEpoch: 1049. Loss: 0.1304. Acc.: 70.21%\nEpoch: 1050. Loss: 0.1402. Acc.: 70.87%\nEpoch: 1051. Loss: 0.1269. Acc.: 71.26%\nEpoch: 1052. Loss: 0.1221. Acc.: 72.70%\nEpoch: 1053. Loss: 0.1237. Acc.: 70.47%\nEpoch: 1054. Loss: 0.1316. Acc.: 71.00%\nEpoch: 1055. Loss: 0.1241. Acc.: 71.92%\nEpoch: 1056. Loss: 0.1409. Acc.: 71.39%\nEpoch: 1057. Loss: 0.1284. Acc.: 69.82%\nEpoch: 1058. Loss: 0.1229. Acc.: 70.47%\nEpoch: 1059. Loss: 0.1209. Acc.: 71.13%\nEpoch: 1060. Loss: 0.1291. Acc.: 71.39%\nEpoch: 1061. Loss: 0.1247. Acc.: 71.13%\nEpoch: 1062. Loss: 0.1160. Acc.: 71.78%\nEpoch: 1063. Loss: 0.1272. Acc.: 72.44%\nEpoch: 1064. Loss: 0.1384. Acc.: 71.52%\nEpoch: 1065. Loss: 0.1312. Acc.: 71.78%\nEpoch: 1066. Loss: 0.1252. Acc.: 71.13%\nEpoch: 1067. Loss: 0.1404. Acc.: 71.00%\nEpoch: 1068. Loss: 0.1310. Acc.: 70.73%\nEpoch: 1069. Loss: 0.1280. Acc.: 72.05%\nEpoch: 1070. Loss: 0.1258. Acc.: 72.44%\nEpoch: 1071. Loss: 0.1139. Acc.: 72.31%\nEpoch: 1072. Loss: 0.1226. Acc.: 72.44%\nEpoch: 1073. Loss: 0.1276. Acc.: 72.05%\nEpoch: 1074. Loss: 0.1113. Acc.: 72.31%\nEpoch: 1075. Loss: 0.1306. Acc.: 72.05%\nEpoch: 1076. Loss: 0.1230. Acc.: 71.65%\nEpoch: 1077. Loss: 0.1352. Acc.: 71.52%\nEpoch: 1078. Loss: 0.1300. Acc.: 71.00%\nEpoch: 1079. Loss: 0.1126. Acc.: 71.65%\nEpoch: 1080. Loss: 0.1204. Acc.: 71.00%\nEpoch: 1081. Loss: 0.1255. Acc.: 70.87%\nEpoch: 1082. Loss: 0.1362. Acc.: 71.00%\nEpoch: 1083. Loss: 0.1271. Acc.: 70.60%\nEpoch: 1084. Loss: 0.1282. Acc.: 71.00%\nEpoch: 1085. Loss: 0.1236. Acc.: 70.73%\nEpoch: 1086. Loss: 0.1354. Acc.: 72.70%\nEpoch: 1087. Loss: 0.1272. Acc.: 72.31%\nEpoch: 1088. Loss: 0.1159. Acc.: 71.52%\nEpoch: 1089. Loss: 0.1249. Acc.: 71.26%\nEpoch: 1090. Loss: 0.1156. Acc.: 70.60%\nEpoch: 1091. Loss: 0.1234. Acc.: 71.39%\nEpoch: 1092. Loss: 0.1205. Acc.: 71.65%\nEpoch: 1093. Loss: 0.1322. Acc.: 72.31%\nEpoch: 1094. Loss: 0.1272. Acc.: 72.70%\nEpoch: 1095. Loss: 0.1210. Acc.: 72.31%\nEpoch: 1096. Loss: 0.1277. Acc.: 71.78%\nEpoch: 1097. Loss: 0.1257. Acc.: 72.18%\nEpoch: 1098. Loss: 0.1194. Acc.: 71.26%\nEpoch: 1099. Loss: 0.1312. Acc.: 70.73%\nEpoch: 1100. Loss: 0.1239. Acc.: 71.00%\nEpoch: 1101. Loss: 0.1157. Acc.: 70.87%\nEpoch: 1102. Loss: 0.1225. Acc.: 70.73%\nEpoch: 1103. Loss: 0.1290. Acc.: 70.87%\nEpoch: 1104. Loss: 0.1205. Acc.: 70.87%\nEpoch: 1105. Loss: 0.1154. Acc.: 71.26%\nEpoch: 1106. Loss: 0.1179. Acc.: 70.21%\nEpoch: 1107. Loss: 0.1159. Acc.: 70.60%\nEpoch: 1108. Loss: 0.1158. Acc.: 70.73%\nEpoch: 1109. Loss: 0.1266. Acc.: 71.65%\nEpoch: 1110. Loss: 0.1216. Acc.: 72.18%\nEpoch: 1111. Loss: 0.1263. Acc.: 72.70%\nEpoch: 1112. Loss: 0.1281. Acc.: 72.31%\nEpoch: 1113. Loss: 0.1176. Acc.: 72.18%\nEpoch: 1114. Loss: 0.1197. Acc.: 72.57%\nEpoch: 1115. Loss: 0.1150. Acc.: 72.31%\nEpoch: 1116. Loss: 0.1266. Acc.: 72.97%\nEpoch 1116 best model saved with accuracy: 72.97%\nEpoch: 1117. Loss: 0.1135. Acc.: 73.23%\nEpoch 1117 best model saved with accuracy: 73.23%\nEpoch: 1118. Loss: 0.1164. Acc.: 72.05%\nEpoch: 1119. Loss: 0.1160. Acc.: 72.05%\nEpoch: 1120. Loss: 0.1199. Acc.: 71.13%\nEpoch: 1121. Loss: 0.1224. Acc.: 71.92%\nEpoch: 1122. Loss: 0.1152. Acc.: 70.73%\nEpoch: 1123. Loss: 0.1276. Acc.: 71.65%\nEpoch: 1124. Loss: 0.1051. Acc.: 71.78%\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 1125. Loss: 0.1161. Acc.: 71.52%\nEpoch: 1126. Loss: 0.1185. Acc.: 72.05%\nEpoch: 1127. Loss: 0.1213. Acc.: 71.13%\nEpoch: 1128. Loss: 0.1107. Acc.: 70.87%\nEpoch: 1129. Loss: 0.1265. Acc.: 70.47%\nEpoch: 1130. Loss: 0.1202. Acc.: 71.52%\nEpoch: 1131. Loss: 0.1145. Acc.: 71.39%\nEpoch: 1132. Loss: 0.1305. Acc.: 72.05%\nEpoch: 1133. Loss: 0.1128. Acc.: 72.70%\nEpoch: 1134. Loss: 0.1159. Acc.: 72.83%\nEpoch: 1135. Loss: 0.1168. Acc.: 72.05%\nEpoch: 1136. Loss: 0.1229. Acc.: 72.83%\nEpoch: 1137. Loss: 0.1150. Acc.: 72.97%\nEpoch: 1138. Loss: 0.1137. Acc.: 71.52%\nEpoch: 1139. Loss: 0.1183. Acc.: 71.65%\nEpoch: 1140. Loss: 0.1234. Acc.: 72.05%\nEpoch: 1141. Loss: 0.1171. Acc.: 73.10%\nEpoch: 1142. Loss: 0.1188. Acc.: 73.23%\nEpoch: 1143. Loss: 0.1076. Acc.: 73.49%\nEpoch 1143 best model saved with accuracy: 73.49%\nEpoch: 1144. Loss: 0.1140. Acc.: 73.49%\nEpoch: 1145. Loss: 0.1234. Acc.: 72.57%\nEpoch: 1146. Loss: 0.1207. Acc.: 72.05%\nEpoch: 1147. Loss: 0.1187. Acc.: 71.65%\nEpoch: 1148. Loss: 0.1034. Acc.: 71.78%\nEpoch: 1149. Loss: 0.1152. Acc.: 71.78%\nEpoch: 1150. Loss: 0.1175. Acc.: 72.31%\nEpoch: 1151. Loss: 0.1240. Acc.: 72.44%\nEpoch: 1152. Loss: 0.1271. Acc.: 71.92%\nEpoch: 1153. Loss: 0.1164. Acc.: 72.31%\nEpoch: 1154. Loss: 0.1145. Acc.: 72.70%\nEpoch: 1155. Loss: 0.1208. Acc.: 71.65%\nEpoch: 1156. Loss: 0.1129. Acc.: 72.44%\nEpoch: 1157. Loss: 0.1105. Acc.: 72.18%\nEpoch: 1158. Loss: 0.1170. Acc.: 72.31%\nEpoch: 1159. Loss: 0.1155. Acc.: 72.31%\nEpoch: 1160. Loss: 0.1124. Acc.: 72.31%\nEpoch: 1161. Loss: 0.1210. Acc.: 71.92%\nEpoch: 1162. Loss: 0.1171. Acc.: 72.18%\nEpoch: 1163. Loss: 0.1125. Acc.: 71.78%\nEpoch: 1164. Loss: 0.1152. Acc.: 71.92%\nEpoch: 1165. Loss: 0.1187. Acc.: 72.18%\nEpoch: 1166. Loss: 0.1213. Acc.: 71.78%\nEpoch: 1167. Loss: 0.1165. Acc.: 71.13%\nEpoch: 1168. Loss: 0.1228. Acc.: 72.31%\nEpoch: 1169. Loss: 0.1232. Acc.: 72.44%\nEpoch: 1170. Loss: 0.1112. Acc.: 71.52%\nEpoch: 1171. Loss: 0.1193. Acc.: 71.65%\nEpoch: 1172. Loss: 0.1172. Acc.: 71.78%\nEpoch: 1173. Loss: 0.1023. Acc.: 71.52%\nEpoch: 1174. Loss: 0.1178. Acc.: 72.83%\nEpoch: 1175. Loss: 0.1185. Acc.: 71.92%\nEpoch: 1176. Loss: 0.1350. Acc.: 71.92%\nEpoch: 1177. Loss: 0.1254. Acc.: 71.52%\nEpoch: 1178. Loss: 0.1231. Acc.: 71.13%\nEpoch: 1179. Loss: 0.1081. Acc.: 73.23%\nEpoch: 1180. Loss: 0.1217. Acc.: 73.88%\nEpoch 1180 best model saved with accuracy: 73.88%\nEpoch: 1181. Loss: 0.1038. Acc.: 74.41%\nEpoch 1181 best model saved with accuracy: 74.41%\nEpoch: 1182. Loss: 0.1073. Acc.: 73.62%\nEpoch: 1183. Loss: 0.1287. Acc.: 72.05%\nEpoch: 1184. Loss: 0.1152. Acc.: 71.52%\nEpoch: 1185. Loss: 0.1162. Acc.: 71.65%\nEpoch: 1186. Loss: 0.1088. Acc.: 71.65%\nEpoch: 1187. Loss: 0.1135. Acc.: 71.39%\nEpoch: 1188. Loss: 0.1104. Acc.: 72.70%\nEpoch: 1189. Loss: 0.1063. Acc.: 71.52%\nEpoch: 1190. Loss: 0.1282. Acc.: 70.08%\nEpoch: 1191. Loss: 0.1169. Acc.: 70.87%\nEpoch: 1192. Loss: 0.1084. Acc.: 70.87%\nEpoch: 1193. Loss: 0.1229. Acc.: 71.13%\nEpoch: 1194. Loss: 0.1215. Acc.: 71.00%\nEpoch: 1195. Loss: 0.1181. Acc.: 71.26%\nEpoch: 1196. Loss: 0.1079. Acc.: 71.39%\nEpoch: 1197. Loss: 0.1184. Acc.: 72.44%\nEpoch: 1198. Loss: 0.1152. Acc.: 71.92%\nEpoch: 1199. Loss: 0.1336. Acc.: 71.52%\nEpoch: 1200. Loss: 0.1027. Acc.: 71.65%\nEpoch: 1201. Loss: 0.1116. Acc.: 72.57%\nEpoch: 1202. Loss: 0.1148. Acc.: 72.57%\nEpoch: 1203. Loss: 0.1111. Acc.: 72.44%\nEpoch: 1204. Loss: 0.1135. Acc.: 72.44%\nEpoch: 1205. Loss: 0.1189. Acc.: 72.44%\nEpoch: 1206. Loss: 0.1165. Acc.: 71.92%\nEpoch: 1207. Loss: 0.1170. Acc.: 72.97%\nEpoch: 1208. Loss: 0.1152. Acc.: 72.70%\nEpoch: 1209. Loss: 0.1082. Acc.: 72.31%\nEpoch: 1210. Loss: 0.1085. Acc.: 72.97%\nEpoch: 1211. Loss: 0.1060. Acc.: 73.36%\nEpoch: 1212. Loss: 0.1102. Acc.: 72.05%\nEpoch: 1213. Loss: 0.1016. Acc.: 72.44%\nEpoch: 1214. Loss: 0.1238. Acc.: 71.92%\nEpoch: 1215. Loss: 0.1214. Acc.: 72.70%\nEpoch: 1216. Loss: 0.1064. Acc.: 72.97%\nEpoch: 1217. Loss: 0.1055. Acc.: 72.97%\nEpoch: 1218. Loss: 0.1151. Acc.: 72.83%\nEpoch: 1219. Loss: 0.1114. Acc.: 72.18%\nEpoch: 1220. Loss: 0.1159. Acc.: 72.05%\nEpoch: 1221. Loss: 0.1131. Acc.: 72.05%\nEpoch: 1222. Loss: 0.1107. Acc.: 71.78%\nEpoch: 1223. Loss: 0.1095. Acc.: 71.92%\nEpoch: 1224. Loss: 0.1036. Acc.: 73.49%\nEpoch: 1225. Loss: 0.0972. Acc.: 73.75%\nEpoch: 1226. Loss: 0.1054. Acc.: 73.23%\nEpoch: 1227. Loss: 0.1041. Acc.: 72.97%\nEpoch: 1228. Loss: 0.1061. Acc.: 72.83%\nEpoch: 1229. Loss: 0.1053. Acc.: 72.05%\nEpoch: 1230. Loss: 0.0957. Acc.: 73.23%\nEpoch: 1231. Loss: 0.0997. Acc.: 72.57%\nEpoch: 1232. Loss: 0.1130. Acc.: 72.83%\nEpoch: 1233. Loss: 0.1188. Acc.: 72.44%\nEpoch: 1234. Loss: 0.1082. Acc.: 72.83%\nEpoch: 1235. Loss: 0.1103. Acc.: 72.70%\nEpoch: 1236. Loss: 0.1090. Acc.: 73.36%\nEpoch: 1237. Loss: 0.1115. Acc.: 73.23%\nEpoch: 1238. Loss: 0.1137. Acc.: 72.05%\nEpoch: 1239. Loss: 0.1154. Acc.: 72.18%\nEpoch: 1240. Loss: 0.1057. Acc.: 71.26%\nEpoch: 1241. Loss: 0.1059. Acc.: 71.52%\nEpoch: 1242. Loss: 0.1129. Acc.: 72.44%\nEpoch: 1243. Loss: 0.1067. Acc.: 72.70%\nEpoch: 1244. Loss: 0.1034. Acc.: 73.23%\nEpoch: 1245. Loss: 0.1118. Acc.: 72.97%\nEpoch: 1246. Loss: 0.1120. Acc.: 72.18%\nEpoch: 1247. Loss: 0.1111. Acc.: 71.78%\nEpoch: 1248. Loss: 0.1035. Acc.: 72.70%\nEpoch: 1249. Loss: 0.1044. Acc.: 71.39%\nEpoch: 1250. Loss: 0.1075. Acc.: 71.65%\nEpoch: 1251. Loss: 0.0999. Acc.: 72.57%\nEpoch: 1252. Loss: 0.1050. Acc.: 71.39%\nEpoch: 1253. Loss: 0.1150. Acc.: 71.52%\nEpoch: 1254. Loss: 0.1230. Acc.: 71.78%\nEpoch: 1255. Loss: 0.1170. Acc.: 73.49%\nEpoch: 1256. Loss: 0.1093. Acc.: 72.83%\nEpoch: 1257. Loss: 0.1084. Acc.: 72.44%\nEpoch: 1258. Loss: 0.1265. Acc.: 72.44%\nEpoch: 1259. Loss: 0.1011. Acc.: 72.70%\nEpoch: 1260. Loss: 0.1046. Acc.: 72.18%\nEpoch: 1261. Loss: 0.1063. Acc.: 72.05%\nEpoch: 1262. Loss: 0.1064. Acc.: 72.44%\nEpoch: 1263. Loss: 0.1155. Acc.: 71.65%\nEpoch: 1264. Loss: 0.1079. Acc.: 70.73%\nEpoch: 1265. Loss: 0.1145. Acc.: 70.47%\nEpoch: 1266. Loss: 0.1026. Acc.: 71.39%\nEpoch: 1267. Loss: 0.0992. Acc.: 70.73%\nEpoch: 1268. Loss: 0.1146. Acc.: 70.47%\nEpoch: 1269. Loss: 0.1072. Acc.: 71.13%\nEpoch: 1270. Loss: 0.1134. Acc.: 69.69%\nEpoch: 1271. Loss: 0.1042. Acc.: 70.34%\nEpoch: 1272. Loss: 0.1235. Acc.: 72.97%\nEpoch: 1273. Loss: 0.1192. Acc.: 72.70%\nEpoch: 1274. Loss: 0.1075. Acc.: 71.13%\nEpoch: 1275. Loss: 0.1040. Acc.: 71.00%\nEpoch: 1276. Loss: 0.1145. Acc.: 72.57%\nEpoch: 1277. Loss: 0.1024. Acc.: 72.70%\nEpoch: 1278. Loss: 0.1049. Acc.: 72.05%\nEpoch: 1279. Loss: 0.0911. Acc.: 70.73%\nEpoch: 1280. Loss: 0.1003. Acc.: 71.39%\nEpoch: 1281. Loss: 0.1135. Acc.: 71.26%\nEpoch: 1282. Loss: 0.1050. Acc.: 71.00%\nEpoch: 1283. Loss: 0.1047. Acc.: 71.78%\nEpoch: 1284. Loss: 0.0995. Acc.: 71.52%\nEpoch: 1285. Loss: 0.1022. Acc.: 72.18%\nEpoch: 1286. Loss: 0.1097. Acc.: 71.39%\nEpoch: 1287. Loss: 0.1021. Acc.: 71.13%\nEpoch: 1288. Loss: 0.1069. Acc.: 70.60%\nEpoch: 1289. Loss: 0.1141. Acc.: 71.26%\nEpoch: 1290. Loss: 0.1059. Acc.: 72.57%\nEpoch: 1291. Loss: 0.1063. Acc.: 71.39%\nEpoch: 1292. Loss: 0.1034. Acc.: 71.78%\nEpoch: 1293. Loss: 0.0991. Acc.: 71.92%\nEpoch: 1294. Loss: 0.1065. Acc.: 72.57%\nEpoch: 1295. Loss: 0.0989. Acc.: 71.78%\nEpoch: 1296. Loss: 0.1018. Acc.: 70.34%\nEpoch: 1297. Loss: 0.1056. Acc.: 70.87%\nEpoch: 1298. Loss: 0.0953. Acc.: 70.60%\nEpoch: 1299. Loss: 0.1089. Acc.: 72.05%\nEpoch: 1300. Loss: 0.0994. Acc.: 71.78%\nEpoch: 1301. Loss: 0.1089. Acc.: 72.18%\nEpoch: 1302. Loss: 0.1014. Acc.: 72.44%\nEpoch: 1303. Loss: 0.1111. Acc.: 71.78%\nEpoch: 1304. Loss: 0.1221. Acc.: 71.52%\nEpoch: 1305. Loss: 0.0983. Acc.: 72.31%\nEpoch: 1306. Loss: 0.0998. Acc.: 72.97%\nEpoch: 1307. Loss: 0.1097. Acc.: 73.62%\nEpoch: 1308. Loss: 0.1080. Acc.: 72.31%\nEpoch: 1309. Loss: 0.1116. Acc.: 71.92%\nEpoch: 1310. Loss: 0.1023. Acc.: 71.65%\nEpoch: 1311. Loss: 0.0933. Acc.: 71.78%\nEpoch: 1312. Loss: 0.1069. Acc.: 71.52%\nEpoch: 1313. Loss: 0.1055. Acc.: 73.23%\nEpoch: 1314. Loss: 0.1060. Acc.: 72.83%\nEpoch: 1315. Loss: 0.1005. Acc.: 72.70%\nEpoch: 1316. Loss: 0.1037. Acc.: 71.92%\nEpoch: 1317. Loss: 0.0971. Acc.: 71.65%\nEpoch: 1318. Loss: 0.1154. Acc.: 72.31%\nEpoch: 1319. Loss: 0.1085. Acc.: 72.31%\nEpoch: 1320. Loss: 0.1078. Acc.: 71.52%\nEpoch: 1321. Loss: 0.1013. Acc.: 71.78%\nEpoch: 1322. Loss: 0.1022. Acc.: 72.83%\nEpoch: 1323. Loss: 0.1104. Acc.: 72.83%\nEpoch: 1324. Loss: 0.1179. Acc.: 73.23%\nEpoch: 1325. Loss: 0.1105. Acc.: 73.88%\nEpoch: 1326. Loss: 0.1040. Acc.: 72.97%\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 1327. Loss: 0.1168. Acc.: 72.18%\nEpoch: 1328. Loss: 0.1006. Acc.: 70.47%\nEpoch: 1329. Loss: 0.1095. Acc.: 71.78%\nEpoch: 1330. Loss: 0.1040. Acc.: 72.05%\nEpoch: 1331. Loss: 0.1000. Acc.: 72.18%\nEpoch: 1332. Loss: 0.0991. Acc.: 72.31%\nEpoch: 1333. Loss: 0.1060. Acc.: 72.31%\nEpoch: 1334. Loss: 0.0912. Acc.: 71.78%\nEpoch: 1335. Loss: 0.1058. Acc.: 71.92%\nEpoch: 1336. Loss: 0.1089. Acc.: 72.44%\nEpoch: 1337. Loss: 0.1118. Acc.: 71.52%\nEpoch: 1338. Loss: 0.0981. Acc.: 71.78%\nEpoch: 1339. Loss: 0.0990. Acc.: 72.05%\nEpoch: 1340. Loss: 0.1035. Acc.: 72.70%\nEpoch: 1341. Loss: 0.1014. Acc.: 72.97%\nEpoch: 1342. Loss: 0.1072. Acc.: 72.70%\nEpoch: 1343. Loss: 0.1005. Acc.: 72.83%\nEpoch: 1344. Loss: 0.0978. Acc.: 73.49%\nEpoch: 1345. Loss: 0.1090. Acc.: 74.15%\nEpoch: 1346. Loss: 0.1071. Acc.: 73.10%\nEpoch: 1347. Loss: 0.1075. Acc.: 73.10%\nEpoch: 1348. Loss: 0.1067. Acc.: 71.92%\nEpoch: 1349. Loss: 0.1115. Acc.: 71.65%\nEpoch: 1350. Loss: 0.1048. Acc.: 73.23%\nEpoch: 1351. Loss: 0.1046. Acc.: 72.70%\nEpoch: 1352. Loss: 0.1067. Acc.: 72.05%\nEpoch: 1353. Loss: 0.0974. Acc.: 71.26%\nEpoch: 1354. Loss: 0.1060. Acc.: 70.87%\nEpoch: 1355. Loss: 0.1151. Acc.: 71.92%\nEpoch: 1356. Loss: 0.0885. Acc.: 71.78%\nEpoch: 1357. Loss: 0.0887. Acc.: 72.44%\nEpoch: 1358. Loss: 0.1006. Acc.: 72.18%\nEpoch: 1359. Loss: 0.0987. Acc.: 71.92%\nEpoch: 1360. Loss: 0.0977. Acc.: 71.52%\nEpoch: 1361. Loss: 0.1109. Acc.: 71.39%\nEpoch: 1362. Loss: 0.0989. Acc.: 71.00%\nEpoch: 1363. Loss: 0.1019. Acc.: 71.65%\nEpoch: 1364. Loss: 0.1106. Acc.: 71.78%\nEpoch: 1365. Loss: 0.1038. Acc.: 72.18%\nEpoch: 1366. Loss: 0.1102. Acc.: 71.39%\nEpoch: 1367. Loss: 0.0989. Acc.: 70.87%\nEpoch: 1368. Loss: 0.0971. Acc.: 71.39%\nEpoch: 1369. Loss: 0.1037. Acc.: 71.65%\nEpoch: 1370. Loss: 0.0970. Acc.: 70.60%\nEpoch: 1371. Loss: 0.0992. Acc.: 71.78%\nEpoch: 1372. Loss: 0.0999. Acc.: 71.13%\nEpoch: 1373. Loss: 0.1085. Acc.: 71.92%\nEpoch: 1374. Loss: 0.1027. Acc.: 71.92%\nEpoch: 1375. Loss: 0.1061. Acc.: 72.44%\nEpoch: 1376. Loss: 0.1003. Acc.: 72.31%\nEpoch: 1377. Loss: 0.0923. Acc.: 72.83%\nEpoch: 1378. Loss: 0.1012. Acc.: 72.97%\nEpoch: 1379. Loss: 0.0979. Acc.: 71.52%\nEpoch: 1380. Loss: 0.0946. Acc.: 71.52%\nEpoch: 1381. Loss: 0.0944. Acc.: 72.97%\nEpoch: 1382. Loss: 0.1048. Acc.: 73.62%\nEpoch: 1383. Loss: 0.1082. Acc.: 74.28%\nEpoch: 1384. Loss: 0.0951. Acc.: 73.10%\nEpoch: 1385. Loss: 0.1019. Acc.: 73.10%\nEpoch: 1386. Loss: 0.1008. Acc.: 71.78%\nEpoch: 1387. Loss: 0.1012. Acc.: 71.92%\nEpoch: 1388. Loss: 0.0945. Acc.: 71.26%\nEpoch: 1389. Loss: 0.0869. Acc.: 70.47%\nEpoch: 1390. Loss: 0.1010. Acc.: 71.26%\nEpoch: 1391. Loss: 0.0865. Acc.: 72.18%\nEpoch: 1392. Loss: 0.1030. Acc.: 72.57%\nEpoch: 1393. Loss: 0.0940. Acc.: 72.05%\nEpoch: 1394. Loss: 0.1067. Acc.: 73.10%\nEpoch: 1395. Loss: 0.0935. Acc.: 73.49%\nEpoch: 1396. Loss: 0.1006. Acc.: 72.44%\nEpoch: 1397. Loss: 0.1046. Acc.: 72.57%\nEpoch: 1398. Loss: 0.0997. Acc.: 72.57%\nEpoch: 1399. Loss: 0.1016. Acc.: 72.44%\nEpoch: 1400. Loss: 0.0999. Acc.: 72.31%\nEpoch: 1401. Loss: 0.0945. Acc.: 73.36%\nEpoch: 1402. Loss: 0.1077. Acc.: 72.83%\nEpoch: 1403. Loss: 0.1022. Acc.: 71.65%\nEpoch: 1404. Loss: 0.0946. Acc.: 72.44%\nEpoch: 1405. Loss: 0.1039. Acc.: 72.70%\nEpoch: 1406. Loss: 0.0996. Acc.: 72.31%\nEpoch: 1407. Loss: 0.0949. Acc.: 71.52%\nEpoch: 1408. Loss: 0.0877. Acc.: 71.39%\nEpoch: 1409. Loss: 0.0896. Acc.: 72.05%\nEpoch: 1410. Loss: 0.1058. Acc.: 72.83%\nEpoch: 1411. Loss: 0.0972. Acc.: 71.78%\nEpoch: 1412. Loss: 0.0993. Acc.: 71.78%\nEpoch: 1413. Loss: 0.1072. Acc.: 72.18%\nEpoch: 1414. Loss: 0.0960. Acc.: 71.65%\nEpoch: 1415. Loss: 0.0900. Acc.: 71.39%\nEpoch: 1416. Loss: 0.1049. Acc.: 71.92%\nEpoch: 1417. Loss: 0.1104. Acc.: 72.31%\nEpoch: 1418. Loss: 0.0861. Acc.: 72.70%\nEpoch: 1419. Loss: 0.0985. Acc.: 72.44%\nEpoch: 1420. Loss: 0.0953. Acc.: 72.05%\nEpoch: 1421. Loss: 0.0960. Acc.: 72.18%\nEpoch: 1422. Loss: 0.0940. Acc.: 71.65%\nEpoch: 1423. Loss: 0.1023. Acc.: 70.21%\nEpoch: 1424. Loss: 0.1162. Acc.: 70.21%\nEpoch: 1425. Loss: 0.0971. Acc.: 71.65%\nEpoch: 1426. Loss: 0.0880. Acc.: 72.57%\nEpoch: 1427. Loss: 0.1066. Acc.: 71.78%\nEpoch: 1428. Loss: 0.0888. Acc.: 71.52%\nEpoch: 1429. Loss: 0.1068. Acc.: 71.39%\nEpoch: 1430. Loss: 0.0951. Acc.: 70.87%\nEpoch: 1431. Loss: 0.0947. Acc.: 72.57%\nEpoch: 1432. Loss: 0.0897. Acc.: 72.57%\nEpoch: 1433. Loss: 0.0983. Acc.: 71.92%\nEpoch: 1434. Loss: 0.1028. Acc.: 72.31%\nEpoch: 1435. Loss: 0.0956. Acc.: 71.26%\nEpoch: 1436. Loss: 0.0885. Acc.: 72.05%\nEpoch: 1437. Loss: 0.0948. Acc.: 71.39%\nEpoch: 1438. Loss: 0.0982. Acc.: 72.44%\nEpoch: 1439. Loss: 0.1018. Acc.: 71.78%\nEpoch: 1440. Loss: 0.0947. Acc.: 71.78%\nEpoch: 1441. Loss: 0.0908. Acc.: 71.13%\nEpoch: 1442. Loss: 0.1000. Acc.: 73.36%\nEpoch: 1443. Loss: 0.1086. Acc.: 73.36%\nEpoch: 1444. Loss: 0.1043. Acc.: 72.31%\nEpoch: 1445. Loss: 0.0908. Acc.: 73.23%\nEpoch: 1446. Loss: 0.0956. Acc.: 72.44%\nEpoch: 1447. Loss: 0.0970. Acc.: 72.44%\nEpoch: 1448. Loss: 0.0955. Acc.: 72.83%\nEpoch: 1449. Loss: 0.0987. Acc.: 72.70%\nEpoch: 1450. Loss: 0.1132. Acc.: 71.26%\nEpoch: 1451. Loss: 0.0833. Acc.: 71.26%\nEpoch: 1452. Loss: 0.0971. Acc.: 72.83%\nEpoch: 1453. Loss: 0.0924. Acc.: 72.57%\nEpoch: 1454. Loss: 0.1114. Acc.: 71.65%\nEpoch: 1455. Loss: 0.0973. Acc.: 72.57%\nEpoch: 1456. Loss: 0.0845. Acc.: 72.57%\nEpoch: 1457. Loss: 0.0949. Acc.: 72.83%\nEpoch: 1458. Loss: 0.0966. Acc.: 72.70%\nEpoch: 1459. Loss: 0.1012. Acc.: 72.83%\nEpoch: 1460. Loss: 0.0949. Acc.: 73.49%\nEpoch: 1461. Loss: 0.0945. Acc.: 73.75%\nEpoch: 1462. Loss: 0.1050. Acc.: 74.02%\nEpoch: 1463. Loss: 0.0943. Acc.: 73.88%\nEpoch: 1464. Loss: 0.0930. Acc.: 74.15%\nEpoch: 1465. Loss: 0.0895. Acc.: 72.31%\nEpoch: 1466. Loss: 0.0899. Acc.: 72.31%\nEpoch: 1467. Loss: 0.0977. Acc.: 71.78%\nEpoch: 1468. Loss: 0.0946. Acc.: 72.44%\nEpoch: 1469. Loss: 0.0881. Acc.: 72.44%\nEpoch: 1470. Loss: 0.1078. Acc.: 73.36%\nEpoch: 1471. Loss: 0.0915. Acc.: 73.10%\nEpoch: 1472. Loss: 0.0844. Acc.: 73.23%\nEpoch: 1473. Loss: 0.0938. Acc.: 72.31%\nEpoch: 1474. Loss: 0.1039. Acc.: 73.49%\nEpoch: 1475. Loss: 0.0964. Acc.: 72.31%\nEpoch: 1476. Loss: 0.0884. Acc.: 72.57%\nEpoch: 1477. Loss: 0.0940. Acc.: 72.97%\nEpoch: 1478. Loss: 0.0962. Acc.: 72.70%\nEpoch: 1479. Loss: 0.1004. Acc.: 72.18%\nEpoch: 1480. Loss: 0.0946. Acc.: 71.78%\nEpoch: 1481. Loss: 0.1067. Acc.: 72.83%\nEpoch: 1482. Loss: 0.0902. Acc.: 72.70%\nEpoch: 1483. Loss: 0.0910. Acc.: 73.36%\nEpoch: 1484. Loss: 0.0975. Acc.: 73.36%\nEpoch: 1485. Loss: 0.0866. Acc.: 73.10%\nEpoch: 1486. Loss: 0.0983. Acc.: 72.97%\nEpoch: 1487. Loss: 0.1051. Acc.: 73.88%\nEpoch: 1488. Loss: 0.0959. Acc.: 73.49%\nEpoch: 1489. Loss: 0.0938. Acc.: 72.97%\nEpoch: 1490. Loss: 0.0960. Acc.: 73.10%\nEpoch: 1491. Loss: 0.0891. Acc.: 72.57%\nEpoch: 1492. Loss: 0.0993. Acc.: 72.44%\nEpoch: 1493. Loss: 0.0961. Acc.: 72.97%\nEpoch: 1494. Loss: 0.1137. Acc.: 73.23%\nEpoch: 1495. Loss: 0.0920. Acc.: 73.23%\nEpoch: 1496. Loss: 0.0960. Acc.: 73.23%\nEpoch: 1497. Loss: 0.0949. Acc.: 73.88%\nEpoch: 1498. Loss: 0.1011. Acc.: 72.97%\nEpoch: 1499. Loss: 0.0969. Acc.: 72.70%\nEpoch: 1500. Loss: 0.0956. Acc.: 73.88%\nEpoch: 1501. Loss: 0.1070. Acc.: 72.83%\nEpoch: 1502. Loss: 0.0910. Acc.: 71.39%\nEpoch: 1503. Loss: 0.1000. Acc.: 71.26%\nEpoch: 1504. Loss: 0.1053. Acc.: 71.26%\nEpoch: 1505. Loss: 0.0972. Acc.: 72.83%\nEpoch: 1506. Loss: 0.0973. Acc.: 73.88%\nEpoch: 1507. Loss: 0.0946. Acc.: 72.05%\nEpoch: 1508. Loss: 0.0880. Acc.: 72.44%\nEpoch: 1509. Loss: 0.0932. Acc.: 72.57%\nEpoch: 1510. Loss: 0.1000. Acc.: 71.92%\nEpoch: 1511. Loss: 0.0947. Acc.: 73.23%\nEpoch: 1512. Loss: 0.0986. Acc.: 73.49%\nEpoch: 1513. Loss: 0.0907. Acc.: 73.75%\nEpoch: 1514. Loss: 0.0865. Acc.: 73.23%\nEpoch: 1515. Loss: 0.0891. Acc.: 73.62%\nEpoch: 1516. Loss: 0.0949. Acc.: 73.75%\nEpoch: 1517. Loss: 0.1023. Acc.: 72.44%\nEpoch: 1518. Loss: 0.1004. Acc.: 73.10%\nEpoch: 1519. Loss: 0.0886. Acc.: 73.23%\nEpoch: 1520. Loss: 0.1014. Acc.: 73.75%\nEpoch: 1521. Loss: 0.0939. Acc.: 72.83%\nEpoch: 1522. Loss: 0.0891. Acc.: 72.83%\nEpoch: 1523. Loss: 0.0965. Acc.: 73.10%\nEpoch: 1524. Loss: 0.0903. Acc.: 71.78%\nEpoch: 1525. Loss: 0.0917. Acc.: 71.78%\nEpoch: 1526. Loss: 0.0993. Acc.: 72.31%\nEpoch: 1527. Loss: 0.0912. Acc.: 71.92%\nEpoch: 1528. Loss: 0.0898. Acc.: 72.05%\nEpoch: 1529. Loss: 0.0862. Acc.: 72.44%\nEpoch: 1530. Loss: 0.1030. Acc.: 72.70%\nEpoch: 1531. Loss: 0.0884. Acc.: 73.62%\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 1532. Loss: 0.0905. Acc.: 73.62%\nEpoch: 1533. Loss: 0.0974. Acc.: 73.36%\nEpoch: 1534. Loss: 0.0878. Acc.: 71.92%\nEpoch: 1535. Loss: 0.0943. Acc.: 72.05%\nEpoch: 1536. Loss: 0.0933. Acc.: 72.18%\nEpoch: 1537. Loss: 0.1021. Acc.: 72.31%\nEpoch: 1538. Loss: 0.1012. Acc.: 71.65%\nEpoch: 1539. Loss: 0.0904. Acc.: 72.97%\nEpoch: 1540. Loss: 0.0860. Acc.: 72.83%\nEpoch: 1541. Loss: 0.0821. Acc.: 72.97%\nEpoch: 1542. Loss: 0.0899. Acc.: 72.97%\nEpoch: 1543. Loss: 0.0874. Acc.: 72.18%\nEpoch: 1544. Loss: 0.0865. Acc.: 73.10%\nEpoch: 1545. Loss: 0.0943. Acc.: 72.70%\nEpoch: 1546. Loss: 0.0888. Acc.: 72.97%\nEpoch: 1547. Loss: 0.0993. Acc.: 72.57%\nEpoch: 1548. Loss: 0.0977. Acc.: 71.92%\nEpoch: 1549. Loss: 0.0883. Acc.: 72.18%\nEpoch: 1550. Loss: 0.0931. Acc.: 72.31%\nEpoch: 1551. Loss: 0.0890. Acc.: 72.31%\nEpoch: 1552. Loss: 0.0842. Acc.: 72.05%\nEpoch: 1553. Loss: 0.0961. Acc.: 72.83%\nEpoch: 1554. Loss: 0.0866. Acc.: 73.23%\nEpoch: 1555. Loss: 0.1012. Acc.: 73.36%\nEpoch: 1556. Loss: 0.1021. Acc.: 73.10%\nEpoch: 1557. Loss: 0.0989. Acc.: 72.97%\nEpoch: 1558. Loss: 0.0898. Acc.: 72.70%\nEpoch: 1559. Loss: 0.1067. Acc.: 72.57%\nEpoch: 1560. Loss: 0.1069. Acc.: 72.18%\nEpoch: 1561. Loss: 0.1024. Acc.: 71.92%\nEpoch: 1562. Loss: 0.0964. Acc.: 72.70%\nEpoch: 1563. Loss: 0.0873. Acc.: 73.36%\nEpoch: 1564. Loss: 0.0937. Acc.: 73.62%\nEpoch: 1565. Loss: 0.1013. Acc.: 72.05%\nEpoch: 1566. Loss: 0.1005. Acc.: 71.65%\nEpoch: 1567. Loss: 0.0938. Acc.: 71.52%\nEpoch: 1568. Loss: 0.0951. Acc.: 72.18%\nEpoch: 1569. Loss: 0.0990. Acc.: 71.26%\nEpoch: 1570. Loss: 0.0929. Acc.: 72.18%\nEpoch: 1571. Loss: 0.0981. Acc.: 72.57%\nEpoch: 1572. Loss: 0.0911. Acc.: 71.65%\nEpoch: 1573. Loss: 0.0858. Acc.: 71.65%\nEpoch: 1574. Loss: 0.1007. Acc.: 72.05%\nEpoch: 1575. Loss: 0.0840. Acc.: 73.75%\nEpoch: 1576. Loss: 0.0934. Acc.: 71.92%\nEpoch: 1577. Loss: 0.0903. Acc.: 71.39%\nEpoch: 1578. Loss: 0.0890. Acc.: 71.65%\nEpoch: 1579. Loss: 0.0954. Acc.: 72.97%\nEpoch: 1580. Loss: 0.0843. Acc.: 72.44%\nEpoch: 1581. Loss: 0.1051. Acc.: 71.65%\nEpoch: 1582. Loss: 0.0982. Acc.: 73.36%\nEpoch: 1583. Loss: 0.0874. Acc.: 74.28%\nEpoch: 1584. Loss: 0.0911. Acc.: 73.88%\nEpoch: 1585. Loss: 0.0931. Acc.: 73.75%\nEpoch: 1586. Loss: 0.0916. Acc.: 73.36%\nEpoch: 1587. Loss: 0.0976. Acc.: 73.36%\nEpoch: 1588. Loss: 0.0827. Acc.: 71.39%\nEpoch: 1589. Loss: 0.0943. Acc.: 72.44%\nEpoch: 1590. Loss: 0.1036. Acc.: 73.23%\nEpoch: 1591. Loss: 0.0973. Acc.: 72.83%\nEpoch: 1592. Loss: 0.0976. Acc.: 72.97%\nEpoch: 1593. Loss: 0.0870. Acc.: 72.18%\nEpoch: 1594. Loss: 0.0759. Acc.: 73.10%\nEpoch: 1595. Loss: 0.0880. Acc.: 73.10%\nEpoch: 1596. Loss: 0.0881. Acc.: 73.10%\nEpoch: 1597. Loss: 0.0926. Acc.: 73.62%\nEpoch: 1598. Loss: 0.0932. Acc.: 74.80%\nEpoch 1598 best model saved with accuracy: 74.80%\nEpoch: 1599. Loss: 0.0850. Acc.: 73.88%\nEpoch: 1600. Loss: 0.0941. Acc.: 74.02%\nEpoch: 1601. Loss: 0.0932. Acc.: 74.02%\nEpoch: 1602. Loss: 0.0873. Acc.: 73.36%\nEpoch: 1603. Loss: 0.0852. Acc.: 73.62%\nEpoch: 1604. Loss: 0.0863. Acc.: 73.75%\nEpoch: 1605. Loss: 0.0992. Acc.: 72.97%\nEpoch: 1606. Loss: 0.0897. Acc.: 72.05%\nEpoch: 1607. Loss: 0.0939. Acc.: 72.44%\nEpoch: 1608. Loss: 0.0834. Acc.: 72.44%\nEpoch: 1609. Loss: 0.0904. Acc.: 72.57%\nEpoch: 1610. Loss: 0.0889. Acc.: 73.23%\nEpoch: 1611. Loss: 0.0875. Acc.: 72.57%\nEpoch: 1612. Loss: 0.0874. Acc.: 74.15%\nEpoch: 1613. Loss: 0.0918. Acc.: 75.33%\nEpoch 1613 best model saved with accuracy: 75.33%\nEpoch: 1614. Loss: 0.0910. Acc.: 73.88%\nEpoch: 1615. Loss: 0.0931. Acc.: 72.97%\nEpoch: 1616. Loss: 0.0861. Acc.: 72.70%\nEpoch: 1617. Loss: 0.0925. Acc.: 73.62%\nEpoch: 1618. Loss: 0.0889. Acc.: 73.49%\nEpoch: 1619. Loss: 0.0879. Acc.: 73.36%\nEpoch: 1620. Loss: 0.0932. Acc.: 72.83%\nEpoch: 1621. Loss: 0.0825. Acc.: 73.10%\nEpoch: 1622. Loss: 0.0858. Acc.: 72.83%\nEpoch: 1623. Loss: 0.0844. Acc.: 72.70%\nEpoch: 1624. Loss: 0.0801. Acc.: 73.49%\nEpoch: 1625. Loss: 0.0987. Acc.: 73.62%\nEpoch: 1626. Loss: 0.0886. Acc.: 73.75%\nEpoch: 1627. Loss: 0.0881. Acc.: 72.97%\nEpoch: 1628. Loss: 0.0897. Acc.: 72.18%\nEpoch: 1629. Loss: 0.0811. Acc.: 72.44%\nEpoch: 1630. Loss: 0.0921. Acc.: 72.70%\nEpoch: 1631. Loss: 0.1092. Acc.: 72.31%\nEpoch: 1632. Loss: 0.0954. Acc.: 72.57%\nEpoch: 1633. Loss: 0.0893. Acc.: 72.83%\nEpoch: 1634. Loss: 0.0834. Acc.: 71.52%\nEpoch: 1635. Loss: 0.0861. Acc.: 71.92%\nEpoch: 1636. Loss: 0.0898. Acc.: 71.52%\nEpoch: 1637. Loss: 0.0827. Acc.: 71.92%\nEpoch: 1638. Loss: 0.0973. Acc.: 72.83%\nEpoch: 1639. Loss: 0.0883. Acc.: 73.49%\nEpoch: 1640. Loss: 0.0870. Acc.: 73.88%\nEpoch: 1641. Loss: 0.0814. Acc.: 73.10%\nEpoch: 1642. Loss: 0.0798. Acc.: 72.70%\nEpoch: 1643. Loss: 0.0809. Acc.: 73.62%\nEpoch: 1644. Loss: 0.0834. Acc.: 73.23%\nEpoch: 1645. Loss: 0.0959. Acc.: 73.10%\nEpoch: 1646. Loss: 0.0836. Acc.: 74.02%\nEpoch: 1647. Loss: 0.0959. Acc.: 73.10%\nEpoch: 1648. Loss: 0.0845. Acc.: 73.49%\nEpoch: 1649. Loss: 0.1021. Acc.: 73.23%\nEpoch: 1650. Loss: 0.0840. Acc.: 73.23%\nEpoch: 1651. Loss: 0.0878. Acc.: 74.93%\nEpoch: 1652. Loss: 0.0943. Acc.: 74.28%\nEpoch: 1653. Loss: 0.0788. Acc.: 74.02%\nEpoch: 1654. Loss: 0.0820. Acc.: 74.15%\nEpoch: 1655. Loss: 0.0864. Acc.: 73.88%\nEpoch: 1656. Loss: 0.0961. Acc.: 73.36%\nEpoch: 1657. Loss: 0.0981. Acc.: 73.62%\nEpoch: 1658. Loss: 0.0820. Acc.: 73.36%\nEpoch: 1659. Loss: 0.0773. Acc.: 73.10%\nEpoch: 1660. Loss: 0.0806. Acc.: 73.75%\nEpoch: 1661. Loss: 0.0881. Acc.: 73.62%\nEpoch: 1662. Loss: 0.0931. Acc.: 74.15%\nEpoch: 1663. Loss: 0.0929. Acc.: 73.75%\nEpoch: 1664. Loss: 0.0843. Acc.: 73.88%\nEpoch: 1665. Loss: 0.0836. Acc.: 72.97%\nEpoch: 1666. Loss: 0.0975. Acc.: 73.62%\nEpoch: 1667. Loss: 0.0914. Acc.: 73.49%\nEpoch: 1668. Loss: 0.1011. Acc.: 73.62%\nEpoch: 1669. Loss: 0.0866. Acc.: 73.88%\nEpoch: 1670. Loss: 0.0940. Acc.: 73.49%\nEpoch: 1671. Loss: 0.0953. Acc.: 74.02%\nEpoch: 1672. Loss: 0.0901. Acc.: 73.88%\nEpoch: 1673. Loss: 0.0947. Acc.: 74.54%\nEpoch: 1674. Loss: 0.0882. Acc.: 74.15%\nEpoch: 1675. Loss: 0.0823. Acc.: 72.70%\nEpoch: 1676. Loss: 0.0889. Acc.: 73.10%\nEpoch: 1677. Loss: 0.0824. Acc.: 73.10%\nEpoch: 1678. Loss: 0.0858. Acc.: 74.15%\nEpoch: 1679. Loss: 0.0903. Acc.: 74.28%\nEpoch: 1680. Loss: 0.0903. Acc.: 74.15%\nEpoch: 1681. Loss: 0.0860. Acc.: 73.49%\nEpoch: 1682. Loss: 0.0829. Acc.: 73.49%\nEpoch: 1683. Loss: 0.0910. Acc.: 73.23%\nEpoch: 1684. Loss: 0.0918. Acc.: 73.36%\nEpoch: 1685. Loss: 0.0842. Acc.: 72.44%\nEpoch: 1686. Loss: 0.0970. Acc.: 73.75%\nEpoch: 1687. Loss: 0.0789. Acc.: 73.49%\nEpoch: 1688. Loss: 0.0848. Acc.: 72.97%\nEpoch: 1689. Loss: 0.0879. Acc.: 72.83%\nEpoch: 1690. Loss: 0.0972. Acc.: 72.83%\nEpoch: 1691. Loss: 0.0939. Acc.: 72.97%\nEpoch: 1692. Loss: 0.0969. Acc.: 73.62%\nEpoch: 1693. Loss: 0.0853. Acc.: 73.10%\nEpoch: 1694. Loss: 0.0817. Acc.: 73.10%\nEpoch: 1695. Loss: 0.0895. Acc.: 73.36%\nEpoch: 1696. Loss: 0.0855. Acc.: 72.97%\nEpoch: 1697. Loss: 0.0862. Acc.: 73.23%\nEpoch: 1698. Loss: 0.0877. Acc.: 72.05%\nEpoch: 1699. Loss: 0.0860. Acc.: 72.31%\nEpoch: 1700. Loss: 0.0885. Acc.: 72.97%\nEpoch: 1701. Loss: 0.0852. Acc.: 72.70%\nEpoch: 1702. Loss: 0.0825. Acc.: 72.70%\nEpoch: 1703. Loss: 0.0907. Acc.: 72.05%\nEpoch: 1704. Loss: 0.0936. Acc.: 72.83%\nEpoch: 1705. Loss: 0.0920. Acc.: 72.83%\nEpoch: 1706. Loss: 0.0855. Acc.: 75.07%\nEpoch: 1707. Loss: 0.0949. Acc.: 74.93%\nEpoch: 1708. Loss: 0.1022. Acc.: 74.80%\nEpoch: 1709. Loss: 0.0994. Acc.: 73.49%\nEpoch: 1710. Loss: 0.0902. Acc.: 72.44%\nEpoch: 1711. Loss: 0.0870. Acc.: 71.78%\nEpoch: 1712. Loss: 0.1090. Acc.: 71.92%\nEpoch: 1713. Loss: 0.0879. Acc.: 72.70%\nEpoch: 1714. Loss: 0.0881. Acc.: 72.70%\nEpoch: 1715. Loss: 0.0927. Acc.: 72.57%\nEpoch: 1716. Loss: 0.0963. Acc.: 72.44%\nEpoch: 1717. Loss: 0.0914. Acc.: 71.78%\nEpoch: 1718. Loss: 0.0882. Acc.: 72.70%\nEpoch: 1719. Loss: 0.0884. Acc.: 73.23%\nEpoch: 1720. Loss: 0.0839. Acc.: 72.97%\nEpoch: 1721. Loss: 0.0771. Acc.: 72.44%\nEpoch: 1722. Loss: 0.0944. Acc.: 72.18%\nEpoch: 1723. Loss: 0.0900. Acc.: 72.05%\nEpoch: 1724. Loss: 0.1021. Acc.: 72.83%\nEpoch: 1725. Loss: 0.0830. Acc.: 72.44%\nEpoch: 1726. Loss: 0.0933. Acc.: 72.83%\nEpoch: 1727. Loss: 0.0874. Acc.: 73.23%\nEpoch: 1728. Loss: 0.0840. Acc.: 73.75%\nEpoch: 1729. Loss: 0.0751. Acc.: 74.02%\nEpoch: 1730. Loss: 0.0891. Acc.: 75.07%\nEpoch: 1731. Loss: 0.0821. Acc.: 74.93%\nEpoch: 1732. Loss: 0.0797. Acc.: 74.93%\nEpoch: 1733. Loss: 0.0795. Acc.: 73.75%\nEpoch: 1734. Loss: 0.0831. Acc.: 72.70%\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 1735. Loss: 0.0905. Acc.: 72.44%\nEpoch: 1736. Loss: 0.0894. Acc.: 72.18%\nEpoch: 1737. Loss: 0.0900. Acc.: 72.44%\nEpoch: 1738. Loss: 0.0855. Acc.: 73.10%\nEpoch: 1739. Loss: 0.0858. Acc.: 74.15%\nEpoch: 1740. Loss: 0.0822. Acc.: 74.28%\nEpoch: 1741. Loss: 0.0749. Acc.: 74.02%\nEpoch: 1742. Loss: 0.0807. Acc.: 73.49%\nEpoch: 1743. Loss: 0.0724. Acc.: 73.62%\nEpoch: 1744. Loss: 0.0949. Acc.: 73.88%\nEpoch: 1745. Loss: 0.0870. Acc.: 73.62%\nEpoch: 1746. Loss: 0.0928. Acc.: 74.02%\nEpoch: 1747. Loss: 0.0982. Acc.: 73.75%\nEpoch: 1748. Loss: 0.0778. Acc.: 72.97%\nEpoch: 1749. Loss: 0.0859. Acc.: 73.36%\nEpoch: 1750. Loss: 0.0911. Acc.: 72.70%\nEpoch: 1751. Loss: 0.0906. Acc.: 72.97%\nEpoch: 1752. Loss: 0.0921. Acc.: 72.83%\nEpoch: 1753. Loss: 0.0863. Acc.: 73.23%\nEpoch: 1754. Loss: 0.0939. Acc.: 74.15%\nEpoch: 1755. Loss: 0.0925. Acc.: 73.49%\nEpoch: 1756. Loss: 0.0893. Acc.: 73.49%\nEpoch: 1757. Loss: 0.0848. Acc.: 73.49%\nEpoch: 1758. Loss: 0.0710. Acc.: 73.49%\nEpoch: 1759. Loss: 0.0784. Acc.: 73.49%\nEpoch: 1760. Loss: 0.0880. Acc.: 73.75%\nEpoch: 1761. Loss: 0.0838. Acc.: 74.28%\nEpoch: 1762. Loss: 0.0802. Acc.: 73.23%\nEpoch: 1763. Loss: 0.0764. Acc.: 72.83%\nEpoch: 1764. Loss: 0.0804. Acc.: 73.10%\nEpoch: 1765. Loss: 0.0975. Acc.: 71.92%\nEpoch: 1766. Loss: 0.0898. Acc.: 73.36%\nEpoch: 1767. Loss: 0.0783. Acc.: 73.62%\nEpoch: 1768. Loss: 0.0900. Acc.: 73.10%\nEpoch: 1769. Loss: 0.0784. Acc.: 72.83%\nEpoch: 1770. Loss: 0.0862. Acc.: 73.75%\nEpoch: 1771. Loss: 0.0839. Acc.: 73.62%\nEpoch: 1772. Loss: 0.0912. Acc.: 74.02%\nEpoch: 1773. Loss: 0.0890. Acc.: 73.62%\nEpoch: 1774. Loss: 0.0852. Acc.: 73.36%\nEpoch: 1775. Loss: 0.0817. Acc.: 74.15%\nEpoch: 1776. Loss: 0.0931. Acc.: 74.28%\nEpoch: 1777. Loss: 0.0883. Acc.: 74.28%\nEpoch: 1778. Loss: 0.0880. Acc.: 73.49%\nEpoch: 1779. Loss: 0.0793. Acc.: 73.23%\nEpoch: 1780. Loss: 0.0946. Acc.: 72.97%\nEpoch: 1781. Loss: 0.0839. Acc.: 73.36%\nEpoch: 1782. Loss: 0.0890. Acc.: 73.62%\nEpoch: 1783. Loss: 0.0897. Acc.: 73.36%\nEpoch: 1784. Loss: 0.0803. Acc.: 73.75%\nEpoch: 1785. Loss: 0.0851. Acc.: 72.83%\nEpoch: 1786. Loss: 0.0830. Acc.: 73.36%\nEpoch: 1787. Loss: 0.0814. Acc.: 73.62%\nEpoch: 1788. Loss: 0.0763. Acc.: 73.23%\nEpoch: 1789. Loss: 0.0768. Acc.: 73.10%\nEpoch: 1790. Loss: 0.0904. Acc.: 73.49%\nEpoch: 1791. Loss: 0.0922. Acc.: 73.10%\nEpoch: 1792. Loss: 0.0797. Acc.: 72.57%\nEpoch: 1793. Loss: 0.0799. Acc.: 73.10%\nEpoch: 1794. Loss: 0.0827. Acc.: 71.52%\nEpoch: 1795. Loss: 0.0900. Acc.: 71.65%\nEpoch: 1796. Loss: 0.0806. Acc.: 72.83%\nEpoch: 1797. Loss: 0.0764. Acc.: 73.23%\nEpoch: 1798. Loss: 0.0870. Acc.: 73.75%\nEpoch: 1799. Loss: 0.0769. Acc.: 73.10%\nEpoch: 1800. Loss: 0.0811. Acc.: 72.70%\nEpoch: 1801. Loss: 0.0785. Acc.: 73.23%\nEpoch: 1802. Loss: 0.0909. Acc.: 72.05%\nEpoch: 1803. Loss: 0.0888. Acc.: 73.23%\nEpoch: 1804. Loss: 0.0777. Acc.: 72.44%\nEpoch: 1805. Loss: 0.0962. Acc.: 72.31%\nEpoch: 1806. Loss: 0.0828. Acc.: 72.18%\nEpoch: 1807. Loss: 0.0767. Acc.: 71.92%\nEpoch: 1808. Loss: 0.0758. Acc.: 72.97%\nEpoch: 1809. Loss: 0.0869. Acc.: 72.57%\nEpoch: 1810. Loss: 0.0829. Acc.: 73.23%\nEpoch: 1811. Loss: 0.0810. Acc.: 72.31%\nEpoch: 1812. Loss: 0.0849. Acc.: 72.97%\nEpoch: 1813. Loss: 0.0906. Acc.: 72.44%\nEpoch: 1814. Loss: 0.0930. Acc.: 72.70%\nEpoch: 1815. Loss: 0.0786. Acc.: 72.57%\nEpoch: 1816. Loss: 0.0848. Acc.: 72.31%\nEpoch: 1817. Loss: 0.0925. Acc.: 72.18%\nEpoch: 1818. Loss: 0.0839. Acc.: 72.70%\nEpoch: 1819. Loss: 0.0762. Acc.: 73.36%\nEpoch: 1820. Loss: 0.0854. Acc.: 73.49%\nEpoch: 1821. Loss: 0.0799. Acc.: 72.31%\nEpoch: 1822. Loss: 0.0989. Acc.: 73.10%\nEpoch: 1823. Loss: 0.0813. Acc.: 73.62%\nEpoch: 1824. Loss: 0.0804. Acc.: 72.31%\nEpoch: 1825. Loss: 0.0878. Acc.: 72.44%\nEpoch: 1826. Loss: 0.0924. Acc.: 72.18%\nEpoch: 1827. Loss: 0.0997. Acc.: 72.05%\nEpoch: 1828. Loss: 0.0937. Acc.: 73.10%\nEpoch: 1829. Loss: 0.1010. Acc.: 73.75%\nEpoch: 1830. Loss: 0.0862. Acc.: 73.62%\nEpoch: 1831. Loss: 0.0802. Acc.: 73.62%\nEpoch: 1832. Loss: 0.0778. Acc.: 71.78%\nEpoch: 1833. Loss: 0.0816. Acc.: 72.97%\nEpoch: 1834. Loss: 0.0845. Acc.: 74.28%\nEpoch: 1835. Loss: 0.0828. Acc.: 74.41%\nEpoch: 1836. Loss: 0.0880. Acc.: 73.75%\nEpoch: 1837. Loss: 0.0773. Acc.: 72.18%\nEpoch: 1838. Loss: 0.0759. Acc.: 71.26%\nEpoch: 1839. Loss: 0.0785. Acc.: 72.05%\nEpoch: 1840. Loss: 0.0857. Acc.: 72.05%\nEpoch: 1841. Loss: 0.0774. Acc.: 73.49%\nEpoch: 1842. Loss: 0.0852. Acc.: 73.62%\nEpoch: 1843. Loss: 0.0843. Acc.: 72.57%\nEpoch: 1844. Loss: 0.0765. Acc.: 71.39%\nEpoch: 1845. Loss: 0.0861. Acc.: 72.44%\nEpoch: 1846. Loss: 0.0987. Acc.: 73.49%\nEpoch: 1847. Loss: 0.0820. Acc.: 73.75%\nEpoch: 1848. Loss: 0.0831. Acc.: 73.36%\nEpoch: 1849. Loss: 0.0770. Acc.: 74.80%\nEpoch: 1850. Loss: 0.0769. Acc.: 73.88%\nEpoch: 1851. Loss: 0.0730. Acc.: 72.70%\nEpoch: 1852. Loss: 0.0689. Acc.: 72.44%\nEpoch: 1853. Loss: 0.0780. Acc.: 73.23%\nEpoch: 1854. Loss: 0.0752. Acc.: 73.36%\nEpoch: 1855. Loss: 0.0868. Acc.: 73.36%\nEpoch: 1856. Loss: 0.0864. Acc.: 72.44%\nEpoch: 1857. Loss: 0.0808. Acc.: 71.78%\nEpoch: 1858. Loss: 0.0806. Acc.: 72.31%\nEpoch: 1859. Loss: 0.0842. Acc.: 72.70%\nEpoch: 1860. Loss: 0.0779. Acc.: 73.36%\nEpoch: 1861. Loss: 0.0833. Acc.: 73.36%\nEpoch: 1862. Loss: 0.0742. Acc.: 72.97%\nEpoch: 1863. Loss: 0.0841. Acc.: 72.70%\nEpoch: 1864. Loss: 0.0768. Acc.: 73.23%\nEpoch: 1865. Loss: 0.0946. Acc.: 73.10%\nEpoch: 1866. Loss: 0.0873. Acc.: 73.23%\nEpoch: 1867. Loss: 0.0845. Acc.: 73.75%\nEpoch: 1868. Loss: 0.0719. Acc.: 73.75%\nEpoch: 1869. Loss: 0.0910. Acc.: 73.62%\nEpoch: 1870. Loss: 0.0753. Acc.: 73.62%\nEpoch: 1871. Loss: 0.0803. Acc.: 73.49%\nEpoch: 1872. Loss: 0.0885. Acc.: 73.62%\nEpoch: 1873. Loss: 0.0697. Acc.: 73.10%\nEpoch: 1874. Loss: 0.0850. Acc.: 72.44%\nEpoch: 1875. Loss: 0.0879. Acc.: 72.83%\nEpoch: 1876. Loss: 0.0781. Acc.: 72.31%\nEpoch: 1877. Loss: 0.0811. Acc.: 72.18%\nEpoch: 1878. Loss: 0.0724. Acc.: 72.97%\nEpoch: 1879. Loss: 0.0841. Acc.: 72.70%\nEpoch: 1880. Loss: 0.0701. Acc.: 72.97%\nEpoch: 1881. Loss: 0.0794. Acc.: 72.18%\nEpoch: 1882. Loss: 0.0815. Acc.: 72.70%\nEpoch: 1883. Loss: 0.0883. Acc.: 73.36%\nEpoch: 1884. Loss: 0.0718. Acc.: 74.67%\nEpoch: 1885. Loss: 0.0917. Acc.: 71.92%\nEpoch: 1886. Loss: 0.0715. Acc.: 72.05%\nEpoch: 1887. Loss: 0.0797. Acc.: 72.57%\nEpoch: 1888. Loss: 0.0815. Acc.: 73.62%\nEpoch: 1889. Loss: 0.0983. Acc.: 72.97%\nEpoch: 1890. Loss: 0.0853. Acc.: 72.70%\nEpoch: 1891. Loss: 0.0762. Acc.: 72.44%\nEpoch: 1892. Loss: 0.0716. Acc.: 72.83%\nEpoch: 1893. Loss: 0.0807. Acc.: 72.70%\nEpoch: 1894. Loss: 0.0731. Acc.: 73.10%\nEpoch: 1895. Loss: 0.0873. Acc.: 73.75%\nEpoch: 1896. Loss: 0.0763. Acc.: 73.62%\nEpoch: 1897. Loss: 0.0734. Acc.: 73.36%\nEpoch: 1898. Loss: 0.0845. Acc.: 73.62%\nEpoch: 1899. Loss: 0.0776. Acc.: 74.15%\nEpoch: 1900. Loss: 0.0697. Acc.: 73.10%\nEpoch: 1901. Loss: 0.0812. Acc.: 74.80%\nEpoch: 1902. Loss: 0.0817. Acc.: 73.88%\nEpoch: 1903. Loss: 0.0801. Acc.: 74.02%\nEpoch: 1904. Loss: 0.0720. Acc.: 74.28%\nEpoch: 1905. Loss: 0.0910. Acc.: 73.88%\nEpoch: 1906. Loss: 0.0931. Acc.: 72.70%\nEpoch: 1907. Loss: 0.0797. Acc.: 72.97%\nEpoch: 1908. Loss: 0.0837. Acc.: 72.57%\nEpoch: 1909. Loss: 0.0687. Acc.: 72.44%\nEpoch: 1910. Loss: 0.0816. Acc.: 73.10%\nEpoch: 1911. Loss: 0.0807. Acc.: 71.39%\nEpoch: 1912. Loss: 0.0743. Acc.: 71.78%\nEpoch: 1913. Loss: 0.0885. Acc.: 72.44%\nEpoch: 1914. Loss: 0.0805. Acc.: 72.31%\nEpoch: 1915. Loss: 0.0767. Acc.: 72.31%\nEpoch: 1916. Loss: 0.0829. Acc.: 72.18%\nEpoch: 1917. Loss: 0.0859. Acc.: 72.83%\nEpoch: 1918. Loss: 0.0834. Acc.: 73.88%\nEpoch: 1919. Loss: 0.0711. Acc.: 73.49%\nEpoch: 1920. Loss: 0.0835. Acc.: 72.97%\nEpoch: 1921. Loss: 0.0857. Acc.: 72.97%\nEpoch: 1922. Loss: 0.0815. Acc.: 72.70%\nEpoch: 1923. Loss: 0.0890. Acc.: 72.57%\nEpoch: 1924. Loss: 0.0831. Acc.: 73.10%\nEpoch: 1925. Loss: 0.0818. Acc.: 73.49%\nEpoch: 1926. Loss: 0.0827. Acc.: 73.49%\nEpoch: 1927. Loss: 0.0896. Acc.: 72.97%\nEpoch: 1928. Loss: 0.0770. Acc.: 73.88%\nEpoch: 1929. Loss: 0.0728. Acc.: 73.10%\nEpoch: 1930. Loss: 0.0736. Acc.: 72.97%\nEpoch: 1931. Loss: 0.0766. Acc.: 73.10%\nEpoch: 1932. Loss: 0.0814. Acc.: 73.36%\nEpoch: 1933. Loss: 0.0815. Acc.: 73.36%\nEpoch: 1934. Loss: 0.0899. Acc.: 72.57%\nEpoch: 1935. Loss: 0.0697. Acc.: 73.10%\nEpoch: 1936. Loss: 0.0778. Acc.: 72.97%\nEpoch: 1937. Loss: 0.0774. Acc.: 73.49%\nEpoch: 1938. Loss: 0.0767. Acc.: 73.75%\nEpoch: 1939. Loss: 0.0844. Acc.: 72.83%\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 1940. Loss: 0.0870. Acc.: 72.70%\nEpoch: 1941. Loss: 0.0824. Acc.: 73.23%\nEpoch: 1942. Loss: 0.0866. Acc.: 73.23%\nEpoch: 1943. Loss: 0.0827. Acc.: 72.57%\nEpoch: 1944. Loss: 0.0747. Acc.: 72.97%\nEpoch: 1945. Loss: 0.0775. Acc.: 72.44%\nEpoch: 1946. Loss: 0.0785. Acc.: 72.57%\nEpoch: 1947. Loss: 0.0827. Acc.: 72.57%\nEpoch: 1948. Loss: 0.0808. Acc.: 72.97%\nEpoch: 1949. Loss: 0.0839. Acc.: 72.44%\nEpoch: 1950. Loss: 0.0810. Acc.: 72.31%\nEpoch: 1951. Loss: 0.0884. Acc.: 71.92%\nEpoch: 1952. Loss: 0.0895. Acc.: 72.70%\nEpoch: 1953. Loss: 0.0921. Acc.: 72.57%\nEpoch: 1954. Loss: 0.0808. Acc.: 72.57%\nEpoch: 1955. Loss: 0.0859. Acc.: 72.97%\nEpoch: 1956. Loss: 0.0775. Acc.: 71.78%\nEpoch: 1957. Loss: 0.0824. Acc.: 72.70%\nEpoch: 1958. Loss: 0.0760. Acc.: 72.83%\nEpoch: 1959. Loss: 0.0807. Acc.: 73.88%\nEpoch: 1960. Loss: 0.0815. Acc.: 73.75%\nEpoch: 1961. Loss: 0.0795. Acc.: 72.97%\nEpoch: 1962. Loss: 0.0783. Acc.: 72.97%\nEpoch: 1963. Loss: 0.0818. Acc.: 73.10%\nEpoch: 1964. Loss: 0.0804. Acc.: 72.97%\nEpoch: 1965. Loss: 0.0820. Acc.: 73.36%\nEpoch: 1966. Loss: 0.0862. Acc.: 74.02%\nEpoch: 1967. Loss: 0.0833. Acc.: 73.75%\nEpoch: 1968. Loss: 0.0890. Acc.: 74.28%\nEpoch: 1969. Loss: 0.0855. Acc.: 73.75%\nEpoch: 1970. Loss: 0.0755. Acc.: 72.97%\nEpoch: 1971. Loss: 0.0980. Acc.: 71.78%\nEpoch: 1972. Loss: 0.0860. Acc.: 71.65%\nEpoch: 1973. Loss: 0.0913. Acc.: 72.97%\nEpoch: 1974. Loss: 0.0907. Acc.: 73.36%\nEpoch: 1975. Loss: 0.0809. Acc.: 72.18%\nEpoch: 1976. Loss: 0.0885. Acc.: 72.05%\nEpoch: 1977. Loss: 0.0916. Acc.: 71.92%\nEpoch: 1978. Loss: 0.0814. Acc.: 72.18%\nEpoch: 1979. Loss: 0.0830. Acc.: 72.83%\nEpoch: 1980. Loss: 0.0735. Acc.: 73.23%\nEpoch: 1981. Loss: 0.0820. Acc.: 72.44%\nEpoch: 1982. Loss: 0.0828. Acc.: 71.65%\nEpoch: 1983. Loss: 0.0796. Acc.: 72.05%\nEpoch: 1984. Loss: 0.0772. Acc.: 72.57%\nEpoch: 1985. Loss: 0.0861. Acc.: 72.83%\nEpoch: 1986. Loss: 0.0799. Acc.: 72.57%\nEpoch: 1987. Loss: 0.0850. Acc.: 73.49%\nEpoch: 1988. Loss: 0.0783. Acc.: 73.49%\nEpoch: 1989. Loss: 0.0794. Acc.: 74.15%\nEpoch: 1990. Loss: 0.0738. Acc.: 74.28%\nEpoch: 1991. Loss: 0.0789. Acc.: 74.02%\nEpoch: 1992. Loss: 0.0772. Acc.: 73.75%\nEpoch: 1993. Loss: 0.0778. Acc.: 73.62%\nEpoch: 1994. Loss: 0.0789. Acc.: 73.49%\nEpoch: 1995. Loss: 0.0791. Acc.: 74.15%\nEpoch: 1996. Loss: 0.0755. Acc.: 74.28%\nEpoch: 1997. Loss: 0.0903. Acc.: 74.67%\nEpoch: 1998. Loss: 0.0742. Acc.: 74.93%\nEpoch: 1999. Loss: 0.0906. Acc.: 74.67%\nEpoch: 2000. Loss: 0.0791. Acc.: 73.62%\nEpoch: 2001. Loss: 0.0768. Acc.: 73.49%\nEpoch: 2002. Loss: 0.0810. Acc.: 73.62%\nEpoch: 2003. Loss: 0.0923. Acc.: 73.62%\nEpoch: 2004. Loss: 0.0806. Acc.: 74.28%\nEpoch: 2005. Loss: 0.0823. Acc.: 73.75%\nEpoch: 2006. Loss: 0.0791. Acc.: 72.83%\nEpoch: 2007. Loss: 0.0812. Acc.: 73.62%\nEpoch: 2008. Loss: 0.0748. Acc.: 73.23%\nEpoch: 2009. Loss: 0.0724. Acc.: 72.70%\nEpoch: 2010. Loss: 0.0685. Acc.: 73.10%\nEpoch: 2011. Loss: 0.0870. Acc.: 73.36%\nEpoch: 2012. Loss: 0.0823. Acc.: 73.36%\nEpoch: 2013. Loss: 0.0870. Acc.: 73.88%\nEpoch: 2014. Loss: 0.0716. Acc.: 73.88%\nEpoch: 2015. Loss: 0.0744. Acc.: 74.41%\nEpoch: 2016. Loss: 0.0854. Acc.: 75.07%\nEpoch: 2017. Loss: 0.0833. Acc.: 74.02%\nEpoch: 2018. Loss: 0.0767. Acc.: 72.83%\nEpoch: 2019. Loss: 0.0736. Acc.: 73.62%\nEpoch: 2020. Loss: 0.0685. Acc.: 73.36%\nEpoch: 2021. Loss: 0.0753. Acc.: 73.75%\nEpoch: 2022. Loss: 0.0910. Acc.: 73.10%\nEpoch: 2023. Loss: 0.0729. Acc.: 71.92%\nEpoch: 2024. Loss: 0.0691. Acc.: 71.92%\nEpoch: 2025. Loss: 0.0745. Acc.: 73.10%\nEpoch: 2026. Loss: 0.0775. Acc.: 74.67%\nEpoch: 2027. Loss: 0.0733. Acc.: 74.67%\nEpoch: 2028. Loss: 0.0812. Acc.: 73.10%\nEpoch: 2029. Loss: 0.0763. Acc.: 72.83%\nEpoch: 2030. Loss: 0.0798. Acc.: 73.10%\nEpoch: 2031. Loss: 0.0782. Acc.: 73.49%\nEpoch: 2032. Loss: 0.0808. Acc.: 73.49%\nEpoch: 2033. Loss: 0.0705. Acc.: 74.15%\nEpoch: 2034. Loss: 0.0742. Acc.: 73.88%\nEpoch: 2035. Loss: 0.0786. Acc.: 72.70%\nEpoch: 2036. Loss: 0.0718. Acc.: 72.31%\nEpoch: 2037. Loss: 0.0755. Acc.: 72.05%\nEpoch: 2038. Loss: 0.0735. Acc.: 72.97%\nEpoch: 2039. Loss: 0.0783. Acc.: 73.10%\nEpoch: 2040. Loss: 0.0759. Acc.: 72.57%\nEpoch: 2041. Loss: 0.0708. Acc.: 72.57%\nEpoch: 2042. Loss: 0.0810. Acc.: 72.70%\nEpoch: 2043. Loss: 0.0820. Acc.: 72.57%\nEpoch: 2044. Loss: 0.0805. Acc.: 74.15%\nEpoch: 2045. Loss: 0.0682. Acc.: 74.15%\nEpoch: 2046. Loss: 0.0739. Acc.: 74.67%\nEpoch: 2047. Loss: 0.0732. Acc.: 74.41%\nEpoch: 2048. Loss: 0.0700. Acc.: 73.75%\nEpoch: 2048. Loss: 0.0700. Acc.: 73.75%\nEpoch: 2049. Loss: 0.0784. Acc.: 73.10%\nEpoch: 2050. Loss: 0.0623. Acc.: 73.36%\nEpoch: 2051. Loss: 0.0794. Acc.: 72.31%\nEpoch: 2052. Loss: 0.0730. Acc.: 72.97%\nEpoch: 2053. Loss: 0.0751. Acc.: 73.62%\nEpoch: 2054. Loss: 0.0785. Acc.: 72.70%\nEpoch: 2055. Loss: 0.0691. Acc.: 72.31%\nEpoch: 2056. Loss: 0.0781. Acc.: 72.97%\nEpoch: 2057. Loss: 0.0739. Acc.: 72.44%\nEpoch: 2058. Loss: 0.0748. Acc.: 73.75%\nEpoch: 2059. Loss: 0.0650. Acc.: 73.62%\nEpoch: 2060. Loss: 0.0781. Acc.: 73.75%\nEpoch: 2061. Loss: 0.0810. Acc.: 73.36%\nEpoch: 2062. Loss: 0.0822. Acc.: 74.02%\nEpoch: 2063. Loss: 0.0728. Acc.: 73.23%\nEpoch: 2064. Loss: 0.0714. Acc.: 74.41%\nEpoch: 2065. Loss: 0.0868. Acc.: 74.28%\nEpoch: 2066. Loss: 0.0868. Acc.: 73.75%\nEpoch: 2067. Loss: 0.0810. Acc.: 74.54%\nEpoch: 2068. Loss: 0.0788. Acc.: 73.62%\nEpoch: 2069. Loss: 0.0849. Acc.: 74.02%\nEpoch: 2070. Loss: 0.0791. Acc.: 73.62%\nEpoch: 2071. Loss: 0.0811. Acc.: 72.70%\nEpoch: 2072. Loss: 0.0836. Acc.: 71.92%\nEpoch: 2073. Loss: 0.0705. Acc.: 72.31%\nEpoch: 2074. Loss: 0.0736. Acc.: 72.83%\nEpoch: 2075. Loss: 0.0826. Acc.: 73.49%\nEpoch: 2076. Loss: 0.0792. Acc.: 73.49%\nEpoch: 2077. Loss: 0.0970. Acc.: 74.02%\nEpoch: 2078. Loss: 0.0764. Acc.: 72.97%\nEpoch: 2079. Loss: 0.0855. Acc.: 73.23%\nEpoch: 2080. Loss: 0.0740. Acc.: 72.83%\nEpoch: 2081. Loss: 0.0675. Acc.: 73.36%\nEpoch: 2082. Loss: 0.0728. Acc.: 73.88%\nEpoch: 2083. Loss: 0.0774. Acc.: 72.83%\nEpoch: 2084. Loss: 0.0699. Acc.: 74.02%\nEpoch: 2085. Loss: 0.0767. Acc.: 73.36%\nEpoch: 2086. Loss: 0.0771. Acc.: 73.49%\nEpoch: 2087. Loss: 0.0731. Acc.: 72.97%\nEpoch: 2088. Loss: 0.0818. Acc.: 73.10%\nEpoch: 2089. Loss: 0.0751. Acc.: 73.36%\nEpoch: 2090. Loss: 0.0751. Acc.: 73.36%\nEpoch: 2091. Loss: 0.0743. Acc.: 73.23%\nEpoch: 2092. Loss: 0.0785. Acc.: 73.62%\nEpoch: 2093. Loss: 0.0713. Acc.: 73.36%\nEpoch: 2094. Loss: 0.0687. Acc.: 73.10%\nEpoch: 2095. Loss: 0.0705. Acc.: 73.36%\nEpoch: 2096. Loss: 0.0757. Acc.: 73.49%\nEpoch: 2097. Loss: 0.0653. Acc.: 74.15%\nEpoch: 2098. Loss: 0.0773. Acc.: 74.28%\nEpoch: 2099. Loss: 0.0749. Acc.: 76.12%\nEpoch 2099 best model saved with accuracy: 76.12%\nEpoch: 2100. Loss: 0.0792. Acc.: 76.12%\nEpoch: 2101. Loss: 0.0810. Acc.: 75.33%\nEpoch: 2102. Loss: 0.0807. Acc.: 74.67%\nEpoch: 2103. Loss: 0.0829. Acc.: 74.67%\nEpoch: 2104. Loss: 0.0863. Acc.: 74.15%\nEpoch: 2105. Loss: 0.0830. Acc.: 74.15%\nEpoch: 2106. Loss: 0.0666. Acc.: 74.67%\nEpoch: 2107. Loss: 0.0777. Acc.: 74.15%\nEpoch: 2108. Loss: 0.0711. Acc.: 74.80%\nEpoch: 2109. Loss: 0.0747. Acc.: 74.02%\nEpoch: 2110. Loss: 0.0721. Acc.: 73.88%\nEpoch: 2111. Loss: 0.0651. Acc.: 73.49%\nEpoch: 2112. Loss: 0.0675. Acc.: 72.70%\nEpoch: 2113. Loss: 0.0717. Acc.: 71.78%\nEpoch: 2114. Loss: 0.0733. Acc.: 72.31%\nEpoch: 2115. Loss: 0.0901. Acc.: 72.44%\nEpoch: 2116. Loss: 0.0839. Acc.: 73.36%\nEpoch: 2117. Loss: 0.0728. Acc.: 74.02%\nEpoch: 2118. Loss: 0.0732. Acc.: 73.49%\nEpoch: 2119. Loss: 0.0692. Acc.: 72.70%\nEpoch: 2120. Loss: 0.0778. Acc.: 72.57%\nEpoch: 2121. Loss: 0.0785. Acc.: 73.23%\nEpoch: 2122. Loss: 0.0849. Acc.: 72.57%\nEpoch: 2123. Loss: 0.0736. Acc.: 73.36%\nEpoch: 2124. Loss: 0.0758. Acc.: 73.88%\nEpoch: 2125. Loss: 0.0725. Acc.: 73.10%\nEpoch: 2126. Loss: 0.0737. Acc.: 72.44%\nEpoch: 2127. Loss: 0.0737. Acc.: 73.10%\nEpoch: 2128. Loss: 0.0672. Acc.: 73.10%\nEpoch: 2129. Loss: 0.0904. Acc.: 72.97%\nEpoch: 2130. Loss: 0.0821. Acc.: 72.83%\nEpoch: 2131. Loss: 0.0731. Acc.: 72.70%\nEpoch: 2132. Loss: 0.0898. Acc.: 74.15%\nEpoch: 2133. Loss: 0.0713. Acc.: 74.15%\nEpoch: 2134. Loss: 0.0686. Acc.: 72.97%\nEpoch: 2135. Loss: 0.0753. Acc.: 73.62%\nEpoch: 2136. Loss: 0.0748. Acc.: 73.10%\nEpoch: 2137. Loss: 0.0779. Acc.: 72.70%\nEpoch: 2138. Loss: 0.0666. Acc.: 72.83%\nEpoch: 2139. Loss: 0.0763. Acc.: 72.97%\nEpoch: 2140. Loss: 0.0782. Acc.: 72.97%\nEpoch: 2141. Loss: 0.0695. Acc.: 73.75%\nEpoch: 2142. Loss: 0.0757. Acc.: 73.49%\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 2143. Loss: 0.0742. Acc.: 73.75%\nEpoch: 2144. Loss: 0.0798. Acc.: 72.57%\nEpoch: 2145. Loss: 0.0849. Acc.: 73.10%\nEpoch: 2146. Loss: 0.0696. Acc.: 73.49%\nEpoch: 2147. Loss: 0.0763. Acc.: 73.10%\nEpoch: 2148. Loss: 0.0726. Acc.: 74.15%\nEpoch: 2149. Loss: 0.0765. Acc.: 74.67%\nEpoch: 2150. Loss: 0.0662. Acc.: 74.28%\nEpoch: 2151. Loss: 0.0687. Acc.: 74.15%\nEpoch: 2152. Loss: 0.0687. Acc.: 73.36%\nEpoch: 2153. Loss: 0.0706. Acc.: 73.88%\nEpoch: 2154. Loss: 0.0936. Acc.: 73.36%\nEpoch: 2155. Loss: 0.0773. Acc.: 72.18%\nEpoch: 2156. Loss: 0.0733. Acc.: 72.18%\nEpoch: 2157. Loss: 0.0795. Acc.: 72.97%\nEpoch: 2158. Loss: 0.0765. Acc.: 73.10%\nEpoch: 2159. Loss: 0.0655. Acc.: 72.31%\nEpoch: 2160. Loss: 0.0766. Acc.: 73.23%\nEpoch: 2161. Loss: 0.0689. Acc.: 72.70%\nEpoch: 2162. Loss: 0.0660. Acc.: 72.44%\nEpoch: 2163. Loss: 0.0669. Acc.: 72.05%\nEpoch: 2164. Loss: 0.0728. Acc.: 72.83%\nEpoch: 2165. Loss: 0.0663. Acc.: 72.57%\nEpoch: 2166. Loss: 0.0662. Acc.: 73.10%\nEpoch: 2167. Loss: 0.0748. Acc.: 73.62%\nEpoch: 2168. Loss: 0.0880. Acc.: 74.02%\nEpoch: 2169. Loss: 0.0761. Acc.: 72.83%\nEpoch: 2170. Loss: 0.0741. Acc.: 72.18%\nEpoch: 2171. Loss: 0.0622. Acc.: 72.97%\nEpoch: 2172. Loss: 0.0864. Acc.: 72.31%\nEpoch: 2173. Loss: 0.0703. Acc.: 72.83%\nEpoch: 2174. Loss: 0.0699. Acc.: 73.23%\nEpoch: 2175. Loss: 0.0709. Acc.: 72.97%\nEpoch: 2176. Loss: 0.0701. Acc.: 73.10%\nEpoch: 2177. Loss: 0.0731. Acc.: 73.10%\nEpoch: 2178. Loss: 0.0791. Acc.: 73.23%\nEpoch: 2179. Loss: 0.0621. Acc.: 73.10%\nEpoch: 2180. Loss: 0.0776. Acc.: 72.31%\nEpoch: 2181. Loss: 0.0914. Acc.: 72.83%\nEpoch: 2182. Loss: 0.0762. Acc.: 73.10%\nEpoch: 2183. Loss: 0.0839. Acc.: 74.02%\nEpoch: 2184. Loss: 0.0864. Acc.: 74.02%\nEpoch: 2185. Loss: 0.0816. Acc.: 74.02%\nEpoch: 2186. Loss: 0.0726. Acc.: 73.10%\nEpoch: 2187. Loss: 0.0802. Acc.: 73.23%\nEpoch: 2188. Loss: 0.0705. Acc.: 73.36%\nEpoch: 2189. Loss: 0.0784. Acc.: 73.23%\nEpoch: 2190. Loss: 0.0643. Acc.: 73.23%\nEpoch: 2191. Loss: 0.0688. Acc.: 72.83%\nEpoch: 2192. Loss: 0.0749. Acc.: 72.70%\nEpoch: 2193. Loss: 0.0705. Acc.: 72.83%\nEpoch: 2194. Loss: 0.0700. Acc.: 73.23%\nEpoch: 2195. Loss: 0.0728. Acc.: 73.10%\nEpoch: 2196. Loss: 0.0635. Acc.: 73.23%\nEpoch: 2197. Loss: 0.0801. Acc.: 73.36%\nEpoch: 2198. Loss: 0.0660. Acc.: 73.88%\nEpoch: 2199. Loss: 0.0698. Acc.: 74.15%\nEpoch: 2200. Loss: 0.0709. Acc.: 74.15%\nEpoch: 2201. Loss: 0.0835. Acc.: 74.67%\nEpoch: 2202. Loss: 0.0764. Acc.: 73.62%\nEpoch: 2203. Loss: 0.0830. Acc.: 72.97%\nEpoch: 2204. Loss: 0.0628. Acc.: 72.83%\nEpoch: 2205. Loss: 0.0651. Acc.: 72.31%\nEpoch: 2206. Loss: 0.0646. Acc.: 72.05%\nEpoch: 2207. Loss: 0.0703. Acc.: 72.18%\nEpoch: 2208. Loss: 0.0807. Acc.: 71.52%\nEpoch: 2209. Loss: 0.0716. Acc.: 72.57%\nEpoch: 2210. Loss: 0.0761. Acc.: 72.83%\nEpoch: 2211. Loss: 0.0737. Acc.: 72.97%\nEpoch: 2212. Loss: 0.0682. Acc.: 73.62%\nEpoch: 2213. Loss: 0.0629. Acc.: 72.97%\nEpoch: 2214. Loss: 0.0729. Acc.: 72.97%\nEpoch: 2215. Loss: 0.0764. Acc.: 72.70%\nEpoch: 2216. Loss: 0.0711. Acc.: 72.70%\nEpoch: 2217. Loss: 0.0654. Acc.: 72.83%\nEpoch: 2218. Loss: 0.0709. Acc.: 73.49%\nEpoch: 2219. Loss: 0.0728. Acc.: 72.83%\nEpoch: 2220. Loss: 0.0898. Acc.: 73.62%\nEpoch: 2221. Loss: 0.0741. Acc.: 72.70%\nEpoch: 2222. Loss: 0.0765. Acc.: 72.57%\nEpoch: 2223. Loss: 0.0811. Acc.: 72.18%\nEpoch: 2224. Loss: 0.0772. Acc.: 72.83%\nEpoch: 2225. Loss: 0.0668. Acc.: 72.44%\nEpoch: 2226. Loss: 0.0746. Acc.: 72.44%\nEpoch: 2227. Loss: 0.0704. Acc.: 72.83%\nEpoch: 2228. Loss: 0.0707. Acc.: 72.70%\nEpoch: 2229. Loss: 0.0826. Acc.: 72.70%\nEpoch: 2230. Loss: 0.0688. Acc.: 73.10%\nEpoch: 2231. Loss: 0.0716. Acc.: 72.44%\nEpoch: 2232. Loss: 0.0756. Acc.: 72.44%\nEpoch: 2233. Loss: 0.0685. Acc.: 72.31%\nEpoch: 2234. Loss: 0.0636. Acc.: 72.44%\nEpoch: 2235. Loss: 0.0755. Acc.: 72.97%\nEpoch: 2236. Loss: 0.0720. Acc.: 72.05%\nEpoch: 2237. Loss: 0.0651. Acc.: 71.92%\nEpoch: 2238. Loss: 0.0810. Acc.: 72.83%\nEpoch: 2239. Loss: 0.0564. Acc.: 72.05%\nEpoch: 2240. Loss: 0.0664. Acc.: 71.92%\nEpoch: 2241. Loss: 0.0769. Acc.: 72.44%\nEpoch: 2242. Loss: 0.0686. Acc.: 72.70%\nEpoch: 2243. Loss: 0.0859. Acc.: 73.88%\nEpoch: 2244. Loss: 0.0775. Acc.: 74.41%\nEpoch: 2245. Loss: 0.0727. Acc.: 71.65%\nEpoch: 2246. Loss: 0.0653. Acc.: 70.47%\nEpoch: 2247. Loss: 0.0757. Acc.: 70.87%\nEpoch: 2248. Loss: 0.0753. Acc.: 71.00%\nEpoch: 2249. Loss: 0.0844. Acc.: 71.92%\nEpoch: 2250. Loss: 0.0721. Acc.: 72.44%\nEpoch: 2251. Loss: 0.0796. Acc.: 73.62%\nEpoch: 2252. Loss: 0.0650. Acc.: 72.57%\nEpoch: 2253. Loss: 0.0792. Acc.: 71.92%\nEpoch: 2254. Loss: 0.0672. Acc.: 72.44%\nEpoch: 2255. Loss: 0.0673. Acc.: 72.83%\nEpoch: 2256. Loss: 0.0621. Acc.: 73.62%\nEpoch: 2257. Loss: 0.0782. Acc.: 72.97%\nEpoch: 2258. Loss: 0.0692. Acc.: 72.83%\nEpoch: 2259. Loss: 0.0636. Acc.: 72.31%\nEpoch: 2260. Loss: 0.0631. Acc.: 72.18%\nEpoch: 2261. Loss: 0.0677. Acc.: 72.57%\nEpoch: 2262. Loss: 0.0596. Acc.: 72.44%\nEpoch: 2263. Loss: 0.0761. Acc.: 73.23%\nEpoch: 2264. Loss: 0.0677. Acc.: 73.75%\nEpoch: 2265. Loss: 0.0683. Acc.: 72.57%\nEpoch: 2266. Loss: 0.0702. Acc.: 71.52%\nEpoch: 2267. Loss: 0.0658. Acc.: 71.52%\nEpoch: 2268. Loss: 0.0679. Acc.: 71.26%\nEpoch: 2269. Loss: 0.0752. Acc.: 72.44%\nEpoch: 2270. Loss: 0.0589. Acc.: 72.83%\nEpoch: 2271. Loss: 0.0643. Acc.: 73.23%\nEpoch: 2272. Loss: 0.0635. Acc.: 73.36%\nEpoch: 2273. Loss: 0.0688. Acc.: 73.36%\nEpoch: 2274. Loss: 0.0699. Acc.: 72.44%\nEpoch: 2275. Loss: 0.0692. Acc.: 72.31%\nEpoch: 2276. Loss: 0.0755. Acc.: 72.18%\nEpoch: 2277. Loss: 0.0673. Acc.: 71.52%\nEpoch: 2278. Loss: 0.0726. Acc.: 72.83%\nEpoch: 2279. Loss: 0.0712. Acc.: 73.49%\nEpoch: 2280. Loss: 0.0616. Acc.: 73.49%\nEpoch: 2281. Loss: 0.0623. Acc.: 72.83%\nEpoch: 2282. Loss: 0.0731. Acc.: 72.83%\nEpoch: 2283. Loss: 0.0749. Acc.: 72.31%\nEpoch: 2284. Loss: 0.0695. Acc.: 72.31%\nEpoch: 2285. Loss: 0.0653. Acc.: 70.73%\nEpoch: 2286. Loss: 0.0715. Acc.: 71.39%\nEpoch: 2287. Loss: 0.0665. Acc.: 72.70%\nEpoch: 2288. Loss: 0.0722. Acc.: 71.52%\nEpoch: 2289. Loss: 0.0667. Acc.: 72.05%\nEpoch: 2290. Loss: 0.0621. Acc.: 71.13%\nEpoch: 2291. Loss: 0.0643. Acc.: 72.18%\nEpoch: 2292. Loss: 0.0687. Acc.: 72.70%\nEpoch: 2293. Loss: 0.0819. Acc.: 73.49%\nEpoch: 2294. Loss: 0.0635. Acc.: 73.10%\nEpoch: 2295. Loss: 0.0728. Acc.: 72.70%\nEpoch: 2296. Loss: 0.0684. Acc.: 73.36%\nEpoch: 2297. Loss: 0.0766. Acc.: 73.23%\nEpoch: 2298. Loss: 0.0669. Acc.: 72.31%\nEpoch: 2299. Loss: 0.0711. Acc.: 72.70%\nEpoch: 2300. Loss: 0.0590. Acc.: 72.83%\nEpoch: 2301. Loss: 0.0708. Acc.: 73.49%\nEpoch: 2302. Loss: 0.0660. Acc.: 73.49%\nEpoch: 2303. Loss: 0.0741. Acc.: 72.83%\nEpoch: 2304. Loss: 0.0710. Acc.: 72.70%\nEpoch: 2305. Loss: 0.0675. Acc.: 73.10%\nEpoch: 2306. Loss: 0.0815. Acc.: 73.10%\nEpoch: 2307. Loss: 0.0647. Acc.: 72.97%\nEpoch: 2308. Loss: 0.0623. Acc.: 73.49%\nEpoch: 2309. Loss: 0.0709. Acc.: 73.36%\nEpoch: 2310. Loss: 0.0704. Acc.: 73.23%\nEpoch: 2311. Loss: 0.0701. Acc.: 72.44%\nEpoch: 2312. Loss: 0.0642. Acc.: 72.31%\nEpoch: 2313. Loss: 0.0690. Acc.: 72.70%\nEpoch: 2314. Loss: 0.0649. Acc.: 73.36%\nEpoch: 2315. Loss: 0.0743. Acc.: 73.75%\nEpoch: 2316. Loss: 0.0632. Acc.: 73.23%\nEpoch: 2317. Loss: 0.0646. Acc.: 72.83%\nEpoch: 2318. Loss: 0.0755. Acc.: 72.97%\nEpoch: 2319. Loss: 0.0680. Acc.: 72.70%\nEpoch: 2320. Loss: 0.0697. Acc.: 73.10%\nEpoch: 2321. Loss: 0.0670. Acc.: 72.83%\nEpoch: 2322. Loss: 0.0626. Acc.: 72.57%\nEpoch: 2323. Loss: 0.0716. Acc.: 72.57%\nEpoch: 2324. Loss: 0.0674. Acc.: 71.78%\nEpoch: 2325. Loss: 0.0713. Acc.: 72.05%\nEpoch: 2326. Loss: 0.0691. Acc.: 72.57%\nEpoch: 2327. Loss: 0.0674. Acc.: 72.70%\nEpoch: 2328. Loss: 0.0727. Acc.: 73.36%\nEpoch: 2329. Loss: 0.0671. Acc.: 73.10%\nEpoch: 2330. Loss: 0.0615. Acc.: 72.83%\nEpoch: 2331. Loss: 0.0623. Acc.: 73.36%\nEpoch: 2332. Loss: 0.0639. Acc.: 73.23%\nEpoch: 2333. Loss: 0.0720. Acc.: 73.62%\nEpoch: 2334. Loss: 0.0640. Acc.: 73.75%\nEpoch: 2335. Loss: 0.0714. Acc.: 73.10%\nEpoch: 2336. Loss: 0.0645. Acc.: 73.62%\nEpoch: 2337. Loss: 0.0617. Acc.: 73.49%\nEpoch: 2338. Loss: 0.0621. Acc.: 73.49%\nEpoch: 2339. Loss: 0.0717. Acc.: 73.36%\nEpoch: 2340. Loss: 0.0694. Acc.: 71.65%\nEpoch: 2341. Loss: 0.0678. Acc.: 71.65%\nEpoch: 2342. Loss: 0.0660. Acc.: 72.57%\nEpoch: 2343. Loss: 0.0647. Acc.: 72.44%\nEpoch: 2344. Loss: 0.0685. Acc.: 72.18%\nEpoch: 2345. Loss: 0.0602. Acc.: 72.70%\nEpoch: 2346. Loss: 0.0581. Acc.: 72.18%\nEpoch: 2347. Loss: 0.0732. Acc.: 73.75%\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 2348. Loss: 0.0635. Acc.: 73.49%\nEpoch: 2349. Loss: 0.0748. Acc.: 73.75%\nEpoch: 2350. Loss: 0.0692. Acc.: 74.02%\nEpoch: 2351. Loss: 0.0691. Acc.: 72.05%\nEpoch: 2352. Loss: 0.0711. Acc.: 71.26%\nEpoch: 2353. Loss: 0.0693. Acc.: 71.52%\nEpoch: 2354. Loss: 0.0753. Acc.: 71.78%\nEpoch: 2355. Loss: 0.0741. Acc.: 72.05%\nEpoch: 2356. Loss: 0.0616. Acc.: 72.18%\nEpoch: 2357. Loss: 0.0692. Acc.: 72.18%\nEpoch: 2358. Loss: 0.0625. Acc.: 72.31%\nEpoch: 2359. Loss: 0.0648. Acc.: 72.18%\nEpoch: 2360. Loss: 0.0643. Acc.: 72.97%\nEpoch: 2361. Loss: 0.0713. Acc.: 72.44%\nEpoch: 2362. Loss: 0.0700. Acc.: 72.31%\nEpoch: 2363. Loss: 0.0702. Acc.: 72.05%\nEpoch: 2364. Loss: 0.0636. Acc.: 71.52%\nEpoch: 2365. Loss: 0.0549. Acc.: 72.05%\nEpoch: 2366. Loss: 0.0677. Acc.: 72.31%\nEpoch: 2367. Loss: 0.0646. Acc.: 70.87%\nEpoch: 2368. Loss: 0.0631. Acc.: 71.13%\nEpoch: 2369. Loss: 0.0657. Acc.: 72.57%\nEpoch: 2370. Loss: 0.0634. Acc.: 71.92%\nEpoch: 2371. Loss: 0.0678. Acc.: 71.13%\nEpoch: 2372. Loss: 0.0669. Acc.: 71.65%\nEpoch: 2373. Loss: 0.0777. Acc.: 72.31%\nEpoch: 2374. Loss: 0.0773. Acc.: 72.05%\nEpoch: 2375. Loss: 0.0703. Acc.: 72.83%\nEpoch: 2376. Loss: 0.0787. Acc.: 72.57%\nEpoch: 2377. Loss: 0.0589. Acc.: 72.18%\nEpoch: 2378. Loss: 0.0754. Acc.: 72.18%\nEpoch: 2379. Loss: 0.0699. Acc.: 72.97%\nEpoch: 2380. Loss: 0.0666. Acc.: 72.83%\nEpoch: 2381. Loss: 0.0619. Acc.: 73.10%\nEpoch: 2382. Loss: 0.0548. Acc.: 72.83%\nEpoch: 2383. Loss: 0.0611. Acc.: 72.44%\nEpoch: 2384. Loss: 0.0634. Acc.: 71.65%\nEpoch: 2385. Loss: 0.0658. Acc.: 71.78%\nEpoch: 2386. Loss: 0.0597. Acc.: 72.05%\nEpoch: 2387. Loss: 0.0650. Acc.: 72.05%\nEpoch: 2388. Loss: 0.0629. Acc.: 72.05%\nEpoch: 2389. Loss: 0.0668. Acc.: 72.18%\nEpoch: 2390. Loss: 0.0619. Acc.: 71.78%\nEpoch: 2391. Loss: 0.0732. Acc.: 72.31%\nEpoch: 2392. Loss: 0.0658. Acc.: 71.52%\nEpoch: 2393. Loss: 0.0724. Acc.: 71.65%\nEpoch: 2394. Loss: 0.0557. Acc.: 71.78%\nEpoch: 2395. Loss: 0.0698. Acc.: 72.57%\nEpoch: 2396. Loss: 0.0665. Acc.: 72.97%\nEpoch: 2397. Loss: 0.0619. Acc.: 73.23%\nEpoch: 2398. Loss: 0.0629. Acc.: 73.36%\nEpoch: 2399. Loss: 0.0661. Acc.: 73.10%\nEpoch: 2400. Loss: 0.0650. Acc.: 72.31%\nEpoch: 2401. Loss: 0.0718. Acc.: 72.70%\nEpoch: 2402. Loss: 0.0615. Acc.: 72.05%\nEpoch: 2403. Loss: 0.0565. Acc.: 72.44%\nEpoch: 2404. Loss: 0.0732. Acc.: 71.65%\nEpoch: 2405. Loss: 0.0543. Acc.: 71.26%\nEpoch: 2406. Loss: 0.0626. Acc.: 71.00%\nEpoch: 2407. Loss: 0.0641. Acc.: 70.73%\nEpoch: 2408. Loss: 0.0702. Acc.: 72.70%\nEpoch: 2409. Loss: 0.0593. Acc.: 72.97%\nEpoch: 2410. Loss: 0.0553. Acc.: 72.70%\nEpoch: 2411. Loss: 0.0536. Acc.: 71.92%\nEpoch: 2412. Loss: 0.0807. Acc.: 73.10%\nEpoch: 2413. Loss: 0.0651. Acc.: 71.78%\nEpoch: 2414. Loss: 0.0626. Acc.: 71.13%\nEpoch: 2415. Loss: 0.0667. Acc.: 72.31%\nEpoch: 2416. Loss: 0.0644. Acc.: 71.39%\nEpoch: 2417. Loss: 0.0518. Acc.: 71.52%\nEpoch: 2418. Loss: 0.0639. Acc.: 71.52%\nEpoch: 2419. Loss: 0.0723. Acc.: 71.92%\nEpoch: 2420. Loss: 0.0610. Acc.: 72.83%\nEpoch: 2421. Loss: 0.0600. Acc.: 73.10%\nEpoch: 2422. Loss: 0.0673. Acc.: 73.49%\nEpoch: 2423. Loss: 0.0708. Acc.: 73.10%\nEpoch: 2424. Loss: 0.0603. Acc.: 73.36%\nEpoch: 2425. Loss: 0.0643. Acc.: 74.15%\nEpoch: 2426. Loss: 0.0757. Acc.: 73.88%\nEpoch: 2427. Loss: 0.0589. Acc.: 72.83%\nEpoch: 2428. Loss: 0.0614. Acc.: 72.83%\nEpoch: 2429. Loss: 0.0605. Acc.: 73.75%\nEpoch: 2430. Loss: 0.0587. Acc.: 73.49%\nEpoch: 2431. Loss: 0.0565. Acc.: 74.02%\nEpoch: 2432. Loss: 0.0694. Acc.: 73.36%\nEpoch: 2433. Loss: 0.0598. Acc.: 73.36%\nEpoch: 2434. Loss: 0.0616. Acc.: 73.36%\nEpoch: 2435. Loss: 0.0553. Acc.: 73.10%\nEpoch: 2436. Loss: 0.0501. Acc.: 72.31%\nEpoch: 2437. Loss: 0.0691. Acc.: 72.31%\nEpoch: 2438. Loss: 0.0664. Acc.: 72.44%\nEpoch: 2439. Loss: 0.0605. Acc.: 72.05%\nEpoch: 2440. Loss: 0.0749. Acc.: 71.92%\nEpoch: 2441. Loss: 0.0521. Acc.: 72.05%\nEpoch: 2442. Loss: 0.0671. Acc.: 73.23%\nEpoch: 2443. Loss: 0.0531. Acc.: 73.36%\nEpoch: 2444. Loss: 0.0620. Acc.: 72.83%\nEpoch: 2445. Loss: 0.0669. Acc.: 73.10%\nEpoch: 2446. Loss: 0.0553. Acc.: 73.49%\nEpoch: 2447. Loss: 0.0673. Acc.: 72.57%\nEpoch: 2448. Loss: 0.0664. Acc.: 72.83%\nEpoch: 2449. Loss: 0.0546. Acc.: 72.57%\nEpoch: 2450. Loss: 0.0748. Acc.: 71.78%\nEpoch: 2451. Loss: 0.0582. Acc.: 71.52%\nEpoch: 2452. Loss: 0.0645. Acc.: 71.65%\nEpoch: 2453. Loss: 0.0555. Acc.: 71.92%\nEpoch: 2454. Loss: 0.0666. Acc.: 71.52%\nEpoch: 2455. Loss: 0.0695. Acc.: 72.44%\nEpoch: 2456. Loss: 0.0709. Acc.: 73.75%\nEpoch: 2457. Loss: 0.0584. Acc.: 72.70%\nEpoch: 2458. Loss: 0.0747. Acc.: 72.44%\nEpoch: 2459. Loss: 0.0569. Acc.: 73.75%\nEpoch: 2460. Loss: 0.0683. Acc.: 73.49%\nEpoch: 2461. Loss: 0.0632. Acc.: 72.44%\nEpoch: 2462. Loss: 0.0739. Acc.: 72.97%\nEpoch: 2463. Loss: 0.0652. Acc.: 71.39%\nEpoch: 2464. Loss: 0.0666. Acc.: 71.78%\nEpoch: 2465. Loss: 0.0702. Acc.: 71.65%\nEpoch: 2466. Loss: 0.0685. Acc.: 72.31%\nEpoch: 2467. Loss: 0.0672. Acc.: 72.44%\nEpoch: 2468. Loss: 0.0607. Acc.: 73.36%\nEpoch: 2469. Loss: 0.0636. Acc.: 73.88%\nEpoch: 2470. Loss: 0.0678. Acc.: 73.10%\nEpoch: 2471. Loss: 0.0619. Acc.: 73.49%\nEpoch: 2472. Loss: 0.0749. Acc.: 73.75%\nEpoch: 2473. Loss: 0.0662. Acc.: 73.49%\nEpoch: 2474. Loss: 0.0700. Acc.: 72.70%\nEpoch: 2475. Loss: 0.0607. Acc.: 73.75%\nEpoch: 2476. Loss: 0.0599. Acc.: 73.88%\nEpoch: 2477. Loss: 0.0702. Acc.: 73.62%\nEpoch: 2478. Loss: 0.0711. Acc.: 72.70%\nEpoch: 2479. Loss: 0.0720. Acc.: 73.49%\nEpoch: 2480. Loss: 0.0700. Acc.: 73.62%\nEpoch: 2481. Loss: 0.0581. Acc.: 73.23%\nEpoch: 2482. Loss: 0.0661. Acc.: 72.18%\nEpoch: 2483. Loss: 0.0571. Acc.: 71.52%\nEpoch: 2484. Loss: 0.0531. Acc.: 71.52%\nEpoch: 2485. Loss: 0.0665. Acc.: 71.65%\nEpoch: 2486. Loss: 0.0543. Acc.: 72.05%\nEpoch: 2487. Loss: 0.0636. Acc.: 73.23%\nEpoch: 2488. Loss: 0.0566. Acc.: 74.02%\nEpoch: 2489. Loss: 0.0605. Acc.: 73.10%\nEpoch: 2490. Loss: 0.0611. Acc.: 72.97%\nEpoch: 2491. Loss: 0.0664. Acc.: 72.57%\nEpoch: 2492. Loss: 0.0542. Acc.: 73.36%\nEpoch: 2493. Loss: 0.0658. Acc.: 72.97%\nEpoch: 2494. Loss: 0.0639. Acc.: 73.36%\nEpoch: 2495. Loss: 0.0585. Acc.: 73.75%\nEpoch: 2496. Loss: 0.0587. Acc.: 72.70%\nEpoch: 2497. Loss: 0.0664. Acc.: 72.97%\nEpoch: 2498. Loss: 0.0658. Acc.: 73.49%\nEpoch: 2499. Loss: 0.0529. Acc.: 72.31%\nEpoch: 2500. Loss: 0.0634. Acc.: 72.44%\nEpoch: 2501. Loss: 0.0593. Acc.: 72.70%\nEpoch: 2502. Loss: 0.0635. Acc.: 73.88%\nEpoch: 2503. Loss: 0.0653. Acc.: 72.83%\nEpoch: 2504. Loss: 0.0568. Acc.: 73.23%\nEpoch: 2505. Loss: 0.0605. Acc.: 72.83%\nEpoch: 2506. Loss: 0.0587. Acc.: 73.49%\nEpoch: 2507. Loss: 0.0680. Acc.: 73.49%\nEpoch: 2508. Loss: 0.0559. Acc.: 73.49%\nEpoch: 2509. Loss: 0.0538. Acc.: 73.23%\nEpoch: 2510. Loss: 0.0654. Acc.: 73.23%\nEpoch: 2511. Loss: 0.0625. Acc.: 72.97%\nEpoch: 2512. Loss: 0.0659. Acc.: 74.02%\nEpoch: 2513. Loss: 0.0631. Acc.: 74.28%\nEpoch: 2514. Loss: 0.0615. Acc.: 71.78%\nEpoch: 2515. Loss: 0.0566. Acc.: 72.44%\nEpoch: 2516. Loss: 0.0544. Acc.: 73.36%\nEpoch: 2517. Loss: 0.0663. Acc.: 72.97%\nEpoch: 2518. Loss: 0.0477. Acc.: 73.10%\nEpoch: 2519. Loss: 0.0599. Acc.: 72.44%\nEpoch: 2520. Loss: 0.0586. Acc.: 73.10%\nEpoch: 2521. Loss: 0.0604. Acc.: 74.41%\nEpoch: 2522. Loss: 0.0587. Acc.: 73.62%\nEpoch: 2523. Loss: 0.0544. Acc.: 73.36%\nEpoch: 2524. Loss: 0.0577. Acc.: 72.83%\nEpoch: 2525. Loss: 0.0558. Acc.: 72.57%\nEpoch: 2526. Loss: 0.0548. Acc.: 73.49%\nEpoch: 2527. Loss: 0.0644. Acc.: 73.88%\nEpoch: 2528. Loss: 0.0585. Acc.: 73.88%\nEpoch: 2529. Loss: 0.0735. Acc.: 72.97%\nEpoch: 2530. Loss: 0.0593. Acc.: 72.05%\nEpoch: 2531. Loss: 0.0581. Acc.: 72.18%\nEpoch: 2532. Loss: 0.0565. Acc.: 72.18%\nEpoch: 2533. Loss: 0.0658. Acc.: 72.83%\nEpoch: 2534. Loss: 0.0553. Acc.: 73.75%\nEpoch: 2535. Loss: 0.0560. Acc.: 73.75%\nEpoch: 2536. Loss: 0.0652. Acc.: 73.10%\nEpoch: 2537. Loss: 0.0564. Acc.: 72.83%\nEpoch: 2538. Loss: 0.0543. Acc.: 72.83%\nEpoch: 2539. Loss: 0.0583. Acc.: 72.97%\nEpoch: 2540. Loss: 0.0584. Acc.: 72.31%\nEpoch: 2541. Loss: 0.0604. Acc.: 71.13%\nEpoch: 2542. Loss: 0.0585. Acc.: 72.05%\nEpoch: 2543. Loss: 0.0609. Acc.: 71.26%\nEpoch: 2544. Loss: 0.0557. Acc.: 72.18%\nEpoch: 2545. Loss: 0.0596. Acc.: 72.57%\nEpoch: 2546. Loss: 0.0532. Acc.: 72.31%\nEpoch: 2547. Loss: 0.0581. Acc.: 70.47%\nEpoch: 2548. Loss: 0.0557. Acc.: 72.05%\nEpoch: 2549. Loss: 0.0533. Acc.: 73.10%\nEpoch: 2550. Loss: 0.0605. Acc.: 73.62%\nEpoch: 2551. Loss: 0.0604. Acc.: 73.75%\nEpoch: 2552. Loss: 0.0556. Acc.: 73.88%\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 2553. Loss: 0.0544. Acc.: 74.54%\nEpoch: 2554. Loss: 0.0607. Acc.: 73.10%\nEpoch: 2555. Loss: 0.0577. Acc.: 72.57%\nEpoch: 2556. Loss: 0.0532. Acc.: 73.23%\nEpoch: 2557. Loss: 0.0540. Acc.: 73.62%\nEpoch: 2558. Loss: 0.0571. Acc.: 73.62%\nEpoch: 2559. Loss: 0.0592. Acc.: 73.23%\nEpoch: 2560. Loss: 0.0607. Acc.: 72.97%\nEpoch: 2561. Loss: 0.0457. Acc.: 74.80%\nEpoch: 2562. Loss: 0.0518. Acc.: 73.62%\nEpoch: 2563. Loss: 0.0617. Acc.: 74.15%\nEpoch: 2564. Loss: 0.0594. Acc.: 74.02%\nEpoch: 2565. Loss: 0.0583. Acc.: 74.15%\nEpoch: 2566. Loss: 0.0580. Acc.: 74.02%\nEpoch: 2567. Loss: 0.0560. Acc.: 73.23%\nEpoch: 2568. Loss: 0.0516. Acc.: 72.31%\nEpoch: 2569. Loss: 0.0552. Acc.: 73.36%\nEpoch: 2570. Loss: 0.0574. Acc.: 73.62%\nEpoch: 2571. Loss: 0.0623. Acc.: 73.75%\nEpoch: 2572. Loss: 0.0615. Acc.: 73.75%\nEpoch: 2573. Loss: 0.0506. Acc.: 74.02%\nEpoch: 2574. Loss: 0.0558. Acc.: 72.44%\nEpoch: 2575. Loss: 0.0635. Acc.: 73.49%\nEpoch: 2576. Loss: 0.0559. Acc.: 73.88%\nEpoch: 2577. Loss: 0.0635. Acc.: 74.67%\nEpoch: 2578. Loss: 0.0475. Acc.: 74.41%\nEpoch: 2579. Loss: 0.0588. Acc.: 75.07%\nEpoch: 2580. Loss: 0.0569. Acc.: 74.28%\nEpoch: 2581. Loss: 0.0626. Acc.: 73.88%\nEpoch: 2582. Loss: 0.0587. Acc.: 73.62%\nEpoch: 2583. Loss: 0.0558. Acc.: 73.62%\nEpoch: 2584. Loss: 0.0702. Acc.: 74.28%\nEpoch: 2585. Loss: 0.0544. Acc.: 74.15%\nEpoch: 2586. Loss: 0.0508. Acc.: 74.02%\nEpoch: 2587. Loss: 0.0454. Acc.: 72.44%\nEpoch: 2588. Loss: 0.0583. Acc.: 72.18%\nEpoch: 2589. Loss: 0.0515. Acc.: 72.83%\nEpoch: 2590. Loss: 0.0428. Acc.: 72.18%\nEpoch: 2591. Loss: 0.0604. Acc.: 71.65%\nEpoch: 2592. Loss: 0.0640. Acc.: 72.83%\nEpoch: 2593. Loss: 0.0516. Acc.: 72.05%\nEpoch: 2594. Loss: 0.0522. Acc.: 71.92%\nEpoch: 2595. Loss: 0.0513. Acc.: 72.70%\nEpoch: 2596. Loss: 0.0583. Acc.: 73.10%\nEpoch: 2597. Loss: 0.0467. Acc.: 72.83%\nEpoch: 2598. Loss: 0.0644. Acc.: 74.15%\nEpoch: 2599. Loss: 0.0582. Acc.: 74.15%\nEarly stopping on epoch 2599\nDone!\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"5\">**Compute result on testing data**</font><br>"},{"metadata":{"_uuid":"d0f483d5-5542-4d65-a70f-1aa7b3e13c3c","_cell_guid":"05b2b8d9-2b63-436e-a906-a839fb008c0e","trusted":true},"cell_type":"code","source":"test_results = []\nmodel.load_state_dict(torch.load('best.pth'))\nmodel.eval()\n\n# for batch in val_dl:\n#     x_raw, y_batch = [t.to(device) for t in batch]\n#     out = model(x_raw)\n#     preds = F.log_softmax(out, dim=1).argmax(dim=1)\n#     total += y_batch.size(0)\n#     correct += (preds == y_batch).sum().item()\n#     acc = correct / total\n#     acc_history.append(acc)\n\n# for batch in tst_dl:\n#     x, y_batche = [t.to(device) for t in (batch)]\n#     out = model(x)\n#     y_hat = F.log_softmax(out, dim=1).argmax(dim=1)\n#     test_results.extend(y_hat.tolist())\n    \nfor batch in tst_dl:\n        x_raw, y_batch = [t.to(device) for t in batch]\n        out = model(x_channel_0, x_channel_1, x_channel_2, x_channel_3, x_channel_4, x_channel_5, x_channel_6, x_channel_7, x_channel_8)\n        preds = F.log_softmax(out, dim=1).argmax(dim=1)\n        total += y_batch.size(0)\n        correct += (preds == y_batch).sum().item()\n    \nacc = correct / total\nprint(\"accurancy on test data: \"+str(acc))","execution_count":90,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"too many values to unpack (expected 2)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-90-70d6de7f5afc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtst_dl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mx_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"]}]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"5\">**Store final result**</font><br>"},{"metadata":{"_uuid":"7a49a38e-e203-4884-ba78-33fe8aa02426","_cell_guid":"f4bd2628-c801-42d5-9af6-11bbd89f2352","trusted":true},"cell_type":"code","source":"'''\nsubmit = pd.read_csv(sample)\nenc = joblib.load(ROOT/'encoder.model')\nsubmit['surface'] = enc.inverse_transform(test_results)\nsubmit.to_csv('submit_base.csv', index=None)\nprint(\"store result successfully!\")\n'''","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}