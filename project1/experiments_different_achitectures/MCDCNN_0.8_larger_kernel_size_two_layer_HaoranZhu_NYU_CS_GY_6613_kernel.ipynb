{"cells":[{"metadata":{},"cell_type":"markdown","source":"<font size=\"5\">**Project Summary**</font><br><br>\nIn this competition, the main task is to do surface classification given robot sensors' time series data. \n\nAfter reviewing the notebook, the champion of this competition is to use fourier analysis [https://www.kaggle.com/trohwer64/simple-fourier-analysis](http://). Other solutions evolve classical machine learning algorithms, e.g Random Forest, Decision Trees... They have achieved good performance. \n\nTraditionally time series classification includes Fourier Transform or feature engineering. Although they can perform well in small datasets, they are heuristic and not task dependent. Traditional machine learning skills may not have the same representation power compared to deep learning. In this project, I mainly explore ways to solve the problem by using deep learning. After searching papers at [https://scholar.google.com/](http://) I found that 1d convolution neural network**[1][2]** is a good technique when doing the time series classification job. So I apply 1d convolutional neural network to solve this problem. The core code is based on pytorch. \n\n\n\nReference Papers:<br>\n*[1] Yi Zheng, Qi Liu, Enhong Chen, Yong Ge, J. Leon Zhao, Time Series Classification Using Multi-Channels Deep Convolutional Neural Networks, IJCAI 2015<br>\n[2] Jian Bo Yang, Minh Nhut Nguyen, Phyo Phyo San, Xiao Li Li, Shonali Krishnaswamy, Deep Convolutional Neural Networks On Multichannel Time Series\nFor Human Activity Recognition, IJCAI 2015<br>*"},{"metadata":{},"cell_type":"markdown","source":"<font size=\"5\">**External libs I use**</font><br>"},{"metadata":{"_uuid":"a6ed998e-ee9a-4333-8332-8fa753b7f0d4","_cell_guid":"e6e0bc2b-9cba-4688-9e0b-15f11d219c2d","trusted":true},"cell_type":"code","source":"# libs we are using\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport torch\nfrom torch import nn\nfrom torch import optim\nfrom torch.nn import functional as F\nfrom torch.utils.data import TensorDataset, DataLoader","execution_count":195,"outputs":[]},{"metadata":{"_uuid":"b8906e48-2384-4bb3-bba7-722160428e39","_cell_guid":"295d38d1-6663-4575-93ba-c6cfac9f6129","trusted":true},"cell_type":"markdown","source":"<font size=\"5\">**Dataset and Data Preprocessing**</font><br>\n\nFor dataset, the original data includes 7816 different time series(First 3810 samples are training data with labels, following 3816 are testing data without labels). Since the professor said there may have some bugs in Kaggle's grading system. We only use original training data right now.\n\nCompared with other methods which firstly prepross data using fourier transformation. I just unify data between 0 and 1 and feed this simply preprocessed data into the neural network. The superising result is that those two methods have similar performance when doing cross validation if feeding different proprecessed data into the same network architecture. This phenomenon seems to tell us that if the neural network is strong enough, it can automatically catch important features in the data.\n\nIn this project, I use the raw data as input of the network. I concatenated all same time series data into one single numpy array. In total there are 7816 time series. The first 3810 rows are training data with given labels, the rest 3816 rows are testing data where labels are not given. In total raw data has 7816 rows. In addition to that, we split the raw training data into training data and validation data by the ratio of 80/20.\n\n\nI create three datasets and data loaders for them to make the data ready for model's training. The process is straightforward. I split the labelled data into two subsets, and keep testing data as is. Also, I convert Numpy arrays into `torch.tensor` objects of proper type (float for samples, and long - for targets).\n"},{"metadata":{"_uuid":"94c41c8d-0c91-4aa1-83a4-ff3f518f35f4","_cell_guid":"88b9ec00-fc64-4040-896b-895d06cd58f2","trusted":true},"cell_type":"code","source":"def create_datasets(data, target, train_size, valid_pct=0.2, seed=None):\n    \"\"\"Converts NumPy arrays into PyTorch datsets.\n    \n    Three datasets are created in total:\n        * training dataset\n        * validation dataset\n        * testing (un-labelled) dataset\n\n    \"\"\"\n    raw =  data\n    sz = train_size\n    idx = np.arange(sz)\n    trn_idx, val_idx = train_test_split(\n        idx, test_size=valid_pct, random_state=seed)\n     \n    trn_ds = TensorDataset(\n        torch.tensor(raw[:sz][trn_idx]).float(), \n        torch.tensor(target[:sz][trn_idx]).long())\n    val_ds = TensorDataset(\n        torch.tensor(raw[:sz][val_idx]).float(), \n        torch.tensor(target[:sz][val_idx]).long())\n    tst_ds = TensorDataset(\n        torch.tensor(raw[sz:]).float(), \n        torch.tensor(target[sz:]).long())\n    return trn_ds, val_ds, tst_ds\n\n# We use this function to divide the data\ndef create_datasets2(data_arr, train_size, valid_pct=0.2, seed=None):\n    \"\"\"Converts NumPy arrays into PyTorch datsets.\n    \n    Three datasets are created in total:\n        * training dataset\n        * validation dataset\n        * testing (un-labelled) dataset\n\n    \"\"\"\n    raw =  data_arr\n    sz = train_size\n\n    idx = np.arange(sz)\n    print(\"total data length\")\n    print(len(idx))\n    trn_idx, tst_idx = train_test_split(idx, test_size=0.2, random_state=1)\n    trn_idx, val_idx = train_test_split(trn_idx, test_size=0.25, random_state=1)\n    \n    print(\"input one dimension shape\")\n    print(raw[0].shape)\n    print(\"training data length\")\n    print(len(trn_idx))\n    print(\"validation data le\")\n    print(len(val_idx))\n    print(\"testing d\")\n    print(len(tst_idx))\n    \n    trn_ds = TensorDataset(\n        torch.tensor(raw[0][:sz][trn_idx]).float(),\n        torch.tensor(raw[1][:sz][trn_idx]).float(),\n        torch.tensor(raw[2][:sz][trn_idx]).float(),\n        torch.tensor(raw[3][:sz][trn_idx]).float(),\n        torch.tensor(raw[4][:sz][trn_idx]).float(),\n        torch.tensor(raw[5][:sz][trn_idx]).float(),\n        torch.tensor(raw[6][:sz][trn_idx]).float(),\n        torch.tensor(raw[7][:sz][trn_idx]).float(),\n        torch.tensor(raw[8][:sz][trn_idx]).float(),\n        torch.tensor(target[:sz][trn_idx]).long())\n    \n    val_ds = TensorDataset(\n        torch.tensor(raw[0][:sz][val_idx]).float(),\n        torch.tensor(raw[1][:sz][val_idx]).float(), \n        torch.tensor(raw[2][:sz][val_idx]).float(), \n        torch.tensor(raw[3][:sz][val_idx]).float(), \n        torch.tensor(raw[4][:sz][val_idx]).float(), \n        torch.tensor(raw[5][:sz][val_idx]).float(), \n        torch.tensor(raw[6][:sz][val_idx]).float(), \n        torch.tensor(raw[7][:sz][val_idx]).float(), \n        torch.tensor(raw[8][:sz][val_idx]).float(), \n        torch.tensor(target[:sz][val_idx]).long())\n    \n    tst_ds = TensorDataset(\n        torch.tensor(raw[0][:sz][tst_idx]).float(),\n        torch.tensor(raw[1][:sz][tst_idx]).float(),\n        torch.tensor(raw[2][:sz][tst_idx]).float(),\n        torch.tensor(raw[3][:sz][tst_idx]).float(),\n        torch.tensor(raw[4][:sz][tst_idx]).float(),\n        torch.tensor(raw[5][:sz][tst_idx]).float(),\n        torch.tensor(raw[6][:sz][tst_idx]).float(),\n        torch.tensor(raw[7][:sz][tst_idx]).float(),\n        torch.tensor(raw[8][:sz][tst_idx]).float(),\n        torch.tensor(target[:sz][tst_idx]).long())\n    \n    return trn_ds, val_ds, tst_ds\n\ndef create_loaders(data, bs=128, jobs=0):\n    \"\"\"Wraps the datasets returned by create_datasets function with data loaders.\"\"\"\n    \n    trn_ds, val_ds, tst_ds = data\n    trn_dl = DataLoader(trn_ds, batch_size=bs, shuffle=True, num_workers=jobs)\n    val_dl = DataLoader(val_ds, batch_size=bs, shuffle=False, num_workers=jobs)\n    tst_dl = DataLoader(tst_ds, batch_size=bs, shuffle=False, num_workers=jobs)\n    return trn_dl, val_dl, tst_dl","execution_count":196,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"5\">**Neural Network Architecture**</font><br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Flatten(nn.Module):\n    \"\"\"Converts N-dimensional tensor into 'flat' one.\"\"\"\n\n    def __init__(self, keep_batch_dim=True):\n        super().__init__()\n        self.keep_batch_dim = keep_batch_dim\n\n    def forward(self, x):\n        if self.keep_batch_dim:\n            return x.view(x.size(0), -1)\n        return x.view(-1)","execution_count":197,"outputs":[]},{"metadata":{"_uuid":"f144e459-53bb-4b5a-860d-abec117fc2cc","_cell_guid":"07870588-c88f-42ff-b782-5ae3c5e6e568","trusted":true},"cell_type":"code","source":"class Surface_Classifier(nn.Module):\n    def __init__(self, raw_ni, no, drop=.5):\n        super().__init__()\n        \n        self.conv1d_channel_0 = nn.Sequential(\n            nn.Conv1d(1, 8, 16, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n\n            nn.Conv1d(8, 16, 11, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n            \n           \n        \n        )\n            \n            \n        \n        self.conv1d_channel_1 = nn.Sequential(\n            nn.Conv1d(1, 8, 16, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n\n            nn.Conv1d(8, 16, 11, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n            \n            \n        )\n        \n        self.conv1d_channel_2 = nn.Sequential(\n            nn.Conv1d(1, 8, 16, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n\n            nn.Conv1d(8, 16, 11, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n            \n           \n        )\n        \n        self.conv1d_channel_3 = nn.Sequential(\n            nn.Conv1d(1, 8, 16, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n\n            nn.Conv1d(8, 16, 11, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n            \n           \n        )\n        \n        self.conv1d_channel_4 = nn.Sequential(\n            nn.Conv1d(1, 8, 16, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n\n            nn.Conv1d(8, 16, 11, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n        )\n            \n            \n        \n        self.conv1d_channel_5 = nn.Sequential(\n            nn.Conv1d(1, 8, 16, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n\n            nn.Conv1d(8, 16, 11, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n            \n        )\n        \n        self.conv1d_channel_6 = nn.Sequential(\n            nn.Conv1d(1, 8, 16, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n\n            nn.Conv1d(8, 16, 11, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n            \n        )\n        \n        self.conv1d_channel_7 = nn.Sequential(\n            nn.Conv1d(1, 8, 16, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n\n            nn.Conv1d(8, 16, 11, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n            \n            \n        )\n            \n\n       \n        \n        self.conv1d_channel_8 = nn.Sequential(\n            nn.Conv1d(1, 8, 16, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n\n            nn.Conv1d(8, 16, 11, 3, 3),\n            nn.ReLU(),\n            nn.Dropout(),\n            \n            nn.MaxPool1d(kernel_size=2),\n            \n           \n        )\n\n            \n\n        self.dense = nn.Sequential(\n            nn.Linear(432, 144),  nn.ReLU(),nn.Dropout(),\n            nn.Linear(144, 64),  nn.ReLU(),nn.Dropout(),\n            nn.Linear(64, no),  nn.ReLU()\n        )\n        \n            \n\n    def forward(self, t_channel_0, t_channel_1, t_channel_2, t_channel_3, t_channel_4, t_channel_5, t_channel_6, t_channel_7, t_channel_8):\n        conv1d_out_channel_0 = self.conv1d_channel_0(t_channel_0)\n        conv1d_out_channel_1 = self.conv1d_channel_1(t_channel_1)\n        conv1d_out_channel_2 = self.conv1d_channel_2(t_channel_2)\n        conv1d_out_channel_3 = self.conv1d_channel_3(t_channel_3)\n        conv1d_out_channel_4 = self.conv1d_channel_4(t_channel_4)\n        conv1d_out_channel_5 = self.conv1d_channel_5(t_channel_5)\n        conv1d_out_channel_6 = self.conv1d_channel_6(t_channel_6)\n        conv1d_out_channel_7 = self.conv1d_channel_7(t_channel_7)\n        conv1d_out_channel_8 = self.conv1d_channel_8(t_channel_8)\n        \n        t_in = torch.cat([conv1d_out_channel_0,conv1d_out_channel_1, conv1d_out_channel_2, conv1d_out_channel_3, conv1d_out_channel_4, conv1d_out_channel_5, conv1d_out_channel_6, conv1d_out_channel_7, conv1d_out_channel_8], dim=1)\n        res = t_in.view(t_in.size(0), -1)\n        out = self.dense(res)\n        return out\n        ","execution_count":198,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"5\">**Initial setup for random seed and project path**</font><br>"},{"metadata":{"_uuid":"b4b74c47-df3d-4201-93a7-f8cb79d6a652","_cell_guid":"aa2ea0fc-dfc2-436e-9a73-aab6bfb63d20","trusted":true},"cell_type":"code","source":"\n# set up the seed\nseed = 1\nnp.random.seed(seed)\n\n# path to sample submission\nsample = Path.cwd().parent/'input'/'career-con-2019'/'sample_submission.csv'\n\nROOT = Path.cwd().parent/'input'/'career-con-2019-preprocessed-data'\nraw_arr = np.load(ROOT/'feat.npy').transpose(0, 2, 1)\ntarget = np.load(ROOT/'target.npy')\n\nraw_dim_data = [None]*9\n#print(raw_dim_data)\nfor i in range(0, 9):\n    raw_dim_data[i] = raw_arr[:,i,:]\n#    print(\"raw data shape\")\n    \n    raw_dim_data[i] = raw_dim_data[i].reshape([7626,1,128])\n#    print(raw_dim_data[i].shape)\n    \n# print(\"raw array shape\")\n# print(raw_arr.shape)\n# print(\"label array shape\")\n# print(target.shape)\n\ntrn_sz = 3810  # only the first `trn_sz` rows in each array include labelled data\n#datasets = create_datasets((raw_arr), target, trn_sz, seed=seed)\ndatasets = create_datasets2((raw_dim_data), trn_sz, seed=seed)\n\n\n# make sure that we run on a proper device (not relevant for Kaggle kernels but helpful in Jupyter sessions)\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\n\ntrn_dl, val_dl, tst_dl = create_loaders(datasets, bs=256)","execution_count":199,"outputs":[{"output_type":"stream","text":"total data length\n3810\ninput one dimension shape\n(7626, 1, 128)\ntraining data length\n2286\nvalidation data le\n762\ntesting d\n762\n","name":"stdout"}]},{"metadata":{"_uuid":"b9026a32-1355-4689-8bf9-581a5228af69","_cell_guid":"5db66d39-80aa-48f4-9aab-4f44eace0192","trusted":true},"cell_type":"markdown","source":"<font size=\"5\">**Training model**</font><br>\nNow everything is ready to create a training loop and see if our model works. For each training epoch, the loop performs the following actions:\n1. train model on the `trn_ds` dataset;\n2. verify quality on the `val_ds` dataset;\n3. check if the quality improved since previous epoch, and if so, save the model's weights onto disk;\n4. in case if the model's quality isn't impoving for `patience` epochs, the training is stopped.\nAlso, the code tracks loss and accuracy history, and prints current scores with exponentially increasing logging frequency, i.e., only at 1, 2, 4, 8... epochs."},{"metadata":{"_uuid":"dee51d07-1cff-4db3-a87d-bebc4bf5bcc0","_cell_guid":"72095ada-fe94-4757-9a8a-918481b04336","trusted":true},"cell_type":"code","source":"raw_feat = raw_arr.shape[1]\n\n\nlr = 0.002\nn_epochs = 10000\niterations_per_epoch = len(trn_dl)\nnum_classes = 9\nbest_acc = 0\npatience, trials = 500, 0\nbase = 1\nstep = 2\nloss_history = []\nacc_history = []\n\nmodel = Surface_Classifier(raw_feat, num_classes).to(device)\nmodel.cuda()\ncriterion = nn.CrossEntropyLoss(reduction='sum')\nopt = optim.Adam(model.parameters(), lr=lr)\n\nprint('Start model training')\n\nfor epoch in range(1, n_epochs + 1):\n    \n    model.train()\n    epoch_loss = 0\n    for i, batch in enumerate(trn_dl):\n        #x_raw, y_batch = [t.to(device) for t in batch]\n        x_channel_0, x_channel_1,  x_channel_2,  x_channel_3,  x_channel_4,  x_channel_5,  x_channel_6,  x_channel_7,  x_channel_8, y_batch = [t.to(device) for t in batch]\n        opt.zero_grad()\n        \n#         print(\"channel 0 type\")\n#         print(type(x_channel_0))\n#         print(\"channel 0 shape\")\n#         print(x_channel_0.shape)\n#         print(\"batch type\")\n#         print(type(batch))\n#         print(len(batch))\n#         print(batch[0].shape)\n#         print(batch[9].shape)\n\n        out = model(x_channel_0, x_channel_1, x_channel_2, x_channel_3, x_channel_4, x_channel_5, x_channel_6, x_channel_7, x_channel_8)\n        \n        \n#         out = []\n#         with torch.no_grad():\n#             for x_channel_0, x_channel_1,  x_channel_2,  x_channel_3,  x_channel_4,  x_channel_5,  x_channel_6,  x_channel_7,  x_channel_8, y_batch in batch:\n#                 output = model(x_channel_0, x_channel_1, x_channel_2, x_channel_3, x_channel_4, x_channel_5, x_channel_6, x_channel_7, x_channel_8)\n#                 out.append(output.detach())\n#         out = torch.cat(out)\n        \n\n    \n        loss = criterion(out, y_batch)\n        epoch_loss += loss.item()\n        loss.backward()\n        opt.step()\n        \n    epoch_loss /= trn_sz\n    loss_history.append(epoch_loss)\n    \n    model.eval()\n    correct, total = 0, 0\n    \n    for batch in val_dl:\n        x_channel_0, x_channel_1,  x_channel_2,  x_channel_3,  x_channel_4,  x_channel_5,  x_channel_6,  x_channel_7,  x_channel_8, y_batch = [t.to(device) for t in batch]\n        \n        out = model(x_channel_0, x_channel_1, x_channel_2, x_channel_3, x_channel_4, x_channel_5, x_channel_6, x_channel_7, x_channel_8)\n        preds = F.log_softmax(out, dim=1).argmax(dim=1)\n        total += y_batch.size(0)\n        correct += (preds == y_batch).sum().item()\n    \n    acc = correct / total\n    acc_history.append(acc)\n\n    if epoch % base == 0:\n        print(f'Epoch: {epoch:3d}. Loss: {epoch_loss:.4f}. Acc.: {acc:2.2%}')\n        base *= step\n    print(f'Epoch: {epoch:3d}. Loss: {epoch_loss:.4f}. Acc.: {acc:2.2%}')\n\n    \n    if acc > best_acc:\n        trials = 0\n        best_acc = acc\n        torch.save(model.state_dict(), 'best.pth')\n        print(f'Epoch {epoch} best model saved with accuracy: {best_acc:2.2%}')\n    else:\n        trials += 1\n        if trials >= patience:\n            print(f'Early stopping on epoch {epoch}')\n            break\n            \nprint('Done!')","execution_count":200,"outputs":[{"output_type":"stream","text":"Start model training\nEpoch:   1. Loss: 1.2813. Acc.: 21.78%\nEpoch:   1. Loss: 1.2813. Acc.: 21.78%\nEpoch 1 best model saved with accuracy: 21.78%\nEpoch:   2. Loss: 1.2446. Acc.: 30.45%\nEpoch:   2. Loss: 1.2446. Acc.: 30.45%\nEpoch 2 best model saved with accuracy: 30.45%\nEpoch:   3. Loss: 1.2006. Acc.: 35.83%\nEpoch 3 best model saved with accuracy: 35.83%\nEpoch:   4. Loss: 1.1605. Acc.: 34.65%\nEpoch:   4. Loss: 1.1605. Acc.: 34.65%\nEpoch:   5. Loss: 1.1238. Acc.: 36.22%\nEpoch 5 best model saved with accuracy: 36.22%\nEpoch:   6. Loss: 1.0983. Acc.: 38.19%\nEpoch 6 best model saved with accuracy: 38.19%\nEpoch:   7. Loss: 1.0753. Acc.: 43.96%\nEpoch 7 best model saved with accuracy: 43.96%\nEpoch:   8. Loss: 1.0632. Acc.: 44.36%\nEpoch:   8. Loss: 1.0632. Acc.: 44.36%\nEpoch 8 best model saved with accuracy: 44.36%\nEpoch:   9. Loss: 1.0452. Acc.: 45.80%\nEpoch 9 best model saved with accuracy: 45.80%\nEpoch:  10. Loss: 1.0404. Acc.: 46.33%\nEpoch 10 best model saved with accuracy: 46.33%\nEpoch:  11. Loss: 1.0205. Acc.: 43.57%\nEpoch:  12. Loss: 1.0086. Acc.: 44.49%\nEpoch:  13. Loss: 1.0009. Acc.: 46.59%\nEpoch 13 best model saved with accuracy: 46.59%\nEpoch:  14. Loss: 0.9728. Acc.: 45.80%\nEpoch:  15. Loss: 0.9827. Acc.: 47.64%\nEpoch 15 best model saved with accuracy: 47.64%\nEpoch:  16. Loss: 0.9690. Acc.: 46.72%\nEpoch:  16. Loss: 0.9690. Acc.: 46.72%\nEpoch:  17. Loss: 0.9532. Acc.: 45.28%\nEpoch:  18. Loss: 0.9443. Acc.: 44.09%\nEpoch:  19. Loss: 0.9468. Acc.: 46.19%\nEpoch:  20. Loss: 0.9347. Acc.: 45.54%\nEpoch:  21. Loss: 0.9361. Acc.: 47.24%\nEpoch:  22. Loss: 0.9148. Acc.: 46.19%\nEpoch:  23. Loss: 0.9220. Acc.: 46.85%\nEpoch:  24. Loss: 0.9164. Acc.: 47.38%\nEpoch:  25. Loss: 0.9047. Acc.: 48.29%\nEpoch 25 best model saved with accuracy: 48.29%\nEpoch:  26. Loss: 0.9079. Acc.: 47.64%\nEpoch:  27. Loss: 0.8843. Acc.: 46.06%\nEpoch:  28. Loss: 0.8868. Acc.: 47.64%\nEpoch:  29. Loss: 0.8936. Acc.: 46.59%\nEpoch:  30. Loss: 0.8821. Acc.: 48.16%\nEpoch:  31. Loss: 0.8754. Acc.: 48.69%\nEpoch 31 best model saved with accuracy: 48.69%\nEpoch:  32. Loss: 0.8604. Acc.: 49.34%\nEpoch:  32. Loss: 0.8604. Acc.: 49.34%\nEpoch 32 best model saved with accuracy: 49.34%\nEpoch:  33. Loss: 0.8711. Acc.: 53.02%\nEpoch 33 best model saved with accuracy: 53.02%\nEpoch:  34. Loss: 0.8511. Acc.: 51.31%\nEpoch:  35. Loss: 0.8563. Acc.: 51.44%\nEpoch:  36. Loss: 0.8470. Acc.: 52.62%\nEpoch:  37. Loss: 0.8462. Acc.: 54.07%\nEpoch 37 best model saved with accuracy: 54.07%\nEpoch:  38. Loss: 0.8334. Acc.: 52.36%\nEpoch:  39. Loss: 0.8412. Acc.: 53.67%\nEpoch:  40. Loss: 0.8330. Acc.: 53.02%\nEpoch:  41. Loss: 0.8186. Acc.: 53.67%\nEpoch:  42. Loss: 0.8144. Acc.: 50.92%\nEpoch:  43. Loss: 0.8164. Acc.: 52.36%\nEpoch:  44. Loss: 0.8136. Acc.: 54.07%\nEpoch:  45. Loss: 0.7983. Acc.: 53.41%\nEpoch:  46. Loss: 0.8034. Acc.: 54.72%\nEpoch 46 best model saved with accuracy: 54.72%\nEpoch:  47. Loss: 0.8044. Acc.: 53.02%\nEpoch:  48. Loss: 0.7866. Acc.: 53.67%\nEpoch:  49. Loss: 0.7831. Acc.: 54.86%\nEpoch 49 best model saved with accuracy: 54.86%\nEpoch:  50. Loss: 0.7805. Acc.: 53.67%\nEpoch:  51. Loss: 0.7718. Acc.: 53.02%\nEpoch:  52. Loss: 0.7678. Acc.: 56.17%\nEpoch 52 best model saved with accuracy: 56.17%\nEpoch:  53. Loss: 0.7673. Acc.: 55.51%\nEpoch:  54. Loss: 0.7603. Acc.: 55.25%\nEpoch:  55. Loss: 0.7524. Acc.: 56.96%\nEpoch 55 best model saved with accuracy: 56.96%\nEpoch:  56. Loss: 0.7518. Acc.: 53.81%\nEpoch:  57. Loss: 0.7564. Acc.: 55.64%\nEpoch:  58. Loss: 0.7524. Acc.: 55.38%\nEpoch:  59. Loss: 0.7505. Acc.: 57.22%\nEpoch 59 best model saved with accuracy: 57.22%\nEpoch:  60. Loss: 0.7232. Acc.: 55.51%\nEpoch:  61. Loss: 0.7413. Acc.: 57.35%\nEpoch 61 best model saved with accuracy: 57.35%\nEpoch:  62. Loss: 0.7191. Acc.: 57.48%\nEpoch 62 best model saved with accuracy: 57.48%\nEpoch:  63. Loss: 0.7289. Acc.: 57.09%\nEpoch:  64. Loss: 0.7164. Acc.: 57.09%\nEpoch:  64. Loss: 0.7164. Acc.: 57.09%\nEpoch:  65. Loss: 0.7120. Acc.: 57.61%\nEpoch 65 best model saved with accuracy: 57.61%\nEpoch:  66. Loss: 0.7098. Acc.: 55.77%\nEpoch:  67. Loss: 0.6982. Acc.: 58.53%\nEpoch 67 best model saved with accuracy: 58.53%\nEpoch:  68. Loss: 0.6880. Acc.: 59.06%\nEpoch 68 best model saved with accuracy: 59.06%\nEpoch:  69. Loss: 0.6971. Acc.: 58.01%\nEpoch:  70. Loss: 0.6867. Acc.: 59.84%\nEpoch 70 best model saved with accuracy: 59.84%\nEpoch:  71. Loss: 0.7010. Acc.: 58.40%\nEpoch:  72. Loss: 0.6813. Acc.: 58.79%\nEpoch:  73. Loss: 0.6677. Acc.: 59.45%\nEpoch:  74. Loss: 0.6826. Acc.: 58.92%\nEpoch:  75. Loss: 0.6802. Acc.: 59.32%\nEpoch:  76. Loss: 0.6631. Acc.: 61.55%\nEpoch 76 best model saved with accuracy: 61.55%\nEpoch:  77. Loss: 0.6708. Acc.: 60.50%\nEpoch:  78. Loss: 0.6583. Acc.: 61.55%\nEpoch:  79. Loss: 0.6652. Acc.: 61.94%\nEpoch 79 best model saved with accuracy: 61.94%\nEpoch:  80. Loss: 0.6683. Acc.: 59.71%\nEpoch:  81. Loss: 0.6533. Acc.: 62.07%\nEpoch 81 best model saved with accuracy: 62.07%\nEpoch:  82. Loss: 0.6490. Acc.: 61.94%\nEpoch:  83. Loss: 0.6385. Acc.: 59.97%\nEpoch:  84. Loss: 0.6301. Acc.: 60.10%\nEpoch:  85. Loss: 0.6290. Acc.: 60.89%\nEpoch:  86. Loss: 0.6529. Acc.: 61.55%\nEpoch:  87. Loss: 0.6218. Acc.: 62.86%\nEpoch 87 best model saved with accuracy: 62.86%\nEpoch:  88. Loss: 0.6335. Acc.: 61.55%\nEpoch:  89. Loss: 0.6285. Acc.: 62.07%\nEpoch:  90. Loss: 0.6255. Acc.: 63.39%\nEpoch 90 best model saved with accuracy: 63.39%\nEpoch:  91. Loss: 0.6366. Acc.: 63.91%\nEpoch 91 best model saved with accuracy: 63.91%\nEpoch:  92. Loss: 0.6240. Acc.: 62.73%\nEpoch:  93. Loss: 0.6412. Acc.: 62.07%\nEpoch:  94. Loss: 0.6164. Acc.: 63.12%\nEpoch:  95. Loss: 0.6082. Acc.: 61.68%\nEpoch:  96. Loss: 0.6210. Acc.: 64.04%\nEpoch 96 best model saved with accuracy: 64.04%\nEpoch:  97. Loss: 0.6106. Acc.: 64.04%\nEpoch:  98. Loss: 0.6034. Acc.: 63.65%\nEpoch:  99. Loss: 0.6058. Acc.: 63.39%\nEpoch: 100. Loss: 0.6067. Acc.: 63.12%\nEpoch: 101. Loss: 0.5981. Acc.: 63.25%\nEpoch: 102. Loss: 0.6080. Acc.: 65.09%\nEpoch 102 best model saved with accuracy: 65.09%\nEpoch: 103. Loss: 0.5985. Acc.: 62.60%\nEpoch: 104. Loss: 0.5962. Acc.: 64.30%\nEpoch: 105. Loss: 0.5775. Acc.: 63.25%\nEpoch: 106. Loss: 0.5831. Acc.: 66.01%\nEpoch 106 best model saved with accuracy: 66.01%\nEpoch: 107. Loss: 0.5997. Acc.: 61.81%\nEpoch: 108. Loss: 0.5900. Acc.: 64.17%\nEpoch: 109. Loss: 0.5765. Acc.: 64.30%\nEpoch: 110. Loss: 0.5912. Acc.: 65.75%\nEpoch: 111. Loss: 0.5801. Acc.: 63.91%\nEpoch: 112. Loss: 0.5741. Acc.: 66.80%\nEpoch 112 best model saved with accuracy: 66.80%\nEpoch: 113. Loss: 0.5677. Acc.: 63.65%\nEpoch: 114. Loss: 0.5810. Acc.: 64.57%\nEpoch: 115. Loss: 0.5648. Acc.: 64.17%\nEpoch: 116. Loss: 0.5765. Acc.: 65.49%\nEpoch: 117. Loss: 0.5765. Acc.: 66.27%\nEpoch: 118. Loss: 0.5568. Acc.: 64.96%\nEpoch: 119. Loss: 0.5624. Acc.: 65.88%\nEpoch: 120. Loss: 0.5525. Acc.: 66.54%\nEpoch: 121. Loss: 0.5648. Acc.: 66.80%\nEpoch: 122. Loss: 0.5575. Acc.: 65.62%\nEpoch: 123. Loss: 0.5619. Acc.: 67.19%\nEpoch 123 best model saved with accuracy: 67.19%\nEpoch: 124. Loss: 0.5601. Acc.: 63.78%\nEpoch: 125. Loss: 0.5619. Acc.: 66.40%\nEpoch: 126. Loss: 0.5634. Acc.: 65.75%\nEpoch: 127. Loss: 0.5612. Acc.: 65.22%\nEpoch: 128. Loss: 0.5342. Acc.: 67.45%\nEpoch: 128. Loss: 0.5342. Acc.: 67.45%\nEpoch 128 best model saved with accuracy: 67.45%\nEpoch: 129. Loss: 0.5496. Acc.: 64.96%\nEpoch: 130. Loss: 0.5332. Acc.: 67.59%\nEpoch 130 best model saved with accuracy: 67.59%\nEpoch: 131. Loss: 0.5439. Acc.: 65.35%\nEpoch: 132. Loss: 0.5401. Acc.: 66.40%\nEpoch: 133. Loss: 0.5364. Acc.: 66.14%\nEpoch: 134. Loss: 0.5413. Acc.: 68.24%\nEpoch 134 best model saved with accuracy: 68.24%\nEpoch: 135. Loss: 0.5384. Acc.: 66.54%\nEpoch: 136. Loss: 0.5349. Acc.: 68.11%\nEpoch: 137. Loss: 0.5390. Acc.: 69.16%\nEpoch 137 best model saved with accuracy: 69.16%\nEpoch: 138. Loss: 0.5326. Acc.: 68.50%\nEpoch: 139. Loss: 0.5315. Acc.: 68.90%\nEpoch: 140. Loss: 0.5164. Acc.: 67.32%\nEpoch: 141. Loss: 0.5126. Acc.: 67.45%\nEpoch: 142. Loss: 0.5212. Acc.: 67.98%\nEpoch: 143. Loss: 0.5071. Acc.: 68.11%\nEpoch: 144. Loss: 0.5042. Acc.: 69.42%\nEpoch 144 best model saved with accuracy: 69.42%\nEpoch: 145. Loss: 0.5173. Acc.: 68.64%\nEpoch: 146. Loss: 0.5111. Acc.: 69.29%\nEpoch: 147. Loss: 0.5161. Acc.: 69.29%\nEpoch: 148. Loss: 0.5026. Acc.: 70.08%\nEpoch 148 best model saved with accuracy: 70.08%\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 149. Loss: 0.5071. Acc.: 70.08%\nEpoch: 150. Loss: 0.5137. Acc.: 68.50%\nEpoch: 151. Loss: 0.5036. Acc.: 69.03%\nEpoch: 152. Loss: 0.5148. Acc.: 70.47%\nEpoch 152 best model saved with accuracy: 70.47%\nEpoch: 153. Loss: 0.4882. Acc.: 70.08%\nEpoch: 154. Loss: 0.5080. Acc.: 69.82%\nEpoch: 155. Loss: 0.4953. Acc.: 70.60%\nEpoch 155 best model saved with accuracy: 70.60%\nEpoch: 156. Loss: 0.5121. Acc.: 69.03%\nEpoch: 157. Loss: 0.5205. Acc.: 70.08%\nEpoch: 158. Loss: 0.5031. Acc.: 72.05%\nEpoch 158 best model saved with accuracy: 72.05%\nEpoch: 159. Loss: 0.4924. Acc.: 69.16%\nEpoch: 160. Loss: 0.4947. Acc.: 69.69%\nEpoch: 161. Loss: 0.4998. Acc.: 70.08%\nEpoch: 162. Loss: 0.4916. Acc.: 71.78%\nEpoch: 163. Loss: 0.4985. Acc.: 69.82%\nEpoch: 164. Loss: 0.4885. Acc.: 70.73%\nEpoch: 165. Loss: 0.5018. Acc.: 70.08%\nEpoch: 166. Loss: 0.4889. Acc.: 70.73%\nEpoch: 167. Loss: 0.4789. Acc.: 69.55%\nEpoch: 168. Loss: 0.4924. Acc.: 71.65%\nEpoch: 169. Loss: 0.4868. Acc.: 69.55%\nEpoch: 170. Loss: 0.4807. Acc.: 71.13%\nEpoch: 171. Loss: 0.4759. Acc.: 70.73%\nEpoch: 172. Loss: 0.4885. Acc.: 71.78%\nEpoch: 173. Loss: 0.4713. Acc.: 71.39%\nEpoch: 174. Loss: 0.4812. Acc.: 71.92%\nEpoch: 175. Loss: 0.4590. Acc.: 71.13%\nEpoch: 176. Loss: 0.4645. Acc.: 71.65%\nEpoch: 177. Loss: 0.4729. Acc.: 71.92%\nEpoch: 178. Loss: 0.4785. Acc.: 73.62%\nEpoch 178 best model saved with accuracy: 73.62%\nEpoch: 179. Loss: 0.4734. Acc.: 69.69%\nEpoch: 180. Loss: 0.4555. Acc.: 72.44%\nEpoch: 181. Loss: 0.4691. Acc.: 71.00%\nEpoch: 182. Loss: 0.4685. Acc.: 74.02%\nEpoch 182 best model saved with accuracy: 74.02%\nEpoch: 183. Loss: 0.4637. Acc.: 69.69%\nEpoch: 184. Loss: 0.4756. Acc.: 72.05%\nEpoch: 185. Loss: 0.4645. Acc.: 72.70%\nEpoch: 186. Loss: 0.4752. Acc.: 72.31%\nEpoch: 187. Loss: 0.4723. Acc.: 71.92%\nEpoch: 188. Loss: 0.4769. Acc.: 72.97%\nEpoch: 189. Loss: 0.4552. Acc.: 72.70%\nEpoch: 190. Loss: 0.4725. Acc.: 73.75%\nEpoch: 191. Loss: 0.4690. Acc.: 72.97%\nEpoch: 192. Loss: 0.4589. Acc.: 73.23%\nEpoch: 193. Loss: 0.4588. Acc.: 73.88%\nEpoch: 194. Loss: 0.4467. Acc.: 73.36%\nEpoch: 195. Loss: 0.4445. Acc.: 73.88%\nEpoch: 196. Loss: 0.4605. Acc.: 72.70%\nEpoch: 197. Loss: 0.4498. Acc.: 73.62%\nEpoch: 198. Loss: 0.4373. Acc.: 72.57%\nEpoch: 199. Loss: 0.4735. Acc.: 72.97%\nEpoch: 200. Loss: 0.4458. Acc.: 73.62%\nEpoch: 201. Loss: 0.4642. Acc.: 72.97%\nEpoch: 202. Loss: 0.4463. Acc.: 73.75%\nEpoch: 203. Loss: 0.4592. Acc.: 73.75%\nEpoch: 204. Loss: 0.4424. Acc.: 75.07%\nEpoch 204 best model saved with accuracy: 75.07%\nEpoch: 205. Loss: 0.4604. Acc.: 73.36%\nEpoch: 206. Loss: 0.4456. Acc.: 73.49%\nEpoch: 207. Loss: 0.4476. Acc.: 72.83%\nEpoch: 208. Loss: 0.4307. Acc.: 75.07%\nEpoch: 209. Loss: 0.4207. Acc.: 74.02%\nEpoch: 210. Loss: 0.4501. Acc.: 72.57%\nEpoch: 211. Loss: 0.4305. Acc.: 74.41%\nEpoch: 212. Loss: 0.4298. Acc.: 73.62%\nEpoch: 213. Loss: 0.4538. Acc.: 74.93%\nEpoch: 214. Loss: 0.4353. Acc.: 74.80%\nEpoch: 215. Loss: 0.4598. Acc.: 75.59%\nEpoch 215 best model saved with accuracy: 75.59%\nEpoch: 216. Loss: 0.4218. Acc.: 74.28%\nEpoch: 217. Loss: 0.4401. Acc.: 74.15%\nEpoch: 218. Loss: 0.4313. Acc.: 74.80%\nEpoch: 219. Loss: 0.4274. Acc.: 74.15%\nEpoch: 220. Loss: 0.4356. Acc.: 74.93%\nEpoch: 221. Loss: 0.4337. Acc.: 75.85%\nEpoch 221 best model saved with accuracy: 75.85%\nEpoch: 222. Loss: 0.4323. Acc.: 74.28%\nEpoch: 223. Loss: 0.4149. Acc.: 75.07%\nEpoch: 224. Loss: 0.4345. Acc.: 74.67%\nEpoch: 225. Loss: 0.4556. Acc.: 74.93%\nEpoch: 226. Loss: 0.4261. Acc.: 74.15%\nEpoch: 227. Loss: 0.4364. Acc.: 75.07%\nEpoch: 228. Loss: 0.4106. Acc.: 74.54%\nEpoch: 229. Loss: 0.4285. Acc.: 74.54%\nEpoch: 230. Loss: 0.4120. Acc.: 74.28%\nEpoch: 231. Loss: 0.4346. Acc.: 74.93%\nEpoch: 232. Loss: 0.4153. Acc.: 74.02%\nEpoch: 233. Loss: 0.4468. Acc.: 74.80%\nEpoch: 234. Loss: 0.4442. Acc.: 72.70%\nEpoch: 235. Loss: 0.4240. Acc.: 74.67%\nEpoch: 236. Loss: 0.4135. Acc.: 73.49%\nEpoch: 237. Loss: 0.4268. Acc.: 75.20%\nEpoch: 238. Loss: 0.4119. Acc.: 74.54%\nEpoch: 239. Loss: 0.4453. Acc.: 75.46%\nEpoch: 240. Loss: 0.4318. Acc.: 74.93%\nEpoch: 241. Loss: 0.4229. Acc.: 74.41%\nEpoch: 242. Loss: 0.4182. Acc.: 74.02%\nEpoch: 243. Loss: 0.4221. Acc.: 75.33%\nEpoch: 244. Loss: 0.4103. Acc.: 75.07%\nEpoch: 245. Loss: 0.4075. Acc.: 74.15%\nEpoch: 246. Loss: 0.4054. Acc.: 74.54%\nEpoch: 247. Loss: 0.4164. Acc.: 74.02%\nEpoch: 248. Loss: 0.4201. Acc.: 74.28%\nEpoch: 249. Loss: 0.4283. Acc.: 75.85%\nEpoch: 250. Loss: 0.4028. Acc.: 75.33%\nEpoch: 251. Loss: 0.4198. Acc.: 73.36%\nEpoch: 252. Loss: 0.4165. Acc.: 74.54%\nEpoch: 253. Loss: 0.4065. Acc.: 74.28%\nEpoch: 254. Loss: 0.3840. Acc.: 74.93%\nEpoch: 255. Loss: 0.4076. Acc.: 74.41%\nEpoch: 256. Loss: 0.4333. Acc.: 72.83%\nEpoch: 256. Loss: 0.4333. Acc.: 72.83%\nEpoch: 257. Loss: 0.4058. Acc.: 75.33%\nEpoch: 258. Loss: 0.4051. Acc.: 74.28%\nEpoch: 259. Loss: 0.3944. Acc.: 75.85%\nEpoch: 260. Loss: 0.4222. Acc.: 74.93%\nEpoch: 261. Loss: 0.4191. Acc.: 74.93%\nEpoch: 262. Loss: 0.3962. Acc.: 74.93%\nEpoch: 263. Loss: 0.4152. Acc.: 73.49%\nEpoch: 264. Loss: 0.4221. Acc.: 74.54%\nEpoch: 265. Loss: 0.4063. Acc.: 74.02%\nEpoch: 266. Loss: 0.4025. Acc.: 74.02%\nEpoch: 267. Loss: 0.3978. Acc.: 75.20%\nEpoch: 268. Loss: 0.3995. Acc.: 73.88%\nEpoch: 269. Loss: 0.4081. Acc.: 75.46%\nEpoch: 270. Loss: 0.3993. Acc.: 75.85%\nEpoch: 271. Loss: 0.4003. Acc.: 76.64%\nEpoch 271 best model saved with accuracy: 76.64%\nEpoch: 272. Loss: 0.4126. Acc.: 77.03%\nEpoch 272 best model saved with accuracy: 77.03%\nEpoch: 273. Loss: 0.3861. Acc.: 76.12%\nEpoch: 274. Loss: 0.3912. Acc.: 75.59%\nEpoch: 275. Loss: 0.4093. Acc.: 76.38%\nEpoch: 276. Loss: 0.3907. Acc.: 76.25%\nEpoch: 277. Loss: 0.3941. Acc.: 77.43%\nEpoch 277 best model saved with accuracy: 77.43%\nEpoch: 278. Loss: 0.3959. Acc.: 77.03%\nEpoch: 279. Loss: 0.3860. Acc.: 75.98%\nEpoch: 280. Loss: 0.3884. Acc.: 76.77%\nEpoch: 281. Loss: 0.3763. Acc.: 76.25%\nEpoch: 282. Loss: 0.3881. Acc.: 76.25%\nEpoch: 283. Loss: 0.4057. Acc.: 74.93%\nEpoch: 284. Loss: 0.3906. Acc.: 74.28%\nEpoch: 285. Loss: 0.3858. Acc.: 76.25%\nEpoch: 286. Loss: 0.3760. Acc.: 76.12%\nEpoch: 287. Loss: 0.3945. Acc.: 75.46%\nEpoch: 288. Loss: 0.4098. Acc.: 76.38%\nEpoch: 289. Loss: 0.3684. Acc.: 75.72%\nEpoch: 290. Loss: 0.3795. Acc.: 76.64%\nEpoch: 291. Loss: 0.3808. Acc.: 76.25%\nEpoch: 292. Loss: 0.4014. Acc.: 75.46%\nEpoch: 293. Loss: 0.3807. Acc.: 75.20%\nEpoch: 294. Loss: 0.3898. Acc.: 76.25%\nEpoch: 295. Loss: 0.3851. Acc.: 74.67%\nEpoch: 296. Loss: 0.3839. Acc.: 75.33%\nEpoch: 297. Loss: 0.3839. Acc.: 75.46%\nEpoch: 298. Loss: 0.3777. Acc.: 75.46%\nEpoch: 299. Loss: 0.3790. Acc.: 75.59%\nEpoch: 300. Loss: 0.4063. Acc.: 74.41%\nEpoch: 301. Loss: 0.4008. Acc.: 75.59%\nEpoch: 302. Loss: 0.3802. Acc.: 74.15%\nEpoch: 303. Loss: 0.4044. Acc.: 74.93%\nEpoch: 304. Loss: 0.3727. Acc.: 75.59%\nEpoch: 305. Loss: 0.3762. Acc.: 74.54%\nEpoch: 306. Loss: 0.3915. Acc.: 76.51%\nEpoch: 307. Loss: 0.3939. Acc.: 75.33%\nEpoch: 308. Loss: 0.3824. Acc.: 75.98%\nEpoch: 309. Loss: 0.4111. Acc.: 75.20%\nEpoch: 310. Loss: 0.4004. Acc.: 75.33%\nEpoch: 311. Loss: 0.3676. Acc.: 75.72%\nEpoch: 312. Loss: 0.3759. Acc.: 75.59%\nEpoch: 313. Loss: 0.3762. Acc.: 76.77%\nEpoch: 314. Loss: 0.3774. Acc.: 76.25%\nEpoch: 315. Loss: 0.3867. Acc.: 76.12%\nEpoch: 316. Loss: 0.3687. Acc.: 75.72%\nEpoch: 317. Loss: 0.3930. Acc.: 75.07%\nEpoch: 318. Loss: 0.3834. Acc.: 74.93%\nEpoch: 319. Loss: 0.3736. Acc.: 75.33%\nEpoch: 320. Loss: 0.3728. Acc.: 74.28%\nEpoch: 321. Loss: 0.3660. Acc.: 75.85%\nEpoch: 322. Loss: 0.3808. Acc.: 74.93%\nEpoch: 323. Loss: 0.3842. Acc.: 75.33%\nEpoch: 324. Loss: 0.3712. Acc.: 75.59%\nEpoch: 325. Loss: 0.3684. Acc.: 74.67%\nEpoch: 326. Loss: 0.3693. Acc.: 76.25%\nEpoch: 327. Loss: 0.3696. Acc.: 76.38%\nEpoch: 328. Loss: 0.3804. Acc.: 76.51%\nEpoch: 329. Loss: 0.3541. Acc.: 75.72%\nEpoch: 330. Loss: 0.3672. Acc.: 75.46%\nEpoch: 331. Loss: 0.3758. Acc.: 74.93%\nEpoch: 332. Loss: 0.3688. Acc.: 75.33%\nEpoch: 333. Loss: 0.3748. Acc.: 75.85%\nEpoch: 334. Loss: 0.3736. Acc.: 75.85%\nEpoch: 335. Loss: 0.3799. Acc.: 74.93%\nEpoch: 336. Loss: 0.3833. Acc.: 75.72%\nEpoch: 337. Loss: 0.3673. Acc.: 75.59%\nEpoch: 338. Loss: 0.3638. Acc.: 77.30%\nEpoch: 339. Loss: 0.3660. Acc.: 75.59%\nEpoch: 340. Loss: 0.3861. Acc.: 76.64%\nEpoch: 341. Loss: 0.3716. Acc.: 75.85%\nEpoch: 342. Loss: 0.3776. Acc.: 76.25%\nEpoch: 343. Loss: 0.3729. Acc.: 75.59%\nEpoch: 344. Loss: 0.3646. Acc.: 77.03%\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 345. Loss: 0.3873. Acc.: 75.59%\nEpoch: 346. Loss: 0.3843. Acc.: 77.17%\nEpoch: 347. Loss: 0.3605. Acc.: 75.72%\nEpoch: 348. Loss: 0.3776. Acc.: 75.72%\nEpoch: 349. Loss: 0.3683. Acc.: 75.85%\nEpoch: 350. Loss: 0.3796. Acc.: 76.38%\nEpoch: 351. Loss: 0.3732. Acc.: 75.85%\nEpoch: 352. Loss: 0.3624. Acc.: 76.51%\nEpoch: 353. Loss: 0.3599. Acc.: 76.51%\nEpoch: 354. Loss: 0.3569. Acc.: 76.25%\nEpoch: 355. Loss: 0.3546. Acc.: 75.59%\nEpoch: 356. Loss: 0.3674. Acc.: 75.33%\nEpoch: 357. Loss: 0.3694. Acc.: 74.93%\nEpoch: 358. Loss: 0.3562. Acc.: 75.07%\nEpoch: 359. Loss: 0.3644. Acc.: 75.46%\nEpoch: 360. Loss: 0.3648. Acc.: 76.51%\nEpoch: 361. Loss: 0.3534. Acc.: 76.77%\nEpoch: 362. Loss: 0.3483. Acc.: 76.90%\nEpoch: 363. Loss: 0.3480. Acc.: 74.80%\nEpoch: 364. Loss: 0.3569. Acc.: 76.51%\nEpoch: 365. Loss: 0.3909. Acc.: 75.85%\nEpoch: 366. Loss: 0.3668. Acc.: 76.64%\nEpoch: 367. Loss: 0.3561. Acc.: 75.33%\nEpoch: 368. Loss: 0.3698. Acc.: 75.98%\nEpoch: 369. Loss: 0.3438. Acc.: 76.77%\nEpoch: 370. Loss: 0.3382. Acc.: 75.33%\nEpoch: 371. Loss: 0.3610. Acc.: 76.90%\nEpoch: 372. Loss: 0.3537. Acc.: 75.33%\nEpoch: 373. Loss: 0.3530. Acc.: 75.07%\nEpoch: 374. Loss: 0.3643. Acc.: 76.51%\nEpoch: 375. Loss: 0.3455. Acc.: 76.77%\nEpoch: 376. Loss: 0.3571. Acc.: 75.59%\nEpoch: 377. Loss: 0.3540. Acc.: 76.64%\nEpoch: 378. Loss: 0.3735. Acc.: 78.35%\nEpoch 378 best model saved with accuracy: 78.35%\nEpoch: 379. Loss: 0.3563. Acc.: 76.90%\nEpoch: 380. Loss: 0.3562. Acc.: 78.22%\nEpoch: 381. Loss: 0.3354. Acc.: 78.08%\nEpoch: 382. Loss: 0.3820. Acc.: 75.85%\nEpoch: 383. Loss: 0.3493. Acc.: 75.85%\nEpoch: 384. Loss: 0.3602. Acc.: 77.03%\nEpoch: 385. Loss: 0.3720. Acc.: 77.03%\nEpoch: 386. Loss: 0.3503. Acc.: 77.56%\nEpoch: 387. Loss: 0.3484. Acc.: 75.33%\nEpoch: 388. Loss: 0.3420. Acc.: 77.03%\nEpoch: 389. Loss: 0.3354. Acc.: 77.43%\nEpoch: 390. Loss: 0.3377. Acc.: 76.25%\nEpoch: 391. Loss: 0.3398. Acc.: 77.69%\nEpoch: 392. Loss: 0.3391. Acc.: 76.12%\nEpoch: 393. Loss: 0.3576. Acc.: 76.51%\nEpoch: 394. Loss: 0.3371. Acc.: 75.98%\nEpoch: 395. Loss: 0.3502. Acc.: 76.51%\nEpoch: 396. Loss: 0.3493. Acc.: 74.80%\nEpoch: 397. Loss: 0.3407. Acc.: 76.90%\nEpoch: 398. Loss: 0.3489. Acc.: 74.93%\nEpoch: 399. Loss: 0.3683. Acc.: 76.64%\nEpoch: 400. Loss: 0.3583. Acc.: 76.51%\nEpoch: 401. Loss: 0.3454. Acc.: 75.59%\nEpoch: 402. Loss: 0.3515. Acc.: 75.72%\nEpoch: 403. Loss: 0.3551. Acc.: 76.12%\nEpoch: 404. Loss: 0.3497. Acc.: 77.17%\nEpoch: 405. Loss: 0.3368. Acc.: 76.51%\nEpoch: 406. Loss: 0.3382. Acc.: 76.25%\nEpoch: 407. Loss: 0.3276. Acc.: 77.43%\nEpoch: 408. Loss: 0.3566. Acc.: 77.03%\nEpoch: 409. Loss: 0.3485. Acc.: 77.69%\nEpoch: 410. Loss: 0.3550. Acc.: 78.22%\nEpoch: 411. Loss: 0.3387. Acc.: 78.87%\nEpoch 411 best model saved with accuracy: 78.87%\nEpoch: 412. Loss: 0.3404. Acc.: 77.56%\nEpoch: 413. Loss: 0.3454. Acc.: 77.82%\nEpoch: 414. Loss: 0.3417. Acc.: 78.08%\nEpoch: 415. Loss: 0.3644. Acc.: 78.22%\nEpoch: 416. Loss: 0.3292. Acc.: 78.74%\nEpoch: 417. Loss: 0.3471. Acc.: 76.90%\nEpoch: 418. Loss: 0.3509. Acc.: 77.69%\nEpoch: 419. Loss: 0.3466. Acc.: 76.38%\nEpoch: 420. Loss: 0.3335. Acc.: 78.08%\nEpoch: 421. Loss: 0.3407. Acc.: 77.30%\nEpoch: 422. Loss: 0.3522. Acc.: 76.90%\nEpoch: 423. Loss: 0.3524. Acc.: 78.74%\nEpoch: 424. Loss: 0.3351. Acc.: 76.90%\nEpoch: 425. Loss: 0.3540. Acc.: 77.82%\nEpoch: 426. Loss: 0.3466. Acc.: 75.98%\nEpoch: 427. Loss: 0.3379. Acc.: 78.35%\nEpoch: 428. Loss: 0.3430. Acc.: 76.90%\nEpoch: 429. Loss: 0.3294. Acc.: 78.61%\nEpoch: 430. Loss: 0.3233. Acc.: 77.43%\nEpoch: 431. Loss: 0.3447. Acc.: 77.17%\nEpoch: 432. Loss: 0.3705. Acc.: 78.35%\nEpoch: 433. Loss: 0.3157. Acc.: 77.43%\nEpoch: 434. Loss: 0.3563. Acc.: 77.56%\nEpoch: 435. Loss: 0.3411. Acc.: 77.30%\nEpoch: 436. Loss: 0.3371. Acc.: 78.08%\nEpoch: 437. Loss: 0.3259. Acc.: 77.43%\nEpoch: 438. Loss: 0.3450. Acc.: 78.22%\nEpoch: 439. Loss: 0.3355. Acc.: 77.30%\nEpoch: 440. Loss: 0.3508. Acc.: 78.22%\nEpoch: 441. Loss: 0.3434. Acc.: 76.77%\nEpoch: 442. Loss: 0.3229. Acc.: 77.95%\nEpoch: 443. Loss: 0.3495. Acc.: 76.64%\nEpoch: 444. Loss: 0.3541. Acc.: 77.43%\nEpoch: 445. Loss: 0.3249. Acc.: 77.17%\nEpoch: 446. Loss: 0.3445. Acc.: 77.43%\nEpoch: 447. Loss: 0.3327. Acc.: 76.25%\nEpoch: 448. Loss: 0.3434. Acc.: 77.95%\nEpoch: 449. Loss: 0.3445. Acc.: 76.51%\nEpoch: 450. Loss: 0.3259. Acc.: 77.30%\nEpoch: 451. Loss: 0.3481. Acc.: 75.98%\nEpoch: 452. Loss: 0.3372. Acc.: 77.30%\nEpoch: 453. Loss: 0.3341. Acc.: 75.72%\nEpoch: 454. Loss: 0.3391. Acc.: 76.12%\nEpoch: 455. Loss: 0.3393. Acc.: 77.03%\nEpoch: 456. Loss: 0.3261. Acc.: 75.98%\nEpoch: 457. Loss: 0.3492. Acc.: 77.30%\nEpoch: 458. Loss: 0.3442. Acc.: 77.43%\nEpoch: 459. Loss: 0.3302. Acc.: 78.08%\nEpoch: 460. Loss: 0.3254. Acc.: 77.69%\nEpoch: 461. Loss: 0.3327. Acc.: 76.90%\nEpoch: 462. Loss: 0.3305. Acc.: 76.64%\nEpoch: 463. Loss: 0.3398. Acc.: 77.69%\nEpoch: 464. Loss: 0.3299. Acc.: 76.12%\nEpoch: 465. Loss: 0.3282. Acc.: 77.43%\nEpoch: 466. Loss: 0.3551. Acc.: 76.90%\nEpoch: 467. Loss: 0.3409. Acc.: 77.95%\nEpoch: 468. Loss: 0.3220. Acc.: 77.43%\nEpoch: 469. Loss: 0.3440. Acc.: 75.98%\nEpoch: 470. Loss: 0.3414. Acc.: 77.82%\nEpoch: 471. Loss: 0.3020. Acc.: 77.43%\nEpoch: 472. Loss: 0.3552. Acc.: 78.35%\nEpoch: 473. Loss: 0.3356. Acc.: 77.82%\nEpoch: 474. Loss: 0.3471. Acc.: 77.43%\nEpoch: 475. Loss: 0.3212. Acc.: 77.30%\nEpoch: 476. Loss: 0.3433. Acc.: 77.17%\nEpoch: 477. Loss: 0.3421. Acc.: 78.35%\nEpoch: 478. Loss: 0.3400. Acc.: 78.87%\nEpoch: 479. Loss: 0.3433. Acc.: 76.64%\nEpoch: 480. Loss: 0.3324. Acc.: 77.95%\nEpoch: 481. Loss: 0.3144. Acc.: 77.82%\nEpoch: 482. Loss: 0.3350. Acc.: 76.77%\nEpoch: 483. Loss: 0.3352. Acc.: 75.59%\nEpoch: 484. Loss: 0.3185. Acc.: 75.98%\nEpoch: 485. Loss: 0.3319. Acc.: 77.95%\nEpoch: 486. Loss: 0.3089. Acc.: 76.90%\nEpoch: 487. Loss: 0.3392. Acc.: 78.08%\nEpoch: 488. Loss: 0.3183. Acc.: 78.48%\nEpoch: 489. Loss: 0.3203. Acc.: 76.25%\nEpoch: 490. Loss: 0.3300. Acc.: 77.69%\nEpoch: 491. Loss: 0.3266. Acc.: 76.90%\nEpoch: 492. Loss: 0.3303. Acc.: 77.03%\nEpoch: 493. Loss: 0.3274. Acc.: 77.43%\nEpoch: 494. Loss: 0.3017. Acc.: 77.03%\nEpoch: 495. Loss: 0.3264. Acc.: 76.90%\nEpoch: 496. Loss: 0.3361. Acc.: 77.03%\nEpoch: 497. Loss: 0.3220. Acc.: 77.56%\nEpoch: 498. Loss: 0.3397. Acc.: 77.03%\nEpoch: 499. Loss: 0.3447. Acc.: 78.61%\nEpoch: 500. Loss: 0.3162. Acc.: 77.30%\nEpoch: 501. Loss: 0.3205. Acc.: 76.77%\nEpoch: 502. Loss: 0.3422. Acc.: 78.61%\nEpoch: 503. Loss: 0.3228. Acc.: 76.25%\nEpoch: 504. Loss: 0.3311. Acc.: 77.43%\nEpoch: 505. Loss: 0.3377. Acc.: 76.90%\nEpoch: 506. Loss: 0.3198. Acc.: 78.08%\nEpoch: 507. Loss: 0.3365. Acc.: 76.90%\nEpoch: 508. Loss: 0.3202. Acc.: 77.82%\nEpoch: 509. Loss: 0.3315. Acc.: 75.85%\nEpoch: 510. Loss: 0.3174. Acc.: 77.82%\nEpoch: 511. Loss: 0.3104. Acc.: 76.90%\nEpoch: 512. Loss: 0.3377. Acc.: 77.82%\nEpoch: 512. Loss: 0.3377. Acc.: 77.82%\nEpoch: 513. Loss: 0.3072. Acc.: 76.51%\nEpoch: 514. Loss: 0.3152. Acc.: 77.30%\nEpoch: 515. Loss: 0.3237. Acc.: 78.08%\nEpoch: 516. Loss: 0.3143. Acc.: 77.17%\nEpoch: 517. Loss: 0.3160. Acc.: 77.43%\nEpoch: 518. Loss: 0.3267. Acc.: 78.35%\nEpoch: 519. Loss: 0.3375. Acc.: 77.43%\nEpoch: 520. Loss: 0.3152. Acc.: 78.08%\nEpoch: 521. Loss: 0.3174. Acc.: 76.25%\nEpoch: 522. Loss: 0.3229. Acc.: 76.77%\nEpoch: 523. Loss: 0.3349. Acc.: 76.25%\nEpoch: 524. Loss: 0.3233. Acc.: 76.51%\nEpoch: 525. Loss: 0.3049. Acc.: 77.17%\nEpoch: 526. Loss: 0.3221. Acc.: 76.77%\nEpoch: 527. Loss: 0.3172. Acc.: 76.25%\nEpoch: 528. Loss: 0.3113. Acc.: 76.25%\nEpoch: 529. Loss: 0.3117. Acc.: 76.64%\nEpoch: 530. Loss: 0.3115. Acc.: 77.56%\nEpoch: 531. Loss: 0.3460. Acc.: 76.51%\nEpoch: 532. Loss: 0.3265. Acc.: 76.38%\nEpoch: 533. Loss: 0.3271. Acc.: 76.77%\nEpoch: 534. Loss: 0.3145. Acc.: 77.43%\nEpoch: 535. Loss: 0.3168. Acc.: 77.69%\nEpoch: 536. Loss: 0.3141. Acc.: 75.98%\nEpoch: 537. Loss: 0.3136. Acc.: 77.56%\nEpoch: 538. Loss: 0.3160. Acc.: 77.17%\nEpoch: 539. Loss: 0.3067. Acc.: 76.90%\nEpoch: 540. Loss: 0.3210. Acc.: 76.64%\nEpoch: 541. Loss: 0.3086. Acc.: 78.74%\nEpoch: 542. Loss: 0.3146. Acc.: 76.25%\nEpoch: 543. Loss: 0.3296. Acc.: 77.30%\nEpoch: 544. Loss: 0.3351. Acc.: 76.64%\nEpoch: 545. Loss: 0.3167. Acc.: 77.17%\nEpoch: 546. Loss: 0.3246. Acc.: 77.95%\nEpoch: 547. Loss: 0.3099. Acc.: 77.43%\nEpoch: 548. Loss: 0.3246. Acc.: 77.03%\nEpoch: 549. Loss: 0.3316. Acc.: 78.61%\nEpoch: 550. Loss: 0.3187. Acc.: 76.12%\nEpoch: 551. Loss: 0.3320. Acc.: 77.69%\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 552. Loss: 0.3317. Acc.: 78.48%\nEpoch: 553. Loss: 0.3120. Acc.: 76.90%\nEpoch: 554. Loss: 0.3168. Acc.: 78.61%\nEpoch: 555. Loss: 0.3155. Acc.: 77.69%\nEpoch: 556. Loss: 0.3289. Acc.: 77.56%\nEpoch: 557. Loss: 0.2954. Acc.: 77.03%\nEpoch: 558. Loss: 0.3088. Acc.: 76.77%\nEpoch: 559. Loss: 0.3219. Acc.: 77.30%\nEpoch: 560. Loss: 0.3189. Acc.: 77.30%\nEpoch: 561. Loss: 0.3220. Acc.: 77.56%\nEpoch: 562. Loss: 0.3215. Acc.: 75.98%\nEpoch: 563. Loss: 0.3054. Acc.: 77.30%\nEpoch: 564. Loss: 0.3281. Acc.: 77.17%\nEpoch: 565. Loss: 0.3186. Acc.: 76.51%\nEpoch: 566. Loss: 0.3057. Acc.: 76.90%\nEpoch: 567. Loss: 0.3081. Acc.: 76.77%\nEpoch: 568. Loss: 0.3179. Acc.: 77.82%\nEpoch: 569. Loss: 0.3417. Acc.: 77.82%\nEpoch: 570. Loss: 0.3241. Acc.: 77.95%\nEpoch: 571. Loss: 0.2979. Acc.: 78.74%\nEpoch: 572. Loss: 0.3237. Acc.: 76.51%\nEpoch: 573. Loss: 0.3163. Acc.: 76.64%\nEpoch: 574. Loss: 0.2853. Acc.: 77.03%\nEpoch: 575. Loss: 0.3126. Acc.: 77.69%\nEpoch: 576. Loss: 0.2930. Acc.: 78.35%\nEpoch: 577. Loss: 0.3110. Acc.: 77.69%\nEpoch: 578. Loss: 0.3269. Acc.: 75.72%\nEpoch: 579. Loss: 0.3282. Acc.: 78.08%\nEpoch: 580. Loss: 0.3282. Acc.: 76.77%\nEpoch: 581. Loss: 0.2872. Acc.: 76.64%\nEpoch: 582. Loss: 0.3279. Acc.: 77.95%\nEpoch: 583. Loss: 0.2996. Acc.: 76.51%\nEpoch: 584. Loss: 0.3000. Acc.: 75.98%\nEpoch: 585. Loss: 0.3154. Acc.: 77.56%\nEpoch: 586. Loss: 0.2937. Acc.: 77.17%\nEpoch: 587. Loss: 0.3043. Acc.: 76.12%\nEpoch: 588. Loss: 0.3223. Acc.: 76.90%\nEpoch: 589. Loss: 0.2958. Acc.: 76.64%\nEpoch: 590. Loss: 0.3153. Acc.: 76.77%\nEpoch: 591. Loss: 0.2987. Acc.: 77.82%\nEpoch: 592. Loss: 0.3148. Acc.: 76.64%\nEpoch: 593. Loss: 0.2993. Acc.: 79.27%\nEpoch 593 best model saved with accuracy: 79.27%\nEpoch: 594. Loss: 0.3055. Acc.: 77.17%\nEpoch: 595. Loss: 0.3080. Acc.: 77.03%\nEpoch: 596. Loss: 0.3241. Acc.: 77.43%\nEpoch: 597. Loss: 0.3018. Acc.: 77.17%\nEpoch: 598. Loss: 0.3146. Acc.: 77.69%\nEpoch: 599. Loss: 0.3129. Acc.: 77.95%\nEpoch: 600. Loss: 0.3091. Acc.: 77.69%\nEpoch: 601. Loss: 0.3043. Acc.: 77.17%\nEpoch: 602. Loss: 0.3057. Acc.: 76.51%\nEpoch: 603. Loss: 0.3230. Acc.: 77.69%\nEpoch: 604. Loss: 0.3251. Acc.: 76.64%\nEpoch: 605. Loss: 0.3116. Acc.: 77.17%\nEpoch: 606. Loss: 0.3137. Acc.: 77.56%\nEpoch: 607. Loss: 0.3061. Acc.: 76.77%\nEpoch: 608. Loss: 0.3154. Acc.: 77.30%\nEpoch: 609. Loss: 0.3136. Acc.: 77.03%\nEpoch: 610. Loss: 0.3001. Acc.: 78.35%\nEpoch: 611. Loss: 0.2860. Acc.: 76.64%\nEpoch: 612. Loss: 0.3380. Acc.: 77.82%\nEpoch: 613. Loss: 0.3075. Acc.: 77.56%\nEpoch: 614. Loss: 0.3027. Acc.: 76.25%\nEpoch: 615. Loss: 0.3031. Acc.: 76.38%\nEpoch: 616. Loss: 0.3217. Acc.: 76.77%\nEpoch: 617. Loss: 0.3083. Acc.: 78.74%\nEpoch: 618. Loss: 0.2953. Acc.: 78.61%\nEpoch: 619. Loss: 0.3000. Acc.: 77.82%\nEpoch: 620. Loss: 0.3070. Acc.: 77.82%\nEpoch: 621. Loss: 0.3145. Acc.: 78.22%\nEpoch: 622. Loss: 0.3009. Acc.: 77.03%\nEpoch: 623. Loss: 0.3027. Acc.: 77.82%\nEpoch: 624. Loss: 0.2939. Acc.: 77.69%\nEpoch: 625. Loss: 0.2957. Acc.: 77.17%\nEpoch: 626. Loss: 0.2960. Acc.: 78.22%\nEpoch: 627. Loss: 0.2946. Acc.: 77.30%\nEpoch: 628. Loss: 0.3073. Acc.: 78.08%\nEpoch: 629. Loss: 0.3075. Acc.: 78.61%\nEpoch: 630. Loss: 0.3031. Acc.: 77.82%\nEpoch: 631. Loss: 0.3162. Acc.: 75.98%\nEpoch: 632. Loss: 0.2921. Acc.: 77.43%\nEpoch: 633. Loss: 0.2959. Acc.: 77.30%\nEpoch: 634. Loss: 0.2840. Acc.: 78.35%\nEpoch: 635. Loss: 0.2926. Acc.: 78.61%\nEpoch: 636. Loss: 0.2936. Acc.: 77.69%\nEpoch: 637. Loss: 0.2888. Acc.: 77.43%\nEpoch: 638. Loss: 0.3081. Acc.: 78.35%\nEpoch: 639. Loss: 0.3068. Acc.: 79.13%\nEpoch: 640. Loss: 0.3103. Acc.: 78.35%\nEpoch: 641. Loss: 0.2898. Acc.: 78.08%\nEpoch: 642. Loss: 0.3004. Acc.: 77.56%\nEpoch: 643. Loss: 0.3091. Acc.: 78.61%\nEpoch: 644. Loss: 0.3061. Acc.: 77.69%\nEpoch: 645. Loss: 0.2910. Acc.: 78.61%\nEpoch: 646. Loss: 0.3034. Acc.: 77.95%\nEpoch: 647. Loss: 0.2975. Acc.: 77.69%\nEpoch: 648. Loss: 0.3064. Acc.: 77.03%\nEpoch: 649. Loss: 0.3081. Acc.: 78.08%\nEpoch: 650. Loss: 0.2961. Acc.: 76.90%\nEpoch: 651. Loss: 0.3051. Acc.: 77.03%\nEpoch: 652. Loss: 0.3036. Acc.: 77.56%\nEpoch: 653. Loss: 0.3282. Acc.: 77.30%\nEpoch: 654. Loss: 0.3002. Acc.: 77.56%\nEpoch: 655. Loss: 0.2981. Acc.: 77.82%\nEpoch: 656. Loss: 0.2868. Acc.: 76.90%\nEpoch: 657. Loss: 0.3047. Acc.: 77.30%\nEpoch: 658. Loss: 0.3056. Acc.: 77.69%\nEpoch: 659. Loss: 0.3147. Acc.: 77.69%\nEpoch: 660. Loss: 0.2966. Acc.: 77.03%\nEpoch: 661. Loss: 0.3085. Acc.: 76.64%\nEpoch: 662. Loss: 0.2975. Acc.: 77.43%\nEpoch: 663. Loss: 0.3001. Acc.: 76.90%\nEpoch: 664. Loss: 0.2958. Acc.: 77.56%\nEpoch: 665. Loss: 0.3019. Acc.: 76.77%\nEpoch: 666. Loss: 0.2858. Acc.: 77.69%\nEpoch: 667. Loss: 0.2825. Acc.: 77.30%\nEpoch: 668. Loss: 0.3024. Acc.: 77.30%\nEpoch: 669. Loss: 0.3057. Acc.: 78.48%\nEpoch: 670. Loss: 0.2954. Acc.: 78.22%\nEpoch: 671. Loss: 0.2985. Acc.: 77.82%\nEpoch: 672. Loss: 0.2803. Acc.: 77.95%\nEpoch: 673. Loss: 0.2893. Acc.: 78.22%\nEpoch: 674. Loss: 0.2993. Acc.: 77.82%\nEpoch: 675. Loss: 0.3235. Acc.: 77.56%\nEpoch: 676. Loss: 0.2972. Acc.: 78.48%\nEpoch: 677. Loss: 0.2920. Acc.: 78.35%\nEpoch: 678. Loss: 0.2965. Acc.: 78.61%\nEpoch: 679. Loss: 0.2873. Acc.: 78.08%\nEpoch: 680. Loss: 0.2984. Acc.: 77.82%\nEpoch: 681. Loss: 0.2993. Acc.: 77.56%\nEpoch: 682. Loss: 0.2779. Acc.: 79.00%\nEpoch: 683. Loss: 0.3145. Acc.: 77.69%\nEpoch: 684. Loss: 0.2830. Acc.: 77.82%\nEpoch: 685. Loss: 0.3019. Acc.: 77.43%\nEpoch: 686. Loss: 0.3029. Acc.: 77.69%\nEpoch: 687. Loss: 0.2877. Acc.: 77.56%\nEpoch: 688. Loss: 0.2955. Acc.: 76.38%\nEpoch: 689. Loss: 0.3069. Acc.: 76.90%\nEpoch: 690. Loss: 0.2903. Acc.: 77.82%\nEpoch: 691. Loss: 0.3086. Acc.: 74.93%\nEpoch: 692. Loss: 0.2999. Acc.: 78.48%\nEpoch: 693. Loss: 0.2848. Acc.: 77.43%\nEpoch: 694. Loss: 0.2977. Acc.: 75.72%\nEpoch: 695. Loss: 0.3088. Acc.: 77.82%\nEpoch: 696. Loss: 0.2988. Acc.: 77.56%\nEpoch: 697. Loss: 0.3149. Acc.: 77.56%\nEpoch: 698. Loss: 0.2798. Acc.: 77.69%\nEpoch: 699. Loss: 0.2948. Acc.: 77.17%\nEpoch: 700. Loss: 0.2893. Acc.: 77.56%\nEpoch: 701. Loss: 0.3189. Acc.: 78.08%\nEpoch: 702. Loss: 0.2912. Acc.: 78.08%\nEpoch: 703. Loss: 0.2912. Acc.: 77.43%\nEpoch: 704. Loss: 0.3143. Acc.: 77.69%\nEpoch: 705. Loss: 0.2993. Acc.: 77.82%\nEpoch: 706. Loss: 0.2856. Acc.: 78.35%\nEpoch: 707. Loss: 0.2802. Acc.: 77.30%\nEpoch: 708. Loss: 0.3007. Acc.: 77.56%\nEpoch: 709. Loss: 0.3035. Acc.: 77.69%\nEpoch: 710. Loss: 0.3036. Acc.: 76.64%\nEpoch: 711. Loss: 0.2866. Acc.: 78.61%\nEpoch: 712. Loss: 0.2861. Acc.: 77.95%\nEpoch: 713. Loss: 0.2955. Acc.: 77.82%\nEpoch: 714. Loss: 0.2826. Acc.: 77.82%\nEpoch: 715. Loss: 0.3020. Acc.: 77.82%\nEpoch: 716. Loss: 0.3029. Acc.: 77.30%\nEpoch: 717. Loss: 0.3023. Acc.: 78.22%\nEpoch: 718. Loss: 0.2813. Acc.: 78.61%\nEpoch: 719. Loss: 0.2925. Acc.: 78.08%\nEpoch: 720. Loss: 0.3038. Acc.: 77.43%\nEpoch: 721. Loss: 0.2999. Acc.: 78.74%\nEpoch: 722. Loss: 0.2779. Acc.: 78.74%\nEpoch: 723. Loss: 0.2816. Acc.: 78.87%\nEpoch: 724. Loss: 0.2986. Acc.: 78.61%\nEpoch: 725. Loss: 0.2906. Acc.: 78.48%\nEpoch: 726. Loss: 0.2934. Acc.: 77.43%\nEpoch: 727. Loss: 0.2920. Acc.: 78.22%\nEpoch: 728. Loss: 0.2992. Acc.: 79.40%\nEpoch 728 best model saved with accuracy: 79.40%\nEpoch: 729. Loss: 0.2986. Acc.: 78.35%\nEpoch: 730. Loss: 0.2946. Acc.: 77.17%\nEpoch: 731. Loss: 0.2984. Acc.: 78.74%\nEpoch: 732. Loss: 0.2992. Acc.: 77.69%\nEpoch: 733. Loss: 0.2877. Acc.: 77.03%\nEpoch: 734. Loss: 0.3054. Acc.: 77.43%\nEpoch: 735. Loss: 0.3005. Acc.: 77.95%\nEpoch: 736. Loss: 0.2893. Acc.: 78.22%\nEpoch: 737. Loss: 0.2934. Acc.: 78.35%\nEpoch: 738. Loss: 0.3077. Acc.: 77.82%\nEpoch: 739. Loss: 0.2922. Acc.: 78.08%\nEpoch: 740. Loss: 0.3015. Acc.: 78.08%\nEpoch: 741. Loss: 0.2719. Acc.: 78.35%\nEpoch: 742. Loss: 0.2871. Acc.: 77.69%\nEpoch: 743. Loss: 0.2971. Acc.: 77.95%\nEpoch: 744. Loss: 0.2933. Acc.: 76.77%\nEpoch: 745. Loss: 0.2952. Acc.: 77.17%\nEpoch: 746. Loss: 0.2989. Acc.: 77.56%\nEpoch: 747. Loss: 0.2886. Acc.: 77.69%\nEpoch: 748. Loss: 0.3010. Acc.: 77.56%\nEpoch: 749. Loss: 0.2925. Acc.: 77.43%\nEpoch: 750. Loss: 0.2928. Acc.: 78.08%\nEpoch: 751. Loss: 0.2651. Acc.: 77.43%\nEpoch: 752. Loss: 0.2872. Acc.: 78.08%\nEpoch: 753. Loss: 0.2670. Acc.: 77.56%\nEpoch: 754. Loss: 0.2774. Acc.: 77.56%\nEpoch: 755. Loss: 0.2872. Acc.: 77.43%\nEpoch: 756. Loss: 0.2938. Acc.: 77.95%\nEpoch: 757. Loss: 0.2867. Acc.: 77.82%\nEpoch: 758. Loss: 0.2992. Acc.: 77.56%\nEpoch: 759. Loss: 0.2828. Acc.: 78.22%\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 760. Loss: 0.3018. Acc.: 78.08%\nEpoch: 761. Loss: 0.2917. Acc.: 77.69%\nEpoch: 762. Loss: 0.2945. Acc.: 77.82%\nEpoch: 763. Loss: 0.2670. Acc.: 78.22%\nEpoch: 764. Loss: 0.2779. Acc.: 77.69%\nEpoch: 765. Loss: 0.2815. Acc.: 77.17%\nEpoch: 766. Loss: 0.2700. Acc.: 78.61%\nEpoch: 767. Loss: 0.2868. Acc.: 78.35%\nEpoch: 768. Loss: 0.2689. Acc.: 78.48%\nEpoch: 769. Loss: 0.2716. Acc.: 77.30%\nEpoch: 770. Loss: 0.2964. Acc.: 78.08%\nEpoch: 771. Loss: 0.2896. Acc.: 78.08%\nEpoch: 772. Loss: 0.2833. Acc.: 77.03%\nEpoch: 773. Loss: 0.2645. Acc.: 76.77%\nEpoch: 774. Loss: 0.2857. Acc.: 78.08%\nEpoch: 775. Loss: 0.2894. Acc.: 77.30%\nEpoch: 776. Loss: 0.2936. Acc.: 77.82%\nEpoch: 777. Loss: 0.2701. Acc.: 77.95%\nEpoch: 778. Loss: 0.2857. Acc.: 78.08%\nEpoch: 779. Loss: 0.2761. Acc.: 77.30%\nEpoch: 780. Loss: 0.3000. Acc.: 77.17%\nEpoch: 781. Loss: 0.2863. Acc.: 77.82%\nEpoch: 782. Loss: 0.2601. Acc.: 77.03%\nEpoch: 783. Loss: 0.3116. Acc.: 77.95%\nEpoch: 784. Loss: 0.2777. Acc.: 76.77%\nEpoch: 785. Loss: 0.3050. Acc.: 78.08%\nEpoch: 786. Loss: 0.2951. Acc.: 77.82%\nEpoch: 787. Loss: 0.3074. Acc.: 77.17%\nEpoch: 788. Loss: 0.2885. Acc.: 77.30%\nEpoch: 789. Loss: 0.2731. Acc.: 77.56%\nEpoch: 790. Loss: 0.2986. Acc.: 77.82%\nEpoch: 791. Loss: 0.2816. Acc.: 77.17%\nEpoch: 792. Loss: 0.2796. Acc.: 77.95%\nEpoch: 793. Loss: 0.2865. Acc.: 77.56%\nEpoch: 794. Loss: 0.2729. Acc.: 78.74%\nEpoch: 795. Loss: 0.2695. Acc.: 77.69%\nEpoch: 796. Loss: 0.2803. Acc.: 77.30%\nEpoch: 797. Loss: 0.2739. Acc.: 78.22%\nEpoch: 798. Loss: 0.2880. Acc.: 76.38%\nEpoch: 799. Loss: 0.2868. Acc.: 78.22%\nEpoch: 800. Loss: 0.2759. Acc.: 77.82%\nEpoch: 801. Loss: 0.2684. Acc.: 76.51%\nEpoch: 802. Loss: 0.2787. Acc.: 78.08%\nEpoch: 803. Loss: 0.2848. Acc.: 78.22%\nEpoch: 804. Loss: 0.2735. Acc.: 77.30%\nEpoch: 805. Loss: 0.2865. Acc.: 77.82%\nEpoch: 806. Loss: 0.2770. Acc.: 78.35%\nEpoch: 807. Loss: 0.2905. Acc.: 77.95%\nEpoch: 808. Loss: 0.2799. Acc.: 79.53%\nEpoch 808 best model saved with accuracy: 79.53%\nEpoch: 809. Loss: 0.2747. Acc.: 77.82%\nEpoch: 810. Loss: 0.2849. Acc.: 77.69%\nEpoch: 811. Loss: 0.2959. Acc.: 79.13%\nEpoch: 812. Loss: 0.2872. Acc.: 77.17%\nEpoch: 813. Loss: 0.2926. Acc.: 78.08%\nEpoch: 814. Loss: 0.2801. Acc.: 77.69%\nEpoch: 815. Loss: 0.2742. Acc.: 77.03%\nEpoch: 816. Loss: 0.2685. Acc.: 78.87%\nEpoch: 817. Loss: 0.2934. Acc.: 77.56%\nEpoch: 818. Loss: 0.3079. Acc.: 78.48%\nEpoch: 819. Loss: 0.2729. Acc.: 77.95%\nEpoch: 820. Loss: 0.2654. Acc.: 77.69%\nEpoch: 821. Loss: 0.2850. Acc.: 77.17%\nEpoch: 822. Loss: 0.2880. Acc.: 76.77%\nEpoch: 823. Loss: 0.2594. Acc.: 77.95%\nEpoch: 824. Loss: 0.2959. Acc.: 77.69%\nEpoch: 825. Loss: 0.2883. Acc.: 77.82%\nEpoch: 826. Loss: 0.2814. Acc.: 78.22%\nEpoch: 827. Loss: 0.2592. Acc.: 79.13%\nEpoch: 828. Loss: 0.2820. Acc.: 77.56%\nEpoch: 829. Loss: 0.2824. Acc.: 78.35%\nEpoch: 830. Loss: 0.2950. Acc.: 78.74%\nEpoch: 831. Loss: 0.2838. Acc.: 78.35%\nEpoch: 832. Loss: 0.2562. Acc.: 77.56%\nEpoch: 833. Loss: 0.2811. Acc.: 77.95%\nEpoch: 834. Loss: 0.2848. Acc.: 77.69%\nEpoch: 835. Loss: 0.2805. Acc.: 78.35%\nEpoch: 836. Loss: 0.2718. Acc.: 78.22%\nEpoch: 837. Loss: 0.2881. Acc.: 77.43%\nEpoch: 838. Loss: 0.2659. Acc.: 77.30%\nEpoch: 839. Loss: 0.2602. Acc.: 76.77%\nEpoch: 840. Loss: 0.2963. Acc.: 78.87%\nEpoch: 841. Loss: 0.2869. Acc.: 76.64%\nEpoch: 842. Loss: 0.2757. Acc.: 77.43%\nEpoch: 843. Loss: 0.2975. Acc.: 77.17%\nEpoch: 844. Loss: 0.2803. Acc.: 76.90%\nEpoch: 845. Loss: 0.2781. Acc.: 78.35%\nEpoch: 846. Loss: 0.2631. Acc.: 78.08%\nEpoch: 847. Loss: 0.2768. Acc.: 77.30%\nEpoch: 848. Loss: 0.2886. Acc.: 78.08%\nEpoch: 849. Loss: 0.2670. Acc.: 77.69%\nEpoch: 850. Loss: 0.2824. Acc.: 77.82%\nEpoch: 851. Loss: 0.2708. Acc.: 78.08%\nEpoch: 852. Loss: 0.2776. Acc.: 77.03%\nEpoch: 853. Loss: 0.2782. Acc.: 77.17%\nEpoch: 854. Loss: 0.2722. Acc.: 77.95%\nEpoch: 855. Loss: 0.2749. Acc.: 77.17%\nEpoch: 856. Loss: 0.2950. Acc.: 77.82%\nEpoch: 857. Loss: 0.2741. Acc.: 78.22%\nEpoch: 858. Loss: 0.2760. Acc.: 77.82%\nEpoch: 859. Loss: 0.2564. Acc.: 77.82%\nEpoch: 860. Loss: 0.2939. Acc.: 78.61%\nEpoch: 861. Loss: 0.2935. Acc.: 78.61%\nEpoch: 862. Loss: 0.2842. Acc.: 77.43%\nEpoch: 863. Loss: 0.2736. Acc.: 77.30%\nEpoch: 864. Loss: 0.2775. Acc.: 78.48%\nEpoch: 865. Loss: 0.2689. Acc.: 78.35%\nEpoch: 866. Loss: 0.2910. Acc.: 77.43%\nEpoch: 867. Loss: 0.2876. Acc.: 78.35%\nEpoch: 868. Loss: 0.2576. Acc.: 79.40%\nEpoch: 869. Loss: 0.2722. Acc.: 77.56%\nEpoch: 870. Loss: 0.2852. Acc.: 77.17%\nEpoch: 871. Loss: 0.2673. Acc.: 78.22%\nEpoch: 872. Loss: 0.2837. Acc.: 77.82%\nEpoch: 873. Loss: 0.2763. Acc.: 78.61%\nEpoch: 874. Loss: 0.2911. Acc.: 77.03%\nEpoch: 875. Loss: 0.2625. Acc.: 78.22%\nEpoch: 876. Loss: 0.2860. Acc.: 78.48%\nEpoch: 877. Loss: 0.2739. Acc.: 77.69%\nEpoch: 878. Loss: 0.2737. Acc.: 78.35%\nEpoch: 879. Loss: 0.2846. Acc.: 79.13%\nEpoch: 880. Loss: 0.2733. Acc.: 79.13%\nEpoch: 881. Loss: 0.2950. Acc.: 77.17%\nEpoch: 882. Loss: 0.2867. Acc.: 77.43%\nEpoch: 883. Loss: 0.2685. Acc.: 78.35%\nEpoch: 884. Loss: 0.2850. Acc.: 78.08%\nEpoch: 885. Loss: 0.2626. Acc.: 77.69%\nEpoch: 886. Loss: 0.2822. Acc.: 77.82%\nEpoch: 887. Loss: 0.2649. Acc.: 77.69%\nEpoch: 888. Loss: 0.2671. Acc.: 78.61%\nEpoch: 889. Loss: 0.2687. Acc.: 77.56%\nEpoch: 890. Loss: 0.2858. Acc.: 77.82%\nEpoch: 891. Loss: 0.2625. Acc.: 79.00%\nEpoch: 892. Loss: 0.2806. Acc.: 79.27%\nEpoch: 893. Loss: 0.2876. Acc.: 78.61%\nEpoch: 894. Loss: 0.2990. Acc.: 78.35%\nEpoch: 895. Loss: 0.2724. Acc.: 78.22%\nEpoch: 896. Loss: 0.2785. Acc.: 78.22%\nEpoch: 897. Loss: 0.2707. Acc.: 78.22%\nEpoch: 898. Loss: 0.2903. Acc.: 77.69%\nEpoch: 899. Loss: 0.2812. Acc.: 78.35%\nEpoch: 900. Loss: 0.2768. Acc.: 76.90%\nEpoch: 901. Loss: 0.2806. Acc.: 77.43%\nEpoch: 902. Loss: 0.2786. Acc.: 78.61%\nEpoch: 903. Loss: 0.3016. Acc.: 77.17%\nEpoch: 904. Loss: 0.2823. Acc.: 76.90%\nEpoch: 905. Loss: 0.2637. Acc.: 77.82%\nEpoch: 906. Loss: 0.2698. Acc.: 77.17%\nEpoch: 907. Loss: 0.2639. Acc.: 76.12%\nEpoch: 908. Loss: 0.2527. Acc.: 77.17%\nEpoch: 909. Loss: 0.2661. Acc.: 76.64%\nEpoch: 910. Loss: 0.2791. Acc.: 77.03%\nEpoch: 911. Loss: 0.2926. Acc.: 78.35%\nEpoch: 912. Loss: 0.2894. Acc.: 78.74%\nEpoch: 913. Loss: 0.2569. Acc.: 78.08%\nEpoch: 914. Loss: 0.2572. Acc.: 77.82%\nEpoch: 915. Loss: 0.2748. Acc.: 78.74%\nEpoch: 916. Loss: 0.2798. Acc.: 78.61%\nEpoch: 917. Loss: 0.2695. Acc.: 77.95%\nEpoch: 918. Loss: 0.2818. Acc.: 77.82%\nEpoch: 919. Loss: 0.2587. Acc.: 77.95%\nEpoch: 920. Loss: 0.2735. Acc.: 77.30%\nEpoch: 921. Loss: 0.2604. Acc.: 77.82%\nEpoch: 922. Loss: 0.2653. Acc.: 77.56%\nEpoch: 923. Loss: 0.2807. Acc.: 77.03%\nEpoch: 924. Loss: 0.2782. Acc.: 78.08%\nEpoch: 925. Loss: 0.2851. Acc.: 77.56%\nEpoch: 926. Loss: 0.2805. Acc.: 77.95%\nEpoch: 927. Loss: 0.2830. Acc.: 77.69%\nEpoch: 928. Loss: 0.2840. Acc.: 78.22%\nEpoch: 929. Loss: 0.2811. Acc.: 79.27%\nEpoch: 930. Loss: 0.2685. Acc.: 78.35%\nEpoch: 931. Loss: 0.2800. Acc.: 77.56%\nEpoch: 932. Loss: 0.2832. Acc.: 78.35%\nEpoch: 933. Loss: 0.2675. Acc.: 77.56%\nEpoch: 934. Loss: 0.2826. Acc.: 78.35%\nEpoch: 935. Loss: 0.2650. Acc.: 77.17%\nEpoch: 936. Loss: 0.2847. Acc.: 77.17%\nEpoch: 937. Loss: 0.2691. Acc.: 78.61%\nEpoch: 938. Loss: 0.2651. Acc.: 77.43%\nEpoch: 939. Loss: 0.2693. Acc.: 77.43%\nEpoch: 940. Loss: 0.2740. Acc.: 79.13%\nEpoch: 941. Loss: 0.2840. Acc.: 79.00%\nEpoch: 942. Loss: 0.2733. Acc.: 76.77%\nEpoch: 943. Loss: 0.2784. Acc.: 77.69%\nEpoch: 944. Loss: 0.2654. Acc.: 78.87%\nEpoch: 945. Loss: 0.2717. Acc.: 79.27%\nEpoch: 946. Loss: 0.2736. Acc.: 76.51%\nEpoch: 947. Loss: 0.2827. Acc.: 77.69%\nEpoch: 948. Loss: 0.2603. Acc.: 78.22%\nEpoch: 949. Loss: 0.2735. Acc.: 78.08%\nEpoch: 950. Loss: 0.2780. Acc.: 79.13%\nEpoch: 951. Loss: 0.2829. Acc.: 79.27%\nEpoch: 952. Loss: 0.2600. Acc.: 77.56%\nEpoch: 953. Loss: 0.2513. Acc.: 79.66%\nEpoch 953 best model saved with accuracy: 79.66%\nEpoch: 954. Loss: 0.2687. Acc.: 78.61%\nEpoch: 955. Loss: 0.2827. Acc.: 79.13%\nEpoch: 956. Loss: 0.2713. Acc.: 78.61%\nEpoch: 957. Loss: 0.2724. Acc.: 77.95%\nEpoch: 958. Loss: 0.2699. Acc.: 78.61%\nEpoch: 959. Loss: 0.2579. Acc.: 78.74%\nEpoch: 960. Loss: 0.2642. Acc.: 79.00%\nEpoch: 961. Loss: 0.2534. Acc.: 78.22%\nEpoch: 962. Loss: 0.2717. Acc.: 78.35%\nEpoch: 963. Loss: 0.2556. Acc.: 78.35%\nEpoch: 964. Loss: 0.2515. Acc.: 77.43%\nEpoch: 965. Loss: 0.2600. Acc.: 78.61%\nEpoch: 966. Loss: 0.2873. Acc.: 77.95%\nEpoch: 967. Loss: 0.2514. Acc.: 77.56%\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 968. Loss: 0.2649. Acc.: 78.35%\nEpoch: 969. Loss: 0.2650. Acc.: 77.95%\nEpoch: 970. Loss: 0.2595. Acc.: 78.61%\nEpoch: 971. Loss: 0.2518. Acc.: 79.13%\nEpoch: 972. Loss: 0.2856. Acc.: 78.22%\nEpoch: 973. Loss: 0.2552. Acc.: 79.13%\nEpoch: 974. Loss: 0.2709. Acc.: 79.53%\nEpoch: 975. Loss: 0.2808. Acc.: 77.03%\nEpoch: 976. Loss: 0.2744. Acc.: 78.61%\nEpoch: 977. Loss: 0.2764. Acc.: 79.40%\nEpoch: 978. Loss: 0.2771. Acc.: 79.79%\nEpoch 978 best model saved with accuracy: 79.79%\nEpoch: 979. Loss: 0.2865. Acc.: 80.31%\nEpoch 979 best model saved with accuracy: 80.31%\nEpoch: 980. Loss: 0.2637. Acc.: 79.00%\nEpoch: 981. Loss: 0.2642. Acc.: 80.58%\nEpoch 981 best model saved with accuracy: 80.58%\nEpoch: 982. Loss: 0.2715. Acc.: 78.35%\nEpoch: 983. Loss: 0.2604. Acc.: 79.13%\nEpoch: 984. Loss: 0.2626. Acc.: 78.87%\nEpoch: 985. Loss: 0.2721. Acc.: 79.13%\nEpoch: 986. Loss: 0.2842. Acc.: 79.13%\nEpoch: 987. Loss: 0.2537. Acc.: 77.56%\nEpoch: 988. Loss: 0.2593. Acc.: 79.27%\nEpoch: 989. Loss: 0.2675. Acc.: 77.69%\nEpoch: 990. Loss: 0.2527. Acc.: 79.66%\nEpoch: 991. Loss: 0.2631. Acc.: 79.13%\nEpoch: 992. Loss: 0.2780. Acc.: 78.48%\nEpoch: 993. Loss: 0.2633. Acc.: 79.27%\nEpoch: 994. Loss: 0.2826. Acc.: 78.61%\nEpoch: 995. Loss: 0.2548. Acc.: 78.48%\nEpoch: 996. Loss: 0.2731. Acc.: 79.92%\nEpoch: 997. Loss: 0.2553. Acc.: 80.18%\nEpoch: 998. Loss: 0.2585. Acc.: 79.27%\nEpoch: 999. Loss: 0.2594. Acc.: 79.00%\nEpoch: 1000. Loss: 0.2542. Acc.: 79.40%\nEpoch: 1001. Loss: 0.2726. Acc.: 79.92%\nEpoch: 1002. Loss: 0.2607. Acc.: 79.66%\nEpoch: 1003. Loss: 0.2606. Acc.: 79.40%\nEpoch: 1004. Loss: 0.2743. Acc.: 76.90%\nEpoch: 1005. Loss: 0.2763. Acc.: 78.48%\nEpoch: 1006. Loss: 0.2627. Acc.: 79.40%\nEpoch: 1007. Loss: 0.2610. Acc.: 79.53%\nEpoch: 1008. Loss: 0.2704. Acc.: 79.40%\nEpoch: 1009. Loss: 0.2749. Acc.: 79.66%\nEpoch: 1010. Loss: 0.2766. Acc.: 79.13%\nEpoch: 1011. Loss: 0.2692. Acc.: 79.66%\nEpoch: 1012. Loss: 0.2576. Acc.: 78.22%\nEpoch: 1013. Loss: 0.2678. Acc.: 78.35%\nEpoch: 1014. Loss: 0.2602. Acc.: 78.61%\nEpoch: 1015. Loss: 0.2680. Acc.: 78.48%\nEpoch: 1016. Loss: 0.2619. Acc.: 78.08%\nEpoch: 1017. Loss: 0.2906. Acc.: 79.92%\nEpoch: 1018. Loss: 0.2469. Acc.: 77.17%\nEpoch: 1019. Loss: 0.2700. Acc.: 78.87%\nEpoch: 1020. Loss: 0.2626. Acc.: 77.82%\nEpoch: 1021. Loss: 0.2628. Acc.: 78.08%\nEpoch: 1022. Loss: 0.2715. Acc.: 79.92%\nEpoch: 1023. Loss: 0.2662. Acc.: 78.35%\nEpoch: 1024. Loss: 0.2469. Acc.: 77.82%\nEpoch: 1024. Loss: 0.2469. Acc.: 77.82%\nEpoch: 1025. Loss: 0.2738. Acc.: 78.74%\nEpoch: 1026. Loss: 0.2520. Acc.: 77.17%\nEpoch: 1027. Loss: 0.2682. Acc.: 77.43%\nEpoch: 1028. Loss: 0.2373. Acc.: 77.43%\nEpoch: 1029. Loss: 0.2491. Acc.: 77.69%\nEpoch: 1030. Loss: 0.2645. Acc.: 78.08%\nEpoch: 1031. Loss: 0.2667. Acc.: 78.35%\nEpoch: 1032. Loss: 0.2833. Acc.: 79.13%\nEpoch: 1033. Loss: 0.2620. Acc.: 76.77%\nEpoch: 1034. Loss: 0.2634. Acc.: 77.95%\nEpoch: 1035. Loss: 0.2587. Acc.: 78.22%\nEpoch: 1036. Loss: 0.2581. Acc.: 78.87%\nEpoch: 1037. Loss: 0.2629. Acc.: 79.00%\nEpoch: 1038. Loss: 0.2634. Acc.: 77.95%\nEpoch: 1039. Loss: 0.2585. Acc.: 77.43%\nEpoch: 1040. Loss: 0.2569. Acc.: 80.71%\nEpoch 1040 best model saved with accuracy: 80.71%\nEpoch: 1041. Loss: 0.2816. Acc.: 79.53%\nEpoch: 1042. Loss: 0.2724. Acc.: 79.66%\nEpoch: 1043. Loss: 0.2791. Acc.: 77.69%\nEpoch: 1044. Loss: 0.2795. Acc.: 79.66%\nEpoch: 1045. Loss: 0.2671. Acc.: 78.48%\nEpoch: 1046. Loss: 0.2473. Acc.: 77.17%\nEpoch: 1047. Loss: 0.2784. Acc.: 79.27%\nEpoch: 1048. Loss: 0.2606. Acc.: 78.87%\nEpoch: 1049. Loss: 0.2515. Acc.: 78.08%\nEpoch: 1050. Loss: 0.2495. Acc.: 77.69%\nEpoch: 1051. Loss: 0.2628. Acc.: 78.74%\nEpoch: 1052. Loss: 0.2558. Acc.: 77.30%\nEpoch: 1053. Loss: 0.2601. Acc.: 78.87%\nEpoch: 1054. Loss: 0.2641. Acc.: 78.48%\nEpoch: 1055. Loss: 0.2568. Acc.: 78.22%\nEpoch: 1056. Loss: 0.2607. Acc.: 78.61%\nEpoch: 1057. Loss: 0.2570. Acc.: 79.66%\nEpoch: 1058. Loss: 0.2635. Acc.: 78.74%\nEpoch: 1059. Loss: 0.2629. Acc.: 79.27%\nEpoch: 1060. Loss: 0.2500. Acc.: 80.71%\nEpoch: 1061. Loss: 0.2575. Acc.: 78.08%\nEpoch: 1062. Loss: 0.2735. Acc.: 79.27%\nEpoch: 1063. Loss: 0.2694. Acc.: 79.27%\nEpoch: 1064. Loss: 0.2686. Acc.: 79.27%\nEpoch: 1065. Loss: 0.2610. Acc.: 78.61%\nEpoch: 1066. Loss: 0.2733. Acc.: 79.00%\nEpoch: 1067. Loss: 0.2625. Acc.: 78.08%\nEpoch: 1068. Loss: 0.2671. Acc.: 78.74%\nEpoch: 1069. Loss: 0.2659. Acc.: 79.00%\nEpoch: 1070. Loss: 0.2487. Acc.: 78.08%\nEpoch: 1071. Loss: 0.2620. Acc.: 78.08%\nEpoch: 1072. Loss: 0.2766. Acc.: 78.35%\nEpoch: 1073. Loss: 0.2493. Acc.: 78.87%\nEpoch: 1074. Loss: 0.2310. Acc.: 78.87%\nEpoch: 1075. Loss: 0.2525. Acc.: 78.61%\nEpoch: 1076. Loss: 0.2544. Acc.: 79.79%\nEpoch: 1077. Loss: 0.2514. Acc.: 79.00%\nEpoch: 1078. Loss: 0.2617. Acc.: 79.00%\nEpoch: 1079. Loss: 0.2550. Acc.: 79.79%\nEpoch: 1080. Loss: 0.2467. Acc.: 79.27%\nEpoch: 1081. Loss: 0.2569. Acc.: 79.00%\nEpoch: 1082. Loss: 0.2631. Acc.: 78.74%\nEpoch: 1083. Loss: 0.2820. Acc.: 79.66%\nEpoch: 1084. Loss: 0.2673. Acc.: 78.35%\nEpoch: 1085. Loss: 0.2601. Acc.: 77.95%\nEpoch: 1086. Loss: 0.2574. Acc.: 78.08%\nEpoch: 1087. Loss: 0.2585. Acc.: 77.95%\nEpoch: 1088. Loss: 0.2789. Acc.: 77.69%\nEpoch: 1089. Loss: 0.2550. Acc.: 77.03%\nEpoch: 1090. Loss: 0.2660. Acc.: 77.69%\nEpoch: 1091. Loss: 0.2640. Acc.: 77.17%\nEpoch: 1092. Loss: 0.2446. Acc.: 78.74%\nEpoch: 1093. Loss: 0.2610. Acc.: 76.51%\nEpoch: 1094. Loss: 0.2555. Acc.: 78.48%\nEpoch: 1095. Loss: 0.2651. Acc.: 78.22%\nEpoch: 1096. Loss: 0.2683. Acc.: 78.61%\nEpoch: 1097. Loss: 0.2596. Acc.: 78.74%\nEpoch: 1098. Loss: 0.2615. Acc.: 79.13%\nEpoch: 1099. Loss: 0.2466. Acc.: 80.18%\nEpoch: 1100. Loss: 0.2551. Acc.: 80.45%\nEpoch: 1101. Loss: 0.2628. Acc.: 78.35%\nEpoch: 1102. Loss: 0.2608. Acc.: 79.00%\nEpoch: 1103. Loss: 0.2667. Acc.: 79.13%\nEpoch: 1104. Loss: 0.2800. Acc.: 78.48%\nEpoch: 1105. Loss: 0.2389. Acc.: 78.61%\nEpoch: 1106. Loss: 0.2485. Acc.: 79.27%\nEpoch: 1107. Loss: 0.2488. Acc.: 79.13%\nEpoch: 1108. Loss: 0.2423. Acc.: 79.92%\nEpoch: 1109. Loss: 0.2580. Acc.: 78.87%\nEpoch: 1110. Loss: 0.2635. Acc.: 78.22%\nEpoch: 1111. Loss: 0.2634. Acc.: 79.00%\nEpoch: 1112. Loss: 0.2625. Acc.: 78.87%\nEpoch: 1113. Loss: 0.2463. Acc.: 78.74%\nEpoch: 1114. Loss: 0.2506. Acc.: 80.84%\nEpoch 1114 best model saved with accuracy: 80.84%\nEpoch: 1115. Loss: 0.2616. Acc.: 78.87%\nEpoch: 1116. Loss: 0.2658. Acc.: 78.74%\nEpoch: 1117. Loss: 0.2510. Acc.: 80.31%\nEpoch: 1118. Loss: 0.2554. Acc.: 78.87%\nEpoch: 1119. Loss: 0.2589. Acc.: 79.27%\nEpoch: 1120. Loss: 0.2704. Acc.: 79.40%\nEpoch: 1121. Loss: 0.2571. Acc.: 80.05%\nEpoch: 1122. Loss: 0.2590. Acc.: 79.92%\nEpoch: 1123. Loss: 0.2640. Acc.: 79.27%\nEpoch: 1124. Loss: 0.2579. Acc.: 80.18%\nEpoch: 1125. Loss: 0.2399. Acc.: 80.45%\nEpoch: 1126. Loss: 0.2381. Acc.: 80.05%\nEpoch: 1127. Loss: 0.2565. Acc.: 79.79%\nEpoch: 1128. Loss: 0.2609. Acc.: 79.53%\nEpoch: 1129. Loss: 0.2344. Acc.: 79.27%\nEpoch: 1130. Loss: 0.2596. Acc.: 77.43%\nEpoch: 1131. Loss: 0.3031. Acc.: 79.92%\nEpoch: 1132. Loss: 0.2842. Acc.: 78.74%\nEpoch: 1133. Loss: 0.2297. Acc.: 79.27%\nEpoch: 1134. Loss: 0.2573. Acc.: 78.87%\nEpoch: 1135. Loss: 0.2423. Acc.: 79.13%\nEpoch: 1136. Loss: 0.2467. Acc.: 79.13%\nEpoch: 1137. Loss: 0.2644. Acc.: 80.58%\nEpoch: 1138. Loss: 0.2532. Acc.: 80.05%\nEpoch: 1139. Loss: 0.2761. Acc.: 78.48%\nEpoch: 1140. Loss: 0.2588. Acc.: 79.27%\nEpoch: 1141. Loss: 0.2665. Acc.: 79.13%\nEpoch: 1142. Loss: 0.2877. Acc.: 80.45%\nEpoch: 1143. Loss: 0.2481. Acc.: 79.66%\nEpoch: 1144. Loss: 0.2531. Acc.: 79.53%\nEpoch: 1145. Loss: 0.2600. Acc.: 80.05%\nEpoch: 1146. Loss: 0.2606. Acc.: 78.61%\nEpoch: 1147. Loss: 0.2570. Acc.: 79.27%\nEpoch: 1148. Loss: 0.2408. Acc.: 79.40%\nEpoch: 1149. Loss: 0.2414. Acc.: 78.61%\nEpoch: 1150. Loss: 0.2627. Acc.: 80.84%\nEpoch: 1151. Loss: 0.2498. Acc.: 79.00%\nEpoch: 1152. Loss: 0.2588. Acc.: 78.35%\nEpoch: 1153. Loss: 0.2557. Acc.: 78.61%\nEpoch: 1154. Loss: 0.2634. Acc.: 79.92%\nEpoch: 1155. Loss: 0.2667. Acc.: 79.53%\nEpoch: 1156. Loss: 0.2540. Acc.: 79.27%\nEpoch: 1157. Loss: 0.2411. Acc.: 79.27%\nEpoch: 1158. Loss: 0.2420. Acc.: 79.53%\nEpoch: 1159. Loss: 0.2644. Acc.: 78.74%\nEpoch: 1160. Loss: 0.2435. Acc.: 78.74%\nEpoch: 1161. Loss: 0.2525. Acc.: 78.61%\nEpoch: 1162. Loss: 0.2525. Acc.: 78.48%\nEpoch: 1163. Loss: 0.2438. Acc.: 77.82%\nEpoch: 1164. Loss: 0.2324. Acc.: 78.35%\nEpoch: 1165. Loss: 0.2626. Acc.: 79.53%\nEpoch: 1166. Loss: 0.2449. Acc.: 78.87%\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 1167. Loss: 0.2460. Acc.: 78.61%\nEpoch: 1168. Loss: 0.2336. Acc.: 79.53%\nEpoch: 1169. Loss: 0.2545. Acc.: 79.92%\nEpoch: 1170. Loss: 0.2558. Acc.: 79.27%\nEpoch: 1171. Loss: 0.2699. Acc.: 78.35%\nEpoch: 1172. Loss: 0.2521. Acc.: 80.05%\nEpoch: 1173. Loss: 0.2510. Acc.: 78.48%\nEpoch: 1174. Loss: 0.2627. Acc.: 78.87%\nEpoch: 1175. Loss: 0.2566. Acc.: 78.22%\nEpoch: 1176. Loss: 0.2605. Acc.: 78.48%\nEpoch: 1177. Loss: 0.2709. Acc.: 79.00%\nEpoch: 1178. Loss: 0.2711. Acc.: 79.66%\nEpoch: 1179. Loss: 0.2355. Acc.: 79.40%\nEpoch: 1180. Loss: 0.2473. Acc.: 79.92%\nEpoch: 1181. Loss: 0.2555. Acc.: 79.27%\nEpoch: 1182. Loss: 0.2706. Acc.: 77.82%\nEpoch: 1183. Loss: 0.2495. Acc.: 79.53%\nEpoch: 1184. Loss: 0.2654. Acc.: 80.71%\nEpoch: 1185. Loss: 0.2619. Acc.: 80.58%\nEpoch: 1186. Loss: 0.2521. Acc.: 79.40%\nEpoch: 1187. Loss: 0.2356. Acc.: 79.92%\nEpoch: 1188. Loss: 0.2346. Acc.: 79.40%\nEpoch: 1189. Loss: 0.2475. Acc.: 79.79%\nEpoch: 1190. Loss: 0.2208. Acc.: 78.87%\nEpoch: 1191. Loss: 0.2510. Acc.: 79.40%\nEpoch: 1192. Loss: 0.2604. Acc.: 79.66%\nEpoch: 1193. Loss: 0.2697. Acc.: 78.87%\nEpoch: 1194. Loss: 0.2771. Acc.: 80.18%\nEpoch: 1195. Loss: 0.2493. Acc.: 80.31%\nEpoch: 1196. Loss: 0.2367. Acc.: 79.00%\nEpoch: 1197. Loss: 0.2427. Acc.: 79.40%\nEpoch: 1198. Loss: 0.2565. Acc.: 80.45%\nEpoch: 1199. Loss: 0.2742. Acc.: 78.35%\nEpoch: 1200. Loss: 0.2800. Acc.: 77.82%\nEpoch: 1201. Loss: 0.2464. Acc.: 79.27%\nEpoch: 1202. Loss: 0.2400. Acc.: 78.35%\nEpoch: 1203. Loss: 0.2430. Acc.: 80.31%\nEpoch: 1204. Loss: 0.2558. Acc.: 79.27%\nEpoch: 1205. Loss: 0.2558. Acc.: 77.69%\nEpoch: 1206. Loss: 0.2553. Acc.: 79.53%\nEpoch: 1207. Loss: 0.2567. Acc.: 79.53%\nEpoch: 1208. Loss: 0.2442. Acc.: 78.48%\nEpoch: 1209. Loss: 0.2274. Acc.: 78.48%\nEpoch: 1210. Loss: 0.2540. Acc.: 78.08%\nEpoch: 1211. Loss: 0.2585. Acc.: 78.87%\nEpoch: 1212. Loss: 0.2436. Acc.: 79.40%\nEpoch: 1213. Loss: 0.2404. Acc.: 78.22%\nEpoch: 1214. Loss: 0.2442. Acc.: 79.53%\nEpoch: 1215. Loss: 0.2578. Acc.: 79.27%\nEpoch: 1216. Loss: 0.2708. Acc.: 80.05%\nEpoch: 1217. Loss: 0.2510. Acc.: 79.00%\nEpoch: 1218. Loss: 0.2654. Acc.: 77.56%\nEpoch: 1219. Loss: 0.2487. Acc.: 78.35%\nEpoch: 1220. Loss: 0.2477. Acc.: 78.48%\nEpoch: 1221. Loss: 0.2598. Acc.: 77.69%\nEpoch: 1222. Loss: 0.2496. Acc.: 78.35%\nEpoch: 1223. Loss: 0.2646. Acc.: 78.22%\nEpoch: 1224. Loss: 0.2521. Acc.: 77.56%\nEpoch: 1225. Loss: 0.2487. Acc.: 78.35%\nEpoch: 1226. Loss: 0.2613. Acc.: 78.48%\nEpoch: 1227. Loss: 0.2523. Acc.: 77.95%\nEpoch: 1228. Loss: 0.2189. Acc.: 77.56%\nEpoch: 1229. Loss: 0.2493. Acc.: 77.17%\nEpoch: 1230. Loss: 0.2634. Acc.: 77.43%\nEpoch: 1231. Loss: 0.2409. Acc.: 76.90%\nEpoch: 1232. Loss: 0.2419. Acc.: 78.35%\nEpoch: 1233. Loss: 0.2509. Acc.: 77.56%\nEpoch: 1234. Loss: 0.2281. Acc.: 78.22%\nEpoch: 1235. Loss: 0.2587. Acc.: 78.35%\nEpoch: 1236. Loss: 0.2605. Acc.: 78.22%\nEpoch: 1237. Loss: 0.2478. Acc.: 79.00%\nEpoch: 1238. Loss: 0.2485. Acc.: 79.40%\nEpoch: 1239. Loss: 0.2608. Acc.: 78.22%\nEpoch: 1240. Loss: 0.2574. Acc.: 78.35%\nEpoch: 1241. Loss: 0.2543. Acc.: 79.00%\nEpoch: 1242. Loss: 0.2419. Acc.: 78.87%\nEpoch: 1243. Loss: 0.2545. Acc.: 78.74%\nEpoch: 1244. Loss: 0.2633. Acc.: 78.87%\nEpoch: 1245. Loss: 0.2476. Acc.: 77.82%\nEpoch: 1246. Loss: 0.2661. Acc.: 78.22%\nEpoch: 1247. Loss: 0.2481. Acc.: 80.31%\nEpoch: 1248. Loss: 0.2463. Acc.: 80.31%\nEpoch: 1249. Loss: 0.2519. Acc.: 80.45%\nEpoch: 1250. Loss: 0.2433. Acc.: 78.48%\nEpoch: 1251. Loss: 0.2499. Acc.: 78.61%\nEpoch: 1252. Loss: 0.2457. Acc.: 78.61%\nEpoch: 1253. Loss: 0.2582. Acc.: 79.40%\nEpoch: 1254. Loss: 0.2348. Acc.: 78.08%\nEpoch: 1255. Loss: 0.2432. Acc.: 79.66%\nEpoch: 1256. Loss: 0.2662. Acc.: 80.45%\nEpoch: 1257. Loss: 0.2348. Acc.: 79.79%\nEpoch: 1258. Loss: 0.2407. Acc.: 79.79%\nEpoch: 1259. Loss: 0.2352. Acc.: 79.79%\nEpoch: 1260. Loss: 0.2428. Acc.: 79.00%\nEpoch: 1261. Loss: 0.2271. Acc.: 79.40%\nEpoch: 1262. Loss: 0.2319. Acc.: 78.87%\nEpoch: 1263. Loss: 0.2392. Acc.: 79.66%\nEpoch: 1264. Loss: 0.2506. Acc.: 79.00%\nEpoch: 1265. Loss: 0.2684. Acc.: 78.35%\nEpoch: 1266. Loss: 0.2449. Acc.: 79.40%\nEpoch: 1267. Loss: 0.2378. Acc.: 79.66%\nEpoch: 1268. Loss: 0.2530. Acc.: 79.00%\nEpoch: 1269. Loss: 0.2639. Acc.: 79.66%\nEpoch: 1270. Loss: 0.2533. Acc.: 80.05%\nEpoch: 1271. Loss: 0.2512. Acc.: 77.95%\nEpoch: 1272. Loss: 0.2741. Acc.: 79.79%\nEpoch: 1273. Loss: 0.2436. Acc.: 79.66%\nEpoch: 1274. Loss: 0.2380. Acc.: 78.74%\nEpoch: 1275. Loss: 0.2373. Acc.: 78.74%\nEpoch: 1276. Loss: 0.2479. Acc.: 79.40%\nEpoch: 1277. Loss: 0.2542. Acc.: 78.87%\nEpoch: 1278. Loss: 0.2532. Acc.: 79.40%\nEpoch: 1279. Loss: 0.2348. Acc.: 78.74%\nEpoch: 1280. Loss: 0.2565. Acc.: 78.61%\nEpoch: 1281. Loss: 0.2520. Acc.: 79.53%\nEpoch: 1282. Loss: 0.2527. Acc.: 79.66%\nEpoch: 1283. Loss: 0.2535. Acc.: 78.87%\nEpoch: 1284. Loss: 0.2545. Acc.: 79.92%\nEpoch: 1285. Loss: 0.2629. Acc.: 79.00%\nEpoch: 1286. Loss: 0.2580. Acc.: 79.13%\nEpoch: 1287. Loss: 0.2533. Acc.: 79.27%\nEpoch: 1288. Loss: 0.2519. Acc.: 78.74%\nEpoch: 1289. Loss: 0.2439. Acc.: 79.79%\nEpoch: 1290. Loss: 0.2472. Acc.: 79.53%\nEpoch: 1291. Loss: 0.2400. Acc.: 78.87%\nEpoch: 1292. Loss: 0.2458. Acc.: 79.27%\nEpoch: 1293. Loss: 0.2405. Acc.: 80.45%\nEpoch: 1294. Loss: 0.2361. Acc.: 79.00%\nEpoch: 1295. Loss: 0.2337. Acc.: 79.92%\nEpoch: 1296. Loss: 0.2296. Acc.: 79.27%\nEpoch: 1297. Loss: 0.2521. Acc.: 79.66%\nEpoch: 1298. Loss: 0.2344. Acc.: 80.05%\nEpoch: 1299. Loss: 0.2475. Acc.: 78.74%\nEpoch: 1300. Loss: 0.2419. Acc.: 79.66%\nEpoch: 1301. Loss: 0.2390. Acc.: 80.45%\nEpoch: 1302. Loss: 0.2247. Acc.: 80.31%\nEpoch: 1303. Loss: 0.2511. Acc.: 79.40%\nEpoch: 1304. Loss: 0.2546. Acc.: 79.79%\nEpoch: 1305. Loss: 0.2423. Acc.: 79.79%\nEpoch: 1306. Loss: 0.2629. Acc.: 79.27%\nEpoch: 1307. Loss: 0.2706. Acc.: 79.40%\nEpoch: 1308. Loss: 0.2351. Acc.: 80.97%\nEpoch 1308 best model saved with accuracy: 80.97%\nEpoch: 1309. Loss: 0.2488. Acc.: 80.05%\nEpoch: 1310. Loss: 0.2427. Acc.: 80.05%\nEpoch: 1311. Loss: 0.2622. Acc.: 79.92%\nEpoch: 1312. Loss: 0.2348. Acc.: 79.40%\nEpoch: 1313. Loss: 0.2451. Acc.: 80.84%\nEpoch: 1314. Loss: 0.2425. Acc.: 79.66%\nEpoch: 1315. Loss: 0.2450. Acc.: 79.66%\nEpoch: 1316. Loss: 0.2390. Acc.: 78.35%\nEpoch: 1317. Loss: 0.2618. Acc.: 79.40%\nEpoch: 1318. Loss: 0.2540. Acc.: 79.13%\nEpoch: 1319. Loss: 0.2533. Acc.: 78.87%\nEpoch: 1320. Loss: 0.2266. Acc.: 79.13%\nEpoch: 1321. Loss: 0.2370. Acc.: 79.13%\nEpoch: 1322. Loss: 0.2502. Acc.: 79.00%\nEpoch: 1323. Loss: 0.2613. Acc.: 78.74%\nEpoch: 1324. Loss: 0.2395. Acc.: 78.48%\nEpoch: 1325. Loss: 0.2429. Acc.: 79.00%\nEpoch: 1326. Loss: 0.2458. Acc.: 79.40%\nEpoch: 1327. Loss: 0.2450. Acc.: 78.87%\nEpoch: 1328. Loss: 0.2483. Acc.: 79.27%\nEpoch: 1329. Loss: 0.2485. Acc.: 80.05%\nEpoch: 1330. Loss: 0.2417. Acc.: 79.40%\nEpoch: 1331. Loss: 0.2703. Acc.: 79.40%\nEpoch: 1332. Loss: 0.2754. Acc.: 79.53%\nEpoch: 1333. Loss: 0.2427. Acc.: 78.35%\nEpoch: 1334. Loss: 0.2489. Acc.: 79.00%\nEpoch: 1335. Loss: 0.2509. Acc.: 79.92%\nEpoch: 1336. Loss: 0.2479. Acc.: 78.48%\nEpoch: 1337. Loss: 0.2354. Acc.: 79.40%\nEpoch: 1338. Loss: 0.2341. Acc.: 80.05%\nEpoch: 1339. Loss: 0.2480. Acc.: 79.53%\nEpoch: 1340. Loss: 0.2414. Acc.: 77.95%\nEpoch: 1341. Loss: 0.2798. Acc.: 79.27%\nEpoch: 1342. Loss: 0.2517. Acc.: 80.05%\nEpoch: 1343. Loss: 0.2441. Acc.: 79.92%\nEpoch: 1344. Loss: 0.2432. Acc.: 78.48%\nEpoch: 1345. Loss: 0.2438. Acc.: 77.82%\nEpoch: 1346. Loss: 0.2399. Acc.: 79.00%\nEpoch: 1347. Loss: 0.2628. Acc.: 79.00%\nEpoch: 1348. Loss: 0.2458. Acc.: 79.92%\nEpoch: 1349. Loss: 0.2524. Acc.: 79.79%\nEpoch: 1350. Loss: 0.2337. Acc.: 79.66%\nEpoch: 1351. Loss: 0.2397. Acc.: 80.31%\nEpoch: 1352. Loss: 0.2400. Acc.: 79.92%\nEpoch: 1353. Loss: 0.2340. Acc.: 79.79%\nEpoch: 1354. Loss: 0.2419. Acc.: 80.18%\nEpoch: 1355. Loss: 0.2452. Acc.: 79.79%\nEpoch: 1356. Loss: 0.2421. Acc.: 79.27%\nEpoch: 1357. Loss: 0.2427. Acc.: 80.05%\nEpoch: 1358. Loss: 0.2501. Acc.: 80.58%\nEpoch: 1359. Loss: 0.2386. Acc.: 79.79%\nEpoch: 1360. Loss: 0.2459. Acc.: 78.48%\nEpoch: 1361. Loss: 0.2323. Acc.: 78.48%\nEpoch: 1362. Loss: 0.2496. Acc.: 80.18%\nEpoch: 1363. Loss: 0.2533. Acc.: 81.36%\nEpoch 1363 best model saved with accuracy: 81.36%\nEpoch: 1364. Loss: 0.2427. Acc.: 81.23%\nEpoch: 1365. Loss: 0.2495. Acc.: 79.66%\nEpoch: 1366. Loss: 0.2357. Acc.: 79.92%\nEpoch: 1367. Loss: 0.2532. Acc.: 79.92%\nEpoch: 1368. Loss: 0.2313. Acc.: 79.00%\nEpoch: 1369. Loss: 0.2372. Acc.: 79.53%\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 1370. Loss: 0.2424. Acc.: 78.61%\nEpoch: 1371. Loss: 0.2519. Acc.: 80.18%\nEpoch: 1372. Loss: 0.2591. Acc.: 79.40%\nEpoch: 1373. Loss: 0.2537. Acc.: 77.56%\nEpoch: 1374. Loss: 0.2362. Acc.: 76.77%\nEpoch: 1375. Loss: 0.2232. Acc.: 79.40%\nEpoch: 1376. Loss: 0.2491. Acc.: 78.48%\nEpoch: 1377. Loss: 0.2396. Acc.: 77.82%\nEpoch: 1378. Loss: 0.2356. Acc.: 79.00%\nEpoch: 1379. Loss: 0.2427. Acc.: 79.66%\nEpoch: 1380. Loss: 0.2508. Acc.: 78.22%\nEpoch: 1381. Loss: 0.2378. Acc.: 78.87%\nEpoch: 1382. Loss: 0.2460. Acc.: 78.22%\nEpoch: 1383. Loss: 0.2513. Acc.: 79.40%\nEpoch: 1384. Loss: 0.2340. Acc.: 78.74%\nEpoch: 1385. Loss: 0.2315. Acc.: 79.66%\nEpoch: 1386. Loss: 0.2435. Acc.: 78.61%\nEpoch: 1387. Loss: 0.2501. Acc.: 78.87%\nEpoch: 1388. Loss: 0.2424. Acc.: 79.79%\nEpoch: 1389. Loss: 0.2459. Acc.: 80.05%\nEpoch: 1390. Loss: 0.2565. Acc.: 78.35%\nEpoch: 1391. Loss: 0.2352. Acc.: 78.35%\nEpoch: 1392. Loss: 0.2505. Acc.: 79.40%\nEpoch: 1393. Loss: 0.2206. Acc.: 79.27%\nEpoch: 1394. Loss: 0.2299. Acc.: 79.40%\nEpoch: 1395. Loss: 0.2261. Acc.: 78.48%\nEpoch: 1396. Loss: 0.2375. Acc.: 78.87%\nEpoch: 1397. Loss: 0.2415. Acc.: 80.05%\nEpoch: 1398. Loss: 0.2399. Acc.: 79.13%\nEpoch: 1399. Loss: 0.2490. Acc.: 79.40%\nEpoch: 1400. Loss: 0.2466. Acc.: 79.27%\nEpoch: 1401. Loss: 0.2247. Acc.: 79.66%\nEpoch: 1402. Loss: 0.2471. Acc.: 78.61%\nEpoch: 1403. Loss: 0.2661. Acc.: 80.05%\nEpoch: 1404. Loss: 0.2450. Acc.: 78.87%\nEpoch: 1405. Loss: 0.2548. Acc.: 80.05%\nEpoch: 1406. Loss: 0.2317. Acc.: 79.00%\nEpoch: 1407. Loss: 0.2452. Acc.: 79.27%\nEpoch: 1408. Loss: 0.2367. Acc.: 80.31%\nEpoch: 1409. Loss: 0.2390. Acc.: 79.13%\nEpoch: 1410. Loss: 0.2430. Acc.: 79.40%\nEpoch: 1411. Loss: 0.2380. Acc.: 78.74%\nEpoch: 1412. Loss: 0.2431. Acc.: 78.22%\nEpoch: 1413. Loss: 0.2400. Acc.: 79.40%\nEpoch: 1414. Loss: 0.2447. Acc.: 79.13%\nEpoch: 1415. Loss: 0.2559. Acc.: 78.61%\nEpoch: 1416. Loss: 0.2282. Acc.: 79.53%\nEpoch: 1417. Loss: 0.2465. Acc.: 80.05%\nEpoch: 1418. Loss: 0.2645. Acc.: 79.13%\nEpoch: 1419. Loss: 0.2347. Acc.: 79.66%\nEpoch: 1420. Loss: 0.2538. Acc.: 79.66%\nEpoch: 1421. Loss: 0.2568. Acc.: 79.92%\nEpoch: 1422. Loss: 0.2519. Acc.: 79.79%\nEpoch: 1423. Loss: 0.2299. Acc.: 78.61%\nEpoch: 1424. Loss: 0.2512. Acc.: 80.31%\nEpoch: 1425. Loss: 0.2576. Acc.: 79.66%\nEpoch: 1426. Loss: 0.2362. Acc.: 80.58%\nEpoch: 1427. Loss: 0.2348. Acc.: 79.40%\nEpoch: 1428. Loss: 0.2323. Acc.: 78.74%\nEpoch: 1429. Loss: 0.2427. Acc.: 79.53%\nEpoch: 1430. Loss: 0.2532. Acc.: 79.79%\nEpoch: 1431. Loss: 0.2589. Acc.: 80.45%\nEpoch: 1432. Loss: 0.2505. Acc.: 79.00%\nEpoch: 1433. Loss: 0.2508. Acc.: 81.23%\nEpoch: 1434. Loss: 0.2592. Acc.: 80.84%\nEpoch: 1435. Loss: 0.2362. Acc.: 79.79%\nEpoch: 1436. Loss: 0.2304. Acc.: 79.13%\nEpoch: 1437. Loss: 0.2485. Acc.: 79.27%\nEpoch: 1438. Loss: 0.2395. Acc.: 80.84%\nEpoch: 1439. Loss: 0.2435. Acc.: 81.36%\nEpoch: 1440. Loss: 0.2422. Acc.: 81.23%\nEpoch: 1441. Loss: 0.2430. Acc.: 81.50%\nEpoch 1441 best model saved with accuracy: 81.50%\nEpoch: 1442. Loss: 0.2366. Acc.: 81.10%\nEpoch: 1443. Loss: 0.2629. Acc.: 81.36%\nEpoch: 1444. Loss: 0.2319. Acc.: 80.45%\nEpoch: 1445. Loss: 0.2578. Acc.: 80.58%\nEpoch: 1446. Loss: 0.2561. Acc.: 81.36%\nEpoch: 1447. Loss: 0.2416. Acc.: 80.97%\nEpoch: 1448. Loss: 0.2292. Acc.: 79.79%\nEpoch: 1449. Loss: 0.2341. Acc.: 80.31%\nEpoch: 1450. Loss: 0.2525. Acc.: 80.18%\nEpoch: 1451. Loss: 0.2360. Acc.: 80.97%\nEpoch: 1452. Loss: 0.2472. Acc.: 80.31%\nEpoch: 1453. Loss: 0.2451. Acc.: 81.63%\nEpoch 1453 best model saved with accuracy: 81.63%\nEpoch: 1454. Loss: 0.2268. Acc.: 81.36%\nEpoch: 1455. Loss: 0.2287. Acc.: 79.92%\nEpoch: 1456. Loss: 0.2522. Acc.: 79.66%\nEpoch: 1457. Loss: 0.2404. Acc.: 80.31%\nEpoch: 1458. Loss: 0.2424. Acc.: 80.45%\nEpoch: 1459. Loss: 0.2676. Acc.: 80.45%\nEpoch: 1460. Loss: 0.2512. Acc.: 79.92%\nEpoch: 1461. Loss: 0.2314. Acc.: 80.18%\nEpoch: 1462. Loss: 0.2544. Acc.: 80.18%\nEpoch: 1463. Loss: 0.2326. Acc.: 81.76%\nEpoch 1463 best model saved with accuracy: 81.76%\nEpoch: 1464. Loss: 0.2260. Acc.: 80.71%\nEpoch: 1465. Loss: 0.2461. Acc.: 79.66%\nEpoch: 1466. Loss: 0.2483. Acc.: 79.92%\nEpoch: 1467. Loss: 0.2580. Acc.: 78.35%\nEpoch: 1468. Loss: 0.2341. Acc.: 80.45%\nEpoch: 1469. Loss: 0.2322. Acc.: 79.66%\nEpoch: 1470. Loss: 0.2399. Acc.: 79.53%\nEpoch: 1471. Loss: 0.2376. Acc.: 80.18%\nEpoch: 1472. Loss: 0.2280. Acc.: 79.53%\nEpoch: 1473. Loss: 0.2393. Acc.: 79.40%\nEpoch: 1474. Loss: 0.2484. Acc.: 79.79%\nEpoch: 1475. Loss: 0.2584. Acc.: 80.71%\nEpoch: 1476. Loss: 0.2297. Acc.: 80.05%\nEpoch: 1477. Loss: 0.2581. Acc.: 79.13%\nEpoch: 1478. Loss: 0.2463. Acc.: 79.53%\nEpoch: 1479. Loss: 0.2272. Acc.: 80.71%\nEpoch: 1480. Loss: 0.2261. Acc.: 80.45%\nEpoch: 1481. Loss: 0.2373. Acc.: 79.40%\nEpoch: 1482. Loss: 0.2416. Acc.: 80.58%\nEpoch: 1483. Loss: 0.2375. Acc.: 79.40%\nEpoch: 1484. Loss: 0.2258. Acc.: 79.66%\nEpoch: 1485. Loss: 0.2427. Acc.: 79.92%\nEpoch: 1486. Loss: 0.2386. Acc.: 79.53%\nEpoch: 1487. Loss: 0.2515. Acc.: 79.00%\nEpoch: 1488. Loss: 0.2231. Acc.: 78.61%\nEpoch: 1489. Loss: 0.2504. Acc.: 79.00%\nEpoch: 1490. Loss: 0.2432. Acc.: 79.53%\nEpoch: 1491. Loss: 0.2396. Acc.: 79.00%\nEpoch: 1492. Loss: 0.2369. Acc.: 79.27%\nEpoch: 1493. Loss: 0.2373. Acc.: 79.00%\nEpoch: 1494. Loss: 0.2499. Acc.: 79.66%\nEpoch: 1495. Loss: 0.2379. Acc.: 80.71%\nEpoch: 1496. Loss: 0.2181. Acc.: 81.10%\nEpoch: 1497. Loss: 0.2424. Acc.: 81.10%\nEpoch: 1498. Loss: 0.2427. Acc.: 81.23%\nEpoch: 1499. Loss: 0.2408. Acc.: 79.53%\nEpoch: 1500. Loss: 0.2365. Acc.: 80.18%\nEpoch: 1501. Loss: 0.2445. Acc.: 80.05%\nEpoch: 1502. Loss: 0.2355. Acc.: 80.18%\nEpoch: 1503. Loss: 0.2416. Acc.: 80.71%\nEpoch: 1504. Loss: 0.2318. Acc.: 79.66%\nEpoch: 1505. Loss: 0.2476. Acc.: 80.05%\nEpoch: 1506. Loss: 0.2633. Acc.: 78.87%\nEpoch: 1507. Loss: 0.2272. Acc.: 79.92%\nEpoch: 1508. Loss: 0.2440. Acc.: 81.36%\nEpoch: 1509. Loss: 0.2204. Acc.: 79.66%\nEpoch: 1510. Loss: 0.2355. Acc.: 79.40%\nEpoch: 1511. Loss: 0.2327. Acc.: 80.58%\nEpoch: 1512. Loss: 0.2444. Acc.: 78.61%\nEpoch: 1513. Loss: 0.2227. Acc.: 79.27%\nEpoch: 1514. Loss: 0.2739. Acc.: 79.79%\nEpoch: 1515. Loss: 0.2270. Acc.: 78.61%\nEpoch: 1516. Loss: 0.2329. Acc.: 79.40%\nEpoch: 1517. Loss: 0.2387. Acc.: 79.92%\nEpoch: 1518. Loss: 0.2325. Acc.: 79.13%\nEpoch: 1519. Loss: 0.2296. Acc.: 79.53%\nEpoch: 1520. Loss: 0.2302. Acc.: 79.53%\nEpoch: 1521. Loss: 0.2397. Acc.: 79.27%\nEpoch: 1522. Loss: 0.2121. Acc.: 78.61%\nEpoch: 1523. Loss: 0.2282. Acc.: 79.92%\nEpoch: 1524. Loss: 0.2510. Acc.: 79.92%\nEpoch: 1525. Loss: 0.2458. Acc.: 80.31%\nEpoch: 1526. Loss: 0.2126. Acc.: 81.76%\nEpoch: 1527. Loss: 0.2318. Acc.: 80.84%\nEpoch: 1528. Loss: 0.2412. Acc.: 80.31%\nEpoch: 1529. Loss: 0.2355. Acc.: 80.18%\nEpoch: 1530. Loss: 0.2272. Acc.: 80.05%\nEpoch: 1531. Loss: 0.2324. Acc.: 80.31%\nEpoch: 1532. Loss: 0.2779. Acc.: 80.58%\nEpoch: 1533. Loss: 0.2399. Acc.: 80.58%\nEpoch: 1534. Loss: 0.2305. Acc.: 80.05%\nEpoch: 1535. Loss: 0.2654. Acc.: 80.45%\nEpoch: 1536. Loss: 0.2543. Acc.: 80.31%\nEpoch: 1537. Loss: 0.2569. Acc.: 80.05%\nEpoch: 1538. Loss: 0.2385. Acc.: 78.74%\nEpoch: 1539. Loss: 0.2411. Acc.: 79.40%\nEpoch: 1540. Loss: 0.2310. Acc.: 78.74%\nEpoch: 1541. Loss: 0.2454. Acc.: 79.27%\nEpoch: 1542. Loss: 0.2486. Acc.: 80.18%\nEpoch: 1543. Loss: 0.2426. Acc.: 80.18%\nEpoch: 1544. Loss: 0.2304. Acc.: 78.87%\nEpoch: 1545. Loss: 0.2370. Acc.: 78.35%\nEpoch: 1546. Loss: 0.2487. Acc.: 80.18%\nEpoch: 1547. Loss: 0.2458. Acc.: 80.18%\nEpoch: 1548. Loss: 0.2433. Acc.: 80.05%\nEpoch: 1549. Loss: 0.2232. Acc.: 79.92%\nEpoch: 1550. Loss: 0.2490. Acc.: 79.53%\nEpoch: 1551. Loss: 0.2308. Acc.: 79.66%\nEpoch: 1552. Loss: 0.2259. Acc.: 80.05%\nEpoch: 1553. Loss: 0.2466. Acc.: 80.45%\nEpoch: 1554. Loss: 0.2234. Acc.: 79.53%\nEpoch: 1555. Loss: 0.2320. Acc.: 81.36%\nEpoch: 1556. Loss: 0.2298. Acc.: 79.66%\nEpoch: 1557. Loss: 0.2425. Acc.: 78.22%\nEpoch: 1558. Loss: 0.2416. Acc.: 79.92%\nEpoch: 1559. Loss: 0.2341. Acc.: 79.79%\nEpoch: 1560. Loss: 0.2318. Acc.: 79.92%\nEpoch: 1561. Loss: 0.2438. Acc.: 79.66%\nEpoch: 1562. Loss: 0.2367. Acc.: 79.40%\nEpoch: 1563. Loss: 0.2486. Acc.: 81.10%\nEpoch: 1564. Loss: 0.2431. Acc.: 80.05%\nEpoch: 1565. Loss: 0.2337. Acc.: 79.66%\nEpoch: 1566. Loss: 0.2228. Acc.: 80.31%\nEpoch: 1567. Loss: 0.2300. Acc.: 80.58%\nEpoch: 1568. Loss: 0.2363. Acc.: 80.18%\nEpoch: 1569. Loss: 0.2394. Acc.: 81.23%\nEpoch: 1570. Loss: 0.2283. Acc.: 80.84%\nEpoch: 1571. Loss: 0.2449. Acc.: 80.18%\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 1572. Loss: 0.2376. Acc.: 80.71%\nEpoch: 1573. Loss: 0.2272. Acc.: 81.10%\nEpoch: 1574. Loss: 0.2288. Acc.: 81.89%\nEpoch 1574 best model saved with accuracy: 81.89%\nEpoch: 1575. Loss: 0.2249. Acc.: 80.31%\nEpoch: 1576. Loss: 0.2619. Acc.: 81.50%\nEpoch: 1577. Loss: 0.2294. Acc.: 81.63%\nEpoch: 1578. Loss: 0.2214. Acc.: 81.50%\nEpoch: 1579. Loss: 0.2248. Acc.: 82.68%\nEpoch 1579 best model saved with accuracy: 82.68%\nEpoch: 1580. Loss: 0.2221. Acc.: 81.89%\nEpoch: 1581. Loss: 0.2411. Acc.: 80.84%\nEpoch: 1582. Loss: 0.2490. Acc.: 80.18%\nEpoch: 1583. Loss: 0.2512. Acc.: 80.84%\nEpoch: 1584. Loss: 0.2490. Acc.: 79.92%\nEpoch: 1585. Loss: 0.2415. Acc.: 81.23%\nEpoch: 1586. Loss: 0.2407. Acc.: 80.84%\nEpoch: 1587. Loss: 0.2347. Acc.: 80.05%\nEpoch: 1588. Loss: 0.2375. Acc.: 80.84%\nEpoch: 1589. Loss: 0.2306. Acc.: 81.50%\nEpoch: 1590. Loss: 0.2425. Acc.: 81.10%\nEpoch: 1591. Loss: 0.2567. Acc.: 80.45%\nEpoch: 1592. Loss: 0.2423. Acc.: 80.97%\nEpoch: 1593. Loss: 0.2284. Acc.: 81.50%\nEpoch: 1594. Loss: 0.2267. Acc.: 80.18%\nEpoch: 1595. Loss: 0.2430. Acc.: 80.58%\nEpoch: 1596. Loss: 0.2615. Acc.: 80.84%\nEpoch: 1597. Loss: 0.2572. Acc.: 80.58%\nEpoch: 1598. Loss: 0.2318. Acc.: 80.05%\nEpoch: 1599. Loss: 0.2369. Acc.: 79.00%\nEpoch: 1600. Loss: 0.2308. Acc.: 78.74%\nEpoch: 1601. Loss: 0.2373. Acc.: 80.45%\nEpoch: 1602. Loss: 0.2364. Acc.: 80.45%\nEpoch: 1603. Loss: 0.2317. Acc.: 80.31%\nEpoch: 1604. Loss: 0.2379. Acc.: 79.79%\nEpoch: 1605. Loss: 0.2281. Acc.: 79.92%\nEpoch: 1606. Loss: 0.2316. Acc.: 79.79%\nEpoch: 1607. Loss: 0.2224. Acc.: 80.58%\nEpoch: 1608. Loss: 0.2148. Acc.: 80.45%\nEpoch: 1609. Loss: 0.2244. Acc.: 80.31%\nEpoch: 1610. Loss: 0.2636. Acc.: 80.31%\nEpoch: 1611. Loss: 0.2454. Acc.: 80.71%\nEpoch: 1612. Loss: 0.2405. Acc.: 80.31%\nEpoch: 1613. Loss: 0.2419. Acc.: 80.31%\nEpoch: 1614. Loss: 0.2397. Acc.: 79.79%\nEpoch: 1615. Loss: 0.2221. Acc.: 81.50%\nEpoch: 1616. Loss: 0.2295. Acc.: 79.79%\nEpoch: 1617. Loss: 0.2351. Acc.: 80.05%\nEpoch: 1618. Loss: 0.2209. Acc.: 80.97%\nEpoch: 1619. Loss: 0.2212. Acc.: 80.18%\nEpoch: 1620. Loss: 0.2417. Acc.: 80.84%\nEpoch: 1621. Loss: 0.2418. Acc.: 80.45%\nEpoch: 1622. Loss: 0.2320. Acc.: 78.87%\nEpoch: 1623. Loss: 0.2445. Acc.: 78.87%\nEpoch: 1624. Loss: 0.2360. Acc.: 79.66%\nEpoch: 1625. Loss: 0.2160. Acc.: 80.18%\nEpoch: 1626. Loss: 0.2278. Acc.: 81.36%\nEpoch: 1627. Loss: 0.2564. Acc.: 80.97%\nEpoch: 1628. Loss: 0.2238. Acc.: 80.84%\nEpoch: 1629. Loss: 0.2481. Acc.: 80.05%\nEpoch: 1630. Loss: 0.2493. Acc.: 78.74%\nEpoch: 1631. Loss: 0.2397. Acc.: 79.27%\nEpoch: 1632. Loss: 0.2433. Acc.: 79.92%\nEpoch: 1633. Loss: 0.2358. Acc.: 80.58%\nEpoch: 1634. Loss: 0.2262. Acc.: 79.92%\nEpoch: 1635. Loss: 0.2469. Acc.: 79.92%\nEpoch: 1636. Loss: 0.2317. Acc.: 80.84%\nEpoch: 1637. Loss: 0.2497. Acc.: 80.58%\nEpoch: 1638. Loss: 0.2321. Acc.: 79.53%\nEpoch: 1639. Loss: 0.2430. Acc.: 79.66%\nEpoch: 1640. Loss: 0.2440. Acc.: 79.53%\nEpoch: 1641. Loss: 0.2387. Acc.: 79.66%\nEpoch: 1642. Loss: 0.2335. Acc.: 80.18%\nEpoch: 1643. Loss: 0.2457. Acc.: 80.31%\nEpoch: 1644. Loss: 0.2457. Acc.: 79.27%\nEpoch: 1645. Loss: 0.2272. Acc.: 80.05%\nEpoch: 1646. Loss: 0.2513. Acc.: 79.00%\nEpoch: 1647. Loss: 0.2444. Acc.: 78.74%\nEpoch: 1648. Loss: 0.2278. Acc.: 80.45%\nEpoch: 1649. Loss: 0.2251. Acc.: 78.87%\nEpoch: 1650. Loss: 0.2356. Acc.: 79.40%\nEpoch: 1651. Loss: 0.2313. Acc.: 80.97%\nEpoch: 1652. Loss: 0.2389. Acc.: 81.23%\nEpoch: 1653. Loss: 0.2354. Acc.: 80.45%\nEpoch: 1654. Loss: 0.2481. Acc.: 80.58%\nEpoch: 1655. Loss: 0.2255. Acc.: 78.22%\nEpoch: 1656. Loss: 0.2283. Acc.: 79.79%\nEpoch: 1657. Loss: 0.2398. Acc.: 80.58%\nEpoch: 1658. Loss: 0.2424. Acc.: 80.71%\nEpoch: 1659. Loss: 0.2620. Acc.: 80.45%\nEpoch: 1660. Loss: 0.2618. Acc.: 81.10%\nEpoch: 1661. Loss: 0.2410. Acc.: 80.71%\nEpoch: 1662. Loss: 0.2293. Acc.: 80.31%\nEpoch: 1663. Loss: 0.2167. Acc.: 80.97%\nEpoch: 1664. Loss: 0.2454. Acc.: 80.45%\nEpoch: 1665. Loss: 0.2259. Acc.: 81.10%\nEpoch: 1666. Loss: 0.2295. Acc.: 81.10%\nEpoch: 1667. Loss: 0.2257. Acc.: 79.79%\nEpoch: 1668. Loss: 0.2328. Acc.: 80.84%\nEpoch: 1669. Loss: 0.2191. Acc.: 80.58%\nEpoch: 1670. Loss: 0.2243. Acc.: 78.74%\nEpoch: 1671. Loss: 0.2190. Acc.: 78.87%\nEpoch: 1672. Loss: 0.2188. Acc.: 79.66%\nEpoch: 1673. Loss: 0.2194. Acc.: 79.79%\nEpoch: 1674. Loss: 0.2304. Acc.: 79.27%\nEpoch: 1675. Loss: 0.2167. Acc.: 80.18%\nEpoch: 1676. Loss: 0.2241. Acc.: 78.74%\nEpoch: 1677. Loss: 0.2335. Acc.: 78.22%\nEpoch: 1678. Loss: 0.2271. Acc.: 79.27%\nEpoch: 1679. Loss: 0.2351. Acc.: 78.48%\nEpoch: 1680. Loss: 0.2235. Acc.: 80.18%\nEpoch: 1681. Loss: 0.2419. Acc.: 78.87%\nEpoch: 1682. Loss: 0.2587. Acc.: 79.53%\nEpoch: 1683. Loss: 0.2396. Acc.: 80.58%\nEpoch: 1684. Loss: 0.2364. Acc.: 79.79%\nEpoch: 1685. Loss: 0.2355. Acc.: 79.53%\nEpoch: 1686. Loss: 0.2339. Acc.: 79.79%\nEpoch: 1687. Loss: 0.2301. Acc.: 78.35%\nEpoch: 1688. Loss: 0.2412. Acc.: 79.66%\nEpoch: 1689. Loss: 0.2334. Acc.: 80.05%\nEpoch: 1690. Loss: 0.2458. Acc.: 80.45%\nEpoch: 1691. Loss: 0.2441. Acc.: 80.71%\nEpoch: 1692. Loss: 0.2408. Acc.: 81.23%\nEpoch: 1693. Loss: 0.2429. Acc.: 79.53%\nEpoch: 1694. Loss: 0.2515. Acc.: 79.00%\nEpoch: 1695. Loss: 0.2324. Acc.: 80.31%\nEpoch: 1696. Loss: 0.2409. Acc.: 79.92%\nEpoch: 1697. Loss: 0.2301. Acc.: 80.31%\nEpoch: 1698. Loss: 0.2536. Acc.: 79.27%\nEpoch: 1699. Loss: 0.2533. Acc.: 79.53%\nEpoch: 1700. Loss: 0.2495. Acc.: 79.53%\nEpoch: 1701. Loss: 0.2338. Acc.: 79.66%\nEpoch: 1702. Loss: 0.2426. Acc.: 79.40%\nEpoch: 1703. Loss: 0.2206. Acc.: 79.79%\nEpoch: 1704. Loss: 0.2475. Acc.: 80.97%\nEpoch: 1705. Loss: 0.2344. Acc.: 80.84%\nEpoch: 1706. Loss: 0.2381. Acc.: 80.05%\nEpoch: 1707. Loss: 0.2222. Acc.: 79.27%\nEpoch: 1708. Loss: 0.2337. Acc.: 80.31%\nEpoch: 1709. Loss: 0.2176. Acc.: 80.97%\nEpoch: 1710. Loss: 0.2320. Acc.: 80.05%\nEpoch: 1711. Loss: 0.2245. Acc.: 79.79%\nEpoch: 1712. Loss: 0.2184. Acc.: 80.45%\nEpoch: 1713. Loss: 0.2233. Acc.: 81.10%\nEpoch: 1714. Loss: 0.2376. Acc.: 80.31%\nEpoch: 1715. Loss: 0.2374. Acc.: 79.66%\nEpoch: 1716. Loss: 0.2311. Acc.: 81.36%\nEpoch: 1717. Loss: 0.2070. Acc.: 80.05%\nEpoch: 1718. Loss: 0.2325. Acc.: 80.58%\nEpoch: 1719. Loss: 0.2264. Acc.: 80.31%\nEpoch: 1720. Loss: 0.2056. Acc.: 80.31%\nEpoch: 1721. Loss: 0.2303. Acc.: 79.92%\nEpoch: 1722. Loss: 0.2254. Acc.: 80.84%\nEpoch: 1723. Loss: 0.2315. Acc.: 79.00%\nEpoch: 1724. Loss: 0.2373. Acc.: 82.02%\nEpoch: 1725. Loss: 0.2230. Acc.: 81.23%\nEpoch: 1726. Loss: 0.2538. Acc.: 81.36%\nEpoch: 1727. Loss: 0.2283. Acc.: 80.97%\nEpoch: 1728. Loss: 0.2167. Acc.: 81.23%\nEpoch: 1729. Loss: 0.2257. Acc.: 80.31%\nEpoch: 1730. Loss: 0.2374. Acc.: 78.22%\nEpoch: 1731. Loss: 0.2105. Acc.: 79.53%\nEpoch: 1732. Loss: 0.2275. Acc.: 80.18%\nEpoch: 1733. Loss: 0.2245. Acc.: 79.79%\nEpoch: 1734. Loss: 0.2347. Acc.: 80.18%\nEpoch: 1735. Loss: 0.2561. Acc.: 80.58%\nEpoch: 1736. Loss: 0.2500. Acc.: 79.40%\nEpoch: 1737. Loss: 0.2355. Acc.: 79.66%\nEpoch: 1738. Loss: 0.2338. Acc.: 80.31%\nEpoch: 1739. Loss: 0.2214. Acc.: 81.50%\nEpoch: 1740. Loss: 0.2602. Acc.: 78.87%\nEpoch: 1741. Loss: 0.2203. Acc.: 80.58%\nEpoch: 1742. Loss: 0.2242. Acc.: 80.31%\nEpoch: 1743. Loss: 0.2392. Acc.: 78.87%\nEpoch: 1744. Loss: 0.2125. Acc.: 80.18%\nEpoch: 1745. Loss: 0.2278. Acc.: 80.71%\nEpoch: 1746. Loss: 0.2397. Acc.: 78.87%\nEpoch: 1747. Loss: 0.2259. Acc.: 80.18%\nEpoch: 1748. Loss: 0.2311. Acc.: 79.00%\nEpoch: 1749. Loss: 0.2533. Acc.: 78.74%\nEpoch: 1750. Loss: 0.2257. Acc.: 78.61%\nEpoch: 1751. Loss: 0.2285. Acc.: 80.05%\nEpoch: 1752. Loss: 0.2332. Acc.: 80.84%\nEpoch: 1753. Loss: 0.2480. Acc.: 80.84%\nEpoch: 1754. Loss: 0.2418. Acc.: 80.05%\nEpoch: 1755. Loss: 0.2376. Acc.: 79.66%\nEpoch: 1756. Loss: 0.2203. Acc.: 80.18%\nEpoch: 1757. Loss: 0.2296. Acc.: 81.36%\nEpoch: 1758. Loss: 0.2182. Acc.: 81.10%\nEpoch: 1759. Loss: 0.2215. Acc.: 80.18%\nEpoch: 1760. Loss: 0.2183. Acc.: 80.05%\nEpoch: 1761. Loss: 0.2349. Acc.: 79.40%\nEpoch: 1762. Loss: 0.2340. Acc.: 79.53%\nEpoch: 1763. Loss: 0.2176. Acc.: 79.79%\nEpoch: 1764. Loss: 0.2147. Acc.: 79.53%\nEpoch: 1765. Loss: 0.2412. Acc.: 80.18%\nEpoch: 1766. Loss: 0.2399. Acc.: 80.45%\nEpoch: 1767. Loss: 0.2193. Acc.: 80.71%\nEpoch: 1768. Loss: 0.2392. Acc.: 80.97%\nEpoch: 1769. Loss: 0.2383. Acc.: 80.97%\nEpoch: 1770. Loss: 0.2329. Acc.: 79.40%\nEpoch: 1771. Loss: 0.2409. Acc.: 79.13%\nEpoch: 1772. Loss: 0.2289. Acc.: 80.05%\nEpoch: 1773. Loss: 0.2424. Acc.: 80.58%\nEpoch: 1774. Loss: 0.2388. Acc.: 80.58%\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 1775. Loss: 0.2376. Acc.: 81.89%\nEpoch: 1776. Loss: 0.2268. Acc.: 82.68%\nEpoch: 1777. Loss: 0.2262. Acc.: 80.45%\nEpoch: 1778. Loss: 0.2211. Acc.: 80.71%\nEpoch: 1779. Loss: 0.2160. Acc.: 79.92%\nEpoch: 1780. Loss: 0.2319. Acc.: 80.71%\nEpoch: 1781. Loss: 0.2306. Acc.: 79.66%\nEpoch: 1782. Loss: 0.2424. Acc.: 80.45%\nEpoch: 1783. Loss: 0.2233. Acc.: 80.84%\nEpoch: 1784. Loss: 0.2233. Acc.: 80.97%\nEpoch: 1785. Loss: 0.2446. Acc.: 81.23%\nEpoch: 1786. Loss: 0.2441. Acc.: 80.18%\nEpoch: 1787. Loss: 0.2194. Acc.: 79.13%\nEpoch: 1788. Loss: 0.2410. Acc.: 80.84%\nEpoch: 1789. Loss: 0.2295. Acc.: 80.84%\nEpoch: 1790. Loss: 0.2309. Acc.: 80.71%\nEpoch: 1791. Loss: 0.2397. Acc.: 80.05%\nEpoch: 1792. Loss: 0.2062. Acc.: 79.92%\nEpoch: 1793. Loss: 0.2340. Acc.: 80.05%\nEpoch: 1794. Loss: 0.2231. Acc.: 80.45%\nEpoch: 1795. Loss: 0.2197. Acc.: 79.79%\nEpoch: 1796. Loss: 0.2245. Acc.: 79.92%\nEpoch: 1797. Loss: 0.2218. Acc.: 79.00%\nEpoch: 1798. Loss: 0.2484. Acc.: 79.27%\nEpoch: 1799. Loss: 0.2328. Acc.: 79.66%\nEpoch: 1800. Loss: 0.2352. Acc.: 79.66%\nEpoch: 1801. Loss: 0.2336. Acc.: 79.53%\nEpoch: 1802. Loss: 0.2212. Acc.: 78.61%\nEpoch: 1803. Loss: 0.2353. Acc.: 80.05%\nEpoch: 1804. Loss: 0.2224. Acc.: 79.53%\nEpoch: 1805. Loss: 0.2282. Acc.: 79.53%\nEpoch: 1806. Loss: 0.2275. Acc.: 79.53%\nEpoch: 1807. Loss: 0.2409. Acc.: 80.18%\nEpoch: 1808. Loss: 0.2213. Acc.: 78.48%\nEpoch: 1809. Loss: 0.2440. Acc.: 78.35%\nEpoch: 1810. Loss: 0.2122. Acc.: 79.13%\nEpoch: 1811. Loss: 0.2403. Acc.: 79.92%\nEpoch: 1812. Loss: 0.2236. Acc.: 78.87%\nEpoch: 1813. Loss: 0.2551. Acc.: 79.13%\nEpoch: 1814. Loss: 0.2581. Acc.: 79.53%\nEpoch: 1815. Loss: 0.2168. Acc.: 79.27%\nEpoch: 1816. Loss: 0.2311. Acc.: 79.92%\nEpoch: 1817. Loss: 0.2377. Acc.: 80.05%\nEpoch: 1818. Loss: 0.2172. Acc.: 80.05%\nEpoch: 1819. Loss: 0.2029. Acc.: 78.48%\nEpoch: 1820. Loss: 0.2427. Acc.: 78.74%\nEpoch: 1821. Loss: 0.2168. Acc.: 79.13%\nEpoch: 1822. Loss: 0.2444. Acc.: 78.35%\nEpoch: 1823. Loss: 0.2185. Acc.: 78.61%\nEpoch: 1824. Loss: 0.2244. Acc.: 78.48%\nEpoch: 1825. Loss: 0.2268. Acc.: 79.00%\nEpoch: 1826. Loss: 0.2149. Acc.: 80.84%\nEpoch: 1827. Loss: 0.2188. Acc.: 79.53%\nEpoch: 1828. Loss: 0.2243. Acc.: 80.18%\nEpoch: 1829. Loss: 0.2191. Acc.: 80.84%\nEpoch: 1830. Loss: 0.2170. Acc.: 79.13%\nEpoch: 1831. Loss: 0.2220. Acc.: 79.40%\nEpoch: 1832. Loss: 0.2075. Acc.: 80.18%\nEpoch: 1833. Loss: 0.2367. Acc.: 78.74%\nEpoch: 1834. Loss: 0.2201. Acc.: 78.22%\nEpoch: 1835. Loss: 0.2195. Acc.: 78.22%\nEpoch: 1836. Loss: 0.2463. Acc.: 78.74%\nEpoch: 1837. Loss: 0.2211. Acc.: 80.18%\nEpoch: 1838. Loss: 0.2189. Acc.: 80.45%\nEpoch: 1839. Loss: 0.2526. Acc.: 78.87%\nEpoch: 1840. Loss: 0.2182. Acc.: 80.05%\nEpoch: 1841. Loss: 0.2113. Acc.: 80.71%\nEpoch: 1842. Loss: 0.2414. Acc.: 80.05%\nEpoch: 1843. Loss: 0.2315. Acc.: 79.27%\nEpoch: 1844. Loss: 0.2065. Acc.: 80.45%\nEpoch: 1845. Loss: 0.2432. Acc.: 80.31%\nEpoch: 1846. Loss: 0.2511. Acc.: 80.18%\nEpoch: 1847. Loss: 0.2270. Acc.: 78.61%\nEpoch: 1848. Loss: 0.2455. Acc.: 79.40%\nEpoch: 1849. Loss: 0.2450. Acc.: 80.97%\nEpoch: 1850. Loss: 0.2228. Acc.: 80.84%\nEpoch: 1851. Loss: 0.2279. Acc.: 79.92%\nEpoch: 1852. Loss: 0.2357. Acc.: 80.18%\nEpoch: 1853. Loss: 0.2269. Acc.: 79.79%\nEpoch: 1854. Loss: 0.2380. Acc.: 78.22%\nEpoch: 1855. Loss: 0.2432. Acc.: 79.92%\nEpoch: 1856. Loss: 0.2330. Acc.: 79.40%\nEpoch: 1857. Loss: 0.2445. Acc.: 80.05%\nEpoch: 1858. Loss: 0.2534. Acc.: 81.50%\nEpoch: 1859. Loss: 0.2395. Acc.: 80.58%\nEpoch: 1860. Loss: 0.2379. Acc.: 79.27%\nEpoch: 1861. Loss: 0.2306. Acc.: 80.31%\nEpoch: 1862. Loss: 0.2284. Acc.: 80.45%\nEpoch: 1863. Loss: 0.2163. Acc.: 79.53%\nEpoch: 1864. Loss: 0.2169. Acc.: 80.18%\nEpoch: 1865. Loss: 0.2375. Acc.: 79.27%\nEpoch: 1866. Loss: 0.2461. Acc.: 78.74%\nEpoch: 1867. Loss: 0.2012. Acc.: 79.66%\nEpoch: 1868. Loss: 0.2233. Acc.: 78.74%\nEpoch: 1869. Loss: 0.2170. Acc.: 78.87%\nEpoch: 1870. Loss: 0.2420. Acc.: 79.13%\nEpoch: 1871. Loss: 0.2368. Acc.: 80.71%\nEpoch: 1872. Loss: 0.2355. Acc.: 79.40%\nEpoch: 1873. Loss: 0.2277. Acc.: 78.74%\nEpoch: 1874. Loss: 0.2365. Acc.: 78.87%\nEpoch: 1875. Loss: 0.2150. Acc.: 79.40%\nEpoch: 1876. Loss: 0.2281. Acc.: 79.53%\nEpoch: 1877. Loss: 0.2377. Acc.: 80.45%\nEpoch: 1878. Loss: 0.2128. Acc.: 79.92%\nEpoch: 1879. Loss: 0.2087. Acc.: 80.18%\nEpoch: 1880. Loss: 0.2276. Acc.: 79.66%\nEpoch: 1881. Loss: 0.2312. Acc.: 79.13%\nEpoch: 1882. Loss: 0.2493. Acc.: 79.79%\nEpoch: 1883. Loss: 0.2240. Acc.: 80.05%\nEpoch: 1884. Loss: 0.2343. Acc.: 79.92%\nEpoch: 1885. Loss: 0.2290. Acc.: 80.84%\nEpoch: 1886. Loss: 0.2299. Acc.: 80.58%\nEpoch: 1887. Loss: 0.2249. Acc.: 79.79%\nEpoch: 1888. Loss: 0.2274. Acc.: 78.61%\nEpoch: 1889. Loss: 0.2376. Acc.: 80.31%\nEpoch: 1890. Loss: 0.2324. Acc.: 79.79%\nEpoch: 1891. Loss: 0.2359. Acc.: 79.53%\nEpoch: 1892. Loss: 0.2432. Acc.: 80.31%\nEpoch: 1893. Loss: 0.2272. Acc.: 79.40%\nEpoch: 1894. Loss: 0.2282. Acc.: 79.00%\nEpoch: 1895. Loss: 0.2260. Acc.: 79.92%\nEpoch: 1896. Loss: 0.2293. Acc.: 79.66%\nEpoch: 1897. Loss: 0.2281. Acc.: 79.40%\nEpoch: 1898. Loss: 0.1955. Acc.: 80.31%\nEpoch: 1899. Loss: 0.2328. Acc.: 80.05%\nEpoch: 1900. Loss: 0.2294. Acc.: 80.45%\nEpoch: 1901. Loss: 0.2275. Acc.: 80.18%\nEpoch: 1902. Loss: 0.2123. Acc.: 78.48%\nEpoch: 1903. Loss: 0.2481. Acc.: 80.05%\nEpoch: 1904. Loss: 0.2142. Acc.: 79.79%\nEpoch: 1905. Loss: 0.2147. Acc.: 80.05%\nEpoch: 1906. Loss: 0.2400. Acc.: 79.79%\nEpoch: 1907. Loss: 0.2232. Acc.: 79.79%\nEpoch: 1908. Loss: 0.2293. Acc.: 79.53%\nEpoch: 1909. Loss: 0.2247. Acc.: 79.00%\nEpoch: 1910. Loss: 0.2359. Acc.: 79.92%\nEpoch: 1911. Loss: 0.2156. Acc.: 79.53%\nEpoch: 1912. Loss: 0.2071. Acc.: 79.53%\nEpoch: 1913. Loss: 0.2199. Acc.: 79.66%\nEpoch: 1914. Loss: 0.1928. Acc.: 80.18%\nEpoch: 1915. Loss: 0.2163. Acc.: 79.66%\nEpoch: 1916. Loss: 0.2212. Acc.: 80.45%\nEpoch: 1917. Loss: 0.2179. Acc.: 80.45%\nEpoch: 1918. Loss: 0.2332. Acc.: 80.45%\nEpoch: 1919. Loss: 0.2224. Acc.: 80.18%\nEpoch: 1920. Loss: 0.2161. Acc.: 80.45%\nEpoch: 1921. Loss: 0.2256. Acc.: 79.27%\nEpoch: 1922. Loss: 0.2246. Acc.: 79.66%\nEpoch: 1923. Loss: 0.2217. Acc.: 80.58%\nEpoch: 1924. Loss: 0.2190. Acc.: 80.45%\nEpoch: 1925. Loss: 0.2339. Acc.: 80.84%\nEpoch: 1926. Loss: 0.2395. Acc.: 80.97%\nEpoch: 1927. Loss: 0.2253. Acc.: 80.84%\nEpoch: 1928. Loss: 0.2062. Acc.: 80.71%\nEpoch: 1929. Loss: 0.2129. Acc.: 79.53%\nEpoch: 1930. Loss: 0.2526. Acc.: 80.45%\nEpoch: 1931. Loss: 0.2290. Acc.: 80.84%\nEpoch: 1932. Loss: 0.2446. Acc.: 79.92%\nEpoch: 1933. Loss: 0.2320. Acc.: 78.74%\nEpoch: 1934. Loss: 0.2473. Acc.: 80.31%\nEpoch: 1935. Loss: 0.2175. Acc.: 79.66%\nEpoch: 1936. Loss: 0.2189. Acc.: 80.18%\nEpoch: 1937. Loss: 0.2244. Acc.: 80.31%\nEpoch: 1938. Loss: 0.2044. Acc.: 80.05%\nEpoch: 1939. Loss: 0.2351. Acc.: 80.84%\nEpoch: 1940. Loss: 0.2382. Acc.: 79.66%\nEpoch: 1941. Loss: 0.2018. Acc.: 79.79%\nEpoch: 1942. Loss: 0.2275. Acc.: 80.58%\nEpoch: 1943. Loss: 0.2208. Acc.: 80.71%\nEpoch: 1944. Loss: 0.2452. Acc.: 80.31%\nEpoch: 1945. Loss: 0.2326. Acc.: 80.18%\nEpoch: 1946. Loss: 0.2295. Acc.: 79.79%\nEpoch: 1947. Loss: 0.2265. Acc.: 79.00%\nEpoch: 1948. Loss: 0.2272. Acc.: 79.27%\nEpoch: 1949. Loss: 0.2398. Acc.: 80.18%\nEpoch: 1950. Loss: 0.2263. Acc.: 80.18%\nEpoch: 1951. Loss: 0.2284. Acc.: 79.66%\nEpoch: 1952. Loss: 0.2542. Acc.: 80.31%\nEpoch: 1953. Loss: 0.2160. Acc.: 80.45%\nEpoch: 1954. Loss: 0.2374. Acc.: 80.45%\nEpoch: 1955. Loss: 0.2341. Acc.: 79.66%\nEpoch: 1956. Loss: 0.2224. Acc.: 79.27%\nEpoch: 1957. Loss: 0.2278. Acc.: 80.45%\nEpoch: 1958. Loss: 0.2250. Acc.: 80.05%\nEpoch: 1959. Loss: 0.2627. Acc.: 79.79%\nEpoch: 1960. Loss: 0.2233. Acc.: 79.00%\nEpoch: 1961. Loss: 0.2148. Acc.: 79.79%\nEpoch: 1962. Loss: 0.2246. Acc.: 79.66%\nEpoch: 1963. Loss: 0.2137. Acc.: 79.92%\nEpoch: 1964. Loss: 0.2434. Acc.: 80.18%\nEpoch: 1965. Loss: 0.2367. Acc.: 79.66%\nEpoch: 1966. Loss: 0.2412. Acc.: 79.27%\nEpoch: 1967. Loss: 0.2421. Acc.: 79.27%\nEpoch: 1968. Loss: 0.2456. Acc.: 79.79%\nEpoch: 1969. Loss: 0.2293. Acc.: 79.40%\nEpoch: 1970. Loss: 0.2225. Acc.: 79.66%\nEpoch: 1971. Loss: 0.2449. Acc.: 79.92%\nEpoch: 1972. Loss: 0.2386. Acc.: 79.13%\nEpoch: 1973. Loss: 0.2233. Acc.: 79.53%\nEpoch: 1974. Loss: 0.2446. Acc.: 79.53%\nEpoch: 1975. Loss: 0.2073. Acc.: 79.00%\nEpoch: 1976. Loss: 0.2165. Acc.: 79.00%\nEpoch: 1977. Loss: 0.2329. Acc.: 79.92%\nEpoch: 1978. Loss: 0.2241. Acc.: 78.61%\nEpoch: 1979. Loss: 0.2390. Acc.: 79.66%\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 1980. Loss: 0.2403. Acc.: 79.40%\nEpoch: 1981. Loss: 0.2184. Acc.: 79.66%\nEpoch: 1982. Loss: 0.2438. Acc.: 80.45%\nEpoch: 1983. Loss: 0.2203. Acc.: 79.92%\nEpoch: 1984. Loss: 0.2024. Acc.: 80.05%\nEpoch: 1985. Loss: 0.2148. Acc.: 79.40%\nEpoch: 1986. Loss: 0.2143. Acc.: 80.18%\nEpoch: 1987. Loss: 0.2280. Acc.: 79.66%\nEpoch: 1988. Loss: 0.2232. Acc.: 79.92%\nEpoch: 1989. Loss: 0.2322. Acc.: 80.31%\nEpoch: 1990. Loss: 0.2168. Acc.: 80.58%\nEpoch: 1991. Loss: 0.2025. Acc.: 80.84%\nEpoch: 1992. Loss: 0.2559. Acc.: 80.45%\nEpoch: 1993. Loss: 0.2216. Acc.: 80.45%\nEpoch: 1994. Loss: 0.2387. Acc.: 80.05%\nEpoch: 1995. Loss: 0.2288. Acc.: 81.36%\nEpoch: 1996. Loss: 0.2159. Acc.: 79.53%\nEpoch: 1997. Loss: 0.2069. Acc.: 80.18%\nEpoch: 1998. Loss: 0.2217. Acc.: 80.18%\nEpoch: 1999. Loss: 0.2219. Acc.: 79.66%\nEpoch: 2000. Loss: 0.2320. Acc.: 78.74%\nEpoch: 2001. Loss: 0.2177. Acc.: 80.71%\nEpoch: 2002. Loss: 0.2372. Acc.: 80.84%\nEpoch: 2003. Loss: 0.2371. Acc.: 79.40%\nEpoch: 2004. Loss: 0.2252. Acc.: 80.05%\nEpoch: 2005. Loss: 0.2121. Acc.: 80.71%\nEpoch: 2006. Loss: 0.2407. Acc.: 81.10%\nEpoch: 2007. Loss: 0.2294. Acc.: 81.10%\nEpoch: 2008. Loss: 0.2133. Acc.: 80.05%\nEpoch: 2009. Loss: 0.2166. Acc.: 80.84%\nEpoch: 2010. Loss: 0.2182. Acc.: 80.84%\nEpoch: 2011. Loss: 0.2202. Acc.: 79.53%\nEpoch: 2012. Loss: 0.2538. Acc.: 79.79%\nEpoch: 2013. Loss: 0.2424. Acc.: 78.87%\nEpoch: 2014. Loss: 0.2298. Acc.: 79.53%\nEpoch: 2015. Loss: 0.2234. Acc.: 80.05%\nEpoch: 2016. Loss: 0.2150. Acc.: 81.50%\nEpoch: 2017. Loss: 0.2327. Acc.: 81.50%\nEpoch: 2018. Loss: 0.2038. Acc.: 80.18%\nEpoch: 2019. Loss: 0.2326. Acc.: 79.27%\nEpoch: 2020. Loss: 0.2140. Acc.: 80.18%\nEpoch: 2021. Loss: 0.2162. Acc.: 79.66%\nEpoch: 2022. Loss: 0.2210. Acc.: 80.18%\nEpoch: 2023. Loss: 0.2452. Acc.: 79.66%\nEpoch: 2024. Loss: 0.2403. Acc.: 79.79%\nEpoch: 2025. Loss: 0.2182. Acc.: 79.79%\nEpoch: 2026. Loss: 0.2066. Acc.: 79.53%\nEpoch: 2027. Loss: 0.2106. Acc.: 79.92%\nEpoch: 2028. Loss: 0.2128. Acc.: 79.00%\nEpoch: 2029. Loss: 0.2004. Acc.: 79.27%\nEpoch: 2030. Loss: 0.2293. Acc.: 80.71%\nEpoch: 2031. Loss: 0.2026. Acc.: 80.58%\nEpoch: 2032. Loss: 0.2366. Acc.: 79.53%\nEpoch: 2033. Loss: 0.2213. Acc.: 81.10%\nEpoch: 2034. Loss: 0.2287. Acc.: 81.10%\nEpoch: 2035. Loss: 0.2327. Acc.: 80.71%\nEpoch: 2036. Loss: 0.2130. Acc.: 79.92%\nEpoch: 2037. Loss: 0.2200. Acc.: 78.61%\nEpoch: 2038. Loss: 0.2246. Acc.: 80.45%\nEpoch: 2039. Loss: 0.2119. Acc.: 80.97%\nEpoch: 2040. Loss: 0.2395. Acc.: 79.27%\nEpoch: 2041. Loss: 0.2444. Acc.: 79.40%\nEpoch: 2042. Loss: 0.2271. Acc.: 79.53%\nEpoch: 2043. Loss: 0.2162. Acc.: 78.22%\nEpoch: 2044. Loss: 0.2419. Acc.: 78.08%\nEpoch: 2045. Loss: 0.2106. Acc.: 80.18%\nEpoch: 2046. Loss: 0.2116. Acc.: 78.74%\nEpoch: 2047. Loss: 0.2204. Acc.: 78.87%\nEpoch: 2048. Loss: 0.2294. Acc.: 78.74%\nEpoch: 2048. Loss: 0.2294. Acc.: 78.74%\nEpoch: 2049. Loss: 0.2392. Acc.: 79.40%\nEpoch: 2050. Loss: 0.2123. Acc.: 79.53%\nEpoch: 2051. Loss: 0.2178. Acc.: 79.66%\nEpoch: 2052. Loss: 0.2271. Acc.: 79.40%\nEpoch: 2053. Loss: 0.2279. Acc.: 78.48%\nEpoch: 2054. Loss: 0.2255. Acc.: 79.40%\nEpoch: 2055. Loss: 0.2210. Acc.: 79.53%\nEpoch: 2056. Loss: 0.2277. Acc.: 78.87%\nEpoch: 2057. Loss: 0.2344. Acc.: 80.05%\nEpoch: 2058. Loss: 0.2287. Acc.: 80.58%\nEpoch: 2059. Loss: 0.2355. Acc.: 80.18%\nEpoch: 2060. Loss: 0.2459. Acc.: 81.63%\nEpoch: 2061. Loss: 0.2203. Acc.: 79.79%\nEpoch: 2062. Loss: 0.2274. Acc.: 79.27%\nEpoch: 2063. Loss: 0.2074. Acc.: 80.05%\nEpoch: 2064. Loss: 0.2209. Acc.: 80.05%\nEpoch: 2065. Loss: 0.2475. Acc.: 79.53%\nEpoch: 2066. Loss: 0.2177. Acc.: 80.05%\nEpoch: 2067. Loss: 0.2256. Acc.: 79.53%\nEpoch: 2068. Loss: 0.2232. Acc.: 78.61%\nEpoch: 2069. Loss: 0.2133. Acc.: 78.87%\nEpoch: 2070. Loss: 0.2313. Acc.: 79.40%\nEpoch: 2071. Loss: 0.2286. Acc.: 79.13%\nEpoch: 2072. Loss: 0.2068. Acc.: 79.66%\nEpoch: 2073. Loss: 0.2195. Acc.: 80.18%\nEpoch: 2074. Loss: 0.2312. Acc.: 79.40%\nEpoch: 2075. Loss: 0.2200. Acc.: 79.40%\nEpoch: 2076. Loss: 0.2482. Acc.: 79.92%\nEpoch: 2077. Loss: 0.2061. Acc.: 78.74%\nEpoch: 2078. Loss: 0.2095. Acc.: 78.22%\nEpoch: 2079. Loss: 0.2166. Acc.: 78.48%\nEarly stopping on epoch 2079\nDone!\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"5\">**Compute result on testing data**</font><br>"},{"metadata":{"_uuid":"d0f483d5-5542-4d65-a70f-1aa7b3e13c3c","_cell_guid":"05b2b8d9-2b63-436e-a906-a839fb008c0e","trusted":true},"cell_type":"code","source":"test_results = []\nmodel.load_state_dict(torch.load('best.pth'))\nmodel.eval()\n\n# for batch in val_dl:\n#     x_raw, y_batch = [t.to(device) for t in batch]\n#     out = model(x_raw)\n#     preds = F.log_softmax(out, dim=1).argmax(dim=1)\n#     total += y_batch.size(0)\n#     correct += (preds == y_batch).sum().item()\n#     acc = correct / total\n#     acc_history.append(acc)\n\n# for batch in tst_dl:\n#     x, y_batche = [t.to(device) for t in (batch)]\n#     out = model(x)\n#     y_hat = F.log_softmax(out, dim=1).argmax(dim=1)\n#     test_results.extend(y_hat.tolist())\n    \nfor batch in tst_dl:\n        x_raw, y_batch = [t.to(device) for t in batch]\n        out = model(x_channel_0, x_channel_1, x_channel_2, x_channel_3, x_channel_4, x_channel_5, x_channel_6, x_channel_7, x_channel_8)\n        preds = F.log_softmax(out, dim=1).argmax(dim=1)\n        total += y_batch.size(0)\n        correct += (preds == y_batch).sum().item()\n    \nacc = correct / total\nprint(\"accurancy on test data: \"+str(acc))","execution_count":201,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"too many values to unpack (expected 2)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-201-d72e3c7313b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtst_dl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mx_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_channel_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_channel_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_channel_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_channel_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_channel_4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_channel_5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_channel_6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_channel_7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_channel_8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"]}]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"5\">**Store final result**</font><br>"},{"metadata":{"_uuid":"7a49a38e-e203-4884-ba78-33fe8aa02426","_cell_guid":"f4bd2628-c801-42d5-9af6-11bbd89f2352","trusted":true},"cell_type":"code","source":"'''\nsubmit = pd.read_csv(sample)\nenc = joblib.load(ROOT/'encoder.model')\nsubmit['surface'] = enc.inverse_transform(test_results)\nsubmit.to_csv('submit_base.csv', index=None)\nprint(\"store result successfully!\")\n'''","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}